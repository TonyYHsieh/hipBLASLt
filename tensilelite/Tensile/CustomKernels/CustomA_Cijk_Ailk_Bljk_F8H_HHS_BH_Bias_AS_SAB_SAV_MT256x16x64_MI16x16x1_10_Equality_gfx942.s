
/******************************************/
/* Begin Kernel                           */
/******************************************/
.amdgcn_target "amdgcn-amd-amdhsa--gfx942"
.text
.protected Custom_Cijk_Ailk_Bljk_F8H_HHS_BH_Bias_AS_SAB_SAV_MT256x16x64_MI16x16x1_10_Equality_gfx942
.globl Custom_Cijk_Ailk_Bljk_F8H_HHS_BH_Bias_AS_SAB_SAV_MT256x16x64_MI16x16x1_10_Equality_gfx942
.p2align 8
.type Custom_Cijk_Ailk_Bljk_F8H_HHS_BH_Bias_AS_SAB_SAV_MT256x16x64_MI16x16x1_10_Equality_gfx942,@function
.section .rodata,#alloc
.p2align 6
.amdhsa_kernel Custom_Cijk_Ailk_Bljk_F8H_HHS_BH_Bias_AS_SAB_SAV_MT256x16x64_MI16x16x1_10_Equality_gfx942
  .amdhsa_user_sgpr_kernarg_segment_ptr 1
  .amdhsa_accum_offset 256 // accvgpr offset
  .amdhsa_next_free_vgpr 272 // vgprs
  .amdhsa_next_free_sgpr 100 // sgprs
  .amdhsa_group_segment_fixed_size 39424 // lds bytes
  .amdhsa_private_segment_fixed_size 0
  .amdhsa_system_sgpr_workgroup_id_x 1
  .amdhsa_system_sgpr_workgroup_id_y 1
  .amdhsa_system_sgpr_workgroup_id_z 1
  .amdhsa_system_vgpr_workitem_id 0
  .amdhsa_float_denorm_mode_32 3
  .amdhsa_float_denorm_mode_16_64 3
  .amdhsa_user_sgpr_count 13
  .amdhsa_user_sgpr_kernarg_preload_length 11
  .amdhsa_user_sgpr_kernarg_preload_offset 0
.end_amdhsa_kernel
.text
/* Num VGPR   =256 */
/* Num AccVGPR=16 */
/* Num SGPR   =100 */

/******************************************/
/* Optimizations and Config:              */
/******************************************/
/* ThreadTile= 16 x 1 */
/* SubGroup= 16 x 16 */
/* VectorWidthA=4 */
/* VectorWidthB=1 */
/* GlobalReadVectorWidthA=16, GlobalReadVectorWidthB=4 */
/* DirectToLdsA=False */
/* DirectToLdsB=False */
/* UseSgprForGRO=False */
.amdgpu_metadata
---
amdhsa.version:
  - 1
  - 1
amdhsa.kernels:
  - .name: Custom_Cijk_Ailk_Bljk_F8H_HHS_BH_Bias_AS_SAB_SAV_MT256x16x64_MI16x16x1_10_Equality_gfx942
    .symbol: 'Custom_Cijk_Ailk_Bljk_F8H_HHS_BH_Bias_AS_SAB_SAV_MT256x16x64_MI16x16x1_10_Equality_gfx942.kd'
    .language:                   OpenCL C
    .language_version:
      - 2
      - 0
    .args:
      - .name:            SizesFree0
        .size:            4
        .offset:          0
        .value_kind:      by_value
        .value_type:      u32
      - .name:            SizesFree1
        .size:            4
        .offset:          4
        .value_kind:      by_value
        .value_type:      u32
      - .name:            SizesFree2
        .size:            4
        .offset:          8
        .value_kind:      by_value
        .value_type:      u32
      - .name:            SizesSum0
        .size:            4
        .offset:          12
        .value_kind:      by_value
        .value_type:      u32
      - .name:            D
        .size:            8
        .offset:          16
        .value_kind:      global_buffer
        .value_type:      f16
        .address_space:   generic
      - .name:            C
        .size:            8
        .offset:          24
        .value_kind:      global_buffer
        .value_type:      f16
        .address_space:   generic
      - .name:            A
        .size:            8
        .offset:          32
        .value_kind:      global_buffer
        .value_type:      f16
        .address_space:   generic
      - .name:            B
        .size:            8
        .offset:          40
        .value_kind:      global_buffer
        .value_type:      f16
        .address_space:   generic
      - .name:            strideD0
        .size:            4
        .offset:          48
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideD1
        .size:            4
        .offset:          52
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideC0
        .size:            4
        .offset:          56
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideC1
        .size:            4
        .offset:          60
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideA0
        .size:            4
        .offset:          64
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideA1
        .size:            4
        .offset:          68
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideB0
        .size:            4
        .offset:          72
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideB1
        .size:            4
        .offset:          76
        .value_kind:      by_value
        .value_type:      u32
      - .name:            alpha
        .size:            4
        .offset:          80
        .value_kind:      by_value
        .value_type:      f32
      - .name:            beta
        .size:            4
        .offset:          84
        .value_kind:      by_value
        .value_type:      f32
      - .name:            internalArgs
        .size:            4
        .offset:          88
        .value_kind:      by_value
        .value_type:      u32
      - .name:            AddressScaleA
        .size:            8
        .offset:          92
        .value_kind:      global_buffer
        .value_type:      f32
        .address_space:   generic
      - .name:            AddressScaleB
        .size:            8
        .offset:          100
        .value_kind:      global_buffer
        .value_type:      f32
        .address_space:   generic
      - .name:            AddressScaleAlphaVec
        .size:            8
        .offset:          108
        .value_kind:      global_buffer
        .value_type:      f32
        .address_space:   generic
      - .name:            bias
        .size:            8
        .offset:          116
        .value_kind:      global_buffer
        .value_type:      void
        .address_space:   generic
      - .name:            biasType
        .size:            4
        .offset:          124
        .value_kind:      by_value
        .value_type:      u32
      - .name:            StrideBias
        .size:            4
        .offset:          128
        .value_kind:      by_value
        .value_type:      u32
      - .name:            activationAlpha
        .size:            4
        .offset:          132
        .value_kind:      by_value
        .value_type:      f32
      - .name:            activationBeta
        .size:            4
        .offset:          136
        .value_kind:      by_value
        .value_type:      f32
      - .name:            activationType
        .size:            4
        .offset:          140
        .value_kind:      by_value
        .value_type:      u32
      - .name:            dstD
        .size:            8
        .offset:          144
        .value_kind:      global_buffer
        .value_type:      f16
        .address_space:   generic
      - .name:            Synchronizer
        .size:            8
        .offset:          152
        .value_kind:      global_buffer
        .value_type:      f32
        .address_space:   generic
      - .name:            GSUSync
        .size:            4
        .offset:          160
        .value_kind:      by_value
        .value_type:      u32
    .group_segment_fixed_size:   39424
    .kernarg_segment_align:      8
    .kernarg_segment_size:       168
    .max_flat_workgroup_size:    256
    .private_segment_fixed_size: 0
    .sgpr_count:                 100
    .sgpr_spill_count:           0
    .vgpr_count:                 256
    .vgpr_spill_count:           0
    .wavefront_size:             64
...
.end_amdgpu_metadata
Custom_Cijk_Ailk_Bljk_F8H_HHS_BH_Bias_AS_SAB_SAV_MT256x16x64_MI16x16x1_10_Equality_gfx942:

/* Magic div and mod functions */
.macro V_MAGIC_DIV dstIdx:req dividend:req magicNumber:req magicShift:req magicA:req
    v_mul_hi_u32 v[\dstIdx+1] \dividend \magicNumber
    v_mul_lo_u32 v[\dstIdx+0] \dividend \magicA
    v_add_u32 v[\dstIdx+0] v[\dstIdx+0] v[\dstIdx+1]
    v_lshrrev_b32 v[\dstIdx+0] \magicShift v[\dstIdx+0]
.endm

/******************************************/
/* VGPR Assignments                       */
/******************************************/
/* ValuC range: [0-0), serializedStore enabled */
.set vgprValuC, 0
/* ValuA/B   Xn=PLR buffer idx,  In=InnerUnroll idx */
.set vgprValuA_X0_I0, 0
.set vgprValuA_X1_I0, 0
.set vgprValuA_X2_I0, 0
.set vgprValuA_X3_I0, 0
.set vgprValuA_X0_I0_D0, 8
.set vgprValuA_X1_I0_D0, 10
.set vgprValuA_X2_I0_D0, 12
.set vgprValuA_X3_I0_D0, 14
.set vgprValuA_X0_I0_D1, 16
.set vgprValuA_X1_I0_D1, 18
.set vgprValuA_X2_I0_D1, 20
.set vgprValuA_X3_I0_D1, 22
.set vgprValuA_X0_I0_D2, 24
.set vgprValuA_X1_I0_D2, 26
.set vgprValuA_X2_I0_D2, 28
.set vgprValuA_X3_I0_D2, 30
.set vgprValuA_X0_I0_D3, 32
.set vgprValuA_X1_I0_D3, 34
.set vgprValuA_X2_I0_D3, 36
.set vgprValuA_X3_I0_D3, 38
.set vgprValuB_X0_I0, 40
.set vgprValuB_X1_I0, 42
.set vgprValuB_X2_I0, 44
.set vgprValuB_X3_I0, 46
.set vgprLocalWriteAddrA, 48
.set vgprLocalWriteAddrB, 49
.set vgprGlobalReadOffsetA, 50
.set vgprGlobalReadOffsetB, 54
.set vgprG2LA, 56
.set vgprG2LB, 88
.set vgprLocalReadAddrA, 90
.set vgprLocalReadAddrB, 91
.set vgprSerial, 92

/******************************************/
/* SGPR Assignments                       */
/******************************************/
.set sgprKernArgAddress, 0
.set sgprWorkGroup0, 2
.set sgprWorkGroup1, 3
.set sgprWorkGroup2, 4
.set sgprGSUSumIdx, 6
.set sgprGSULog2BpeC, 5
.set sgprGSULog2BpeD, 8
.set sgprWGM, 9
.set sgprLoopCounterL, 10
.set sgprOrigLoopCounter, 11
.set sgprSrdD, 12
.set sgprSrdC, 16
.set sgprNumWorkGroups0, 20
.set sgprNumWorkGroups1, 21
.set sgprSrdSync, 24
.set sgprWSDstart, 22
.set sgprSizesFree, 28
.set sgprSizesSum, 31
.set sgprAddressD, 32
.set sgprAddressC, 34
.set sgprAddressA, 36
.set sgprAddressB, 38
.set sgprStridesD, 40
.set sgprStridesC, 42
.set sgprStridesA, 44
.set sgprStridesB, 46
.set sgprAlpha, 48
.set sgprBeta, 49
.set sgprGSU, 50
.set sgprSrdA, 52
.set sgprSrdB, 56
.set sgprShadowLimitA, 60
.set sgprShadowLimitB, 62
.set sgprStaggerUIter, 51
.set sgprWrapUA, 64
.set sgprWrapUB, 66
.set sgprGlobalReadIncsA, 68
.set sgprGlobalReadIncsB, 69
.set sgprPackKForV0, 70
.set sgprPackKForV1, 71

/* Size Assignments */
.set sgprSizeI, sgprSizesFree+0
.set sgprSizeJ, sgprSizesFree+1
.set sgprSizeK, sgprSizesFree+2
.set sgprSizeL, sgprSizesSum+0

/* Stride Assignments */
.set constStrideD0I, 1
.set sgprStrideD1J, sgprStridesD+0
.set sgprStrideDK, sgprStridesD+1
.set constStrideC0I, 1
.set sgprStrideC1J, sgprStridesC+0
.set sgprStrideCK, sgprStridesC+1
.set constStrideA0I, 1
.set sgprStrideAL, sgprStridesA+0
.set sgprStrideAK, sgprStridesA+1
.set constStrideBL, 1
.set sgprStrideB1J, sgprStridesB+0
.set sgprStrideBK, sgprStridesB+1

.set MT0, 256
.set MT1, 16
.set DepthU, 64
.set BpeA, 2
.set BpeALog2, 1
.set BpeB, 2
.set BpeBLog2, 1
.set BpeAGR, 1
.set BpeAGRLog2, 0
.set BpeBGR, 2
.set BpeBGRLog2, 1
/* Number of elements to shift-left SRD */
.set SrdShiftLeftA, 16
.set SrdShiftLeftB, 4
/* 2GB limit - set offsets to -1 to exceed this and clamp */
.set BufferLimit, 0xffffffff
.set BufferOOB, 0x80000000

/******************************************/
/* Bits 127:96 of SRD.                    */
/* hex: 0x00020000                        */
/* dst_sel_x (3b): 0                      */
/* dst_sel_y (3b): 0                      */
/* dst_sel_z (3b): 0                      */
/* dst_sel_w (3b): 0                      */
/* num_format (3b): 0                     */
/* data_format (4b): 4                    */
/* user_vm_enable (1b): 0                 */
/* user_vm_mode (1b): 0                   */
/* index_stride (2b): 0                   */
/* add_tid_enable (1b): 0                 */
/* _unusedA (3b): 0                       */
/* nv (1b): 0                             */
/* _unusedB (2b): 0                       */
/* type (2b): 0                           */
/******************************************/
.set Srd127_96, 0x00020000

/* Global Offset A */
.macro GLOBAL_OFFSET_A vgprAddr:req vgprOffset0I:req vgprOffsetL:req vgprTmp:req
    v_mul_lo_u32 v[\vgprTmp+0] s[sgprStrideAL] v[\vgprOffsetL] // mul d1 lower
    v_add_co_u32 v[\vgprAddr+0] vcc v[\vgprOffset0I] v[\vgprTmp+0] // accumulate K lower
    v_add_u32 v[\vgprAddr+0] 0x10 v[\vgprAddr+0]     // add prepad for pointer shift
                                                       // offset *= bytes/element (multiplier is 1 do nothing)
.endm

/* Global Offset B */
.macro GLOBAL_OFFSET_B vgprAddr:req vgprOffsetL:req vgprOffset1J:req vgprTmp:req
    v_mul_lo_u32 v[\vgprTmp+0] s[sgprStrideB1J] v[\vgprOffset1J] // mul d1 lower
    v_add_co_u32 v[\vgprAddr+0] vcc v[\vgprOffsetL] v[\vgprTmp+0] // accumulate K lower
    v_add_u32 v[\vgprAddr+0] 0x4 v[\vgprAddr+0]      // add prepad for pointer shift
    v_lshlrev_b32 v[\vgprAddr+0] 0x1 v[\vgprAddr+0]  // offset *= bytes/element
.endm

/* Dynamic Scalar Divide: vQuotient=vDividend/vDivisor; vRemainder=vDividend%vDivisor; */
.macro DYNAMIC_VECTOR_DIVIDE vQuotient vRemainder vDividend vDivisor vTmp0 vTmp1 sTmp
    v_cvt_f32_u32 v[\vQuotient] v[\vDivisor]
    v_rcp_f32 v[\vQuotient] v[\vQuotient]
    v_mul_f32 v[\vQuotient] 0x4f800000 v[\vQuotient]
    v_cvt_u32_f32 v[\vQuotient] v[\vQuotient]
    v_mul_lo_u32 v[\vRemainder] v[\vDivisor] v[\vQuotient]
    v_mul_hi_u32 v[\vTmp0] v[\vDivisor] v[\vQuotient]
    v_sub_co_u32 v[\vTmp1] vcc 0x0 v[\vRemainder]
    v_cmp_ne_i32 s[\sTmp:\sTmp+1] 0x0 v[\vTmp0]
    v_cndmask_b32 v[\vRemainder] v[\vTmp1] v[\vRemainder] s[\sTmp:\sTmp+1]
    v_mul_hi_u32 v[\vRemainder] v[\vRemainder] v[\vQuotient]
    v_sub_co_u32 v[\vTmp0] vcc v[\vQuotient] v[\vRemainder]
    v_add_co_u32 v[\vQuotient] vcc v[\vQuotient] v[\vRemainder]
    v_cndmask_b32 v[\vQuotient] v[\vQuotient] v[\vTmp0] s[\sTmp:\sTmp+1]
    v_mul_hi_u32 v[\vQuotient] v[\vQuotient] v[\vDividend]
    v_mul_lo_u32 v[\vRemainder] v[\vQuotient] v[\vDivisor]
    v_sub_co_u32 v[\vTmp0] vcc v[\vDividend] v[\vRemainder]
    v_cmp_ge_u32 s[\sTmp:\sTmp+1] v[\vDividend] v[\vRemainder]
    v_add_co_u32 v[\vRemainder] vcc 0x1 v[\vQuotient]
    v_add_co_u32 v[\vTmp1] vcc -1 v[\vQuotient]
    v_cmp_le_u32 vcc v[\vDivisor] v[\vTmp0]
    s_and_b64 vcc s[\sTmp:\sTmp+1] vcc
    v_cndmask_b32 v[\vQuotient] v[\vQuotient] v[\vRemainder] vcc
    v_cndmask_b32 v[\vQuotient] v[\vTmp1] v[\vQuotient] s[\sTmp:\sTmp+1]
    v_cmp_ne_i32 vcc 0x0 v[\vDivisor]
    v_cndmask_b32 v[\vQuotient] -1 v[\vQuotient] vcc // final result
    v_mul_lo_u32 v[\vRemainder] v[\vQuotient] v[\vDivisor]
    v_sub_co_u32 v[\vRemainder] vcc v[\vDividend] v[\vRemainder] // final result
.endm

/******************************************/
/* Allocate Resources                     */
/******************************************/

/* Load Kernel Args */
s_load_dwordx16 s[28:43], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x0
s_load_dwordx4 s[44:47], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x40
s_load_dwordx2 s[48:49], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x50
s_load_dword s50, s[sgprKernArgAddress:sgprKernArgAddress+1], 0x58
s_branch label_common_kernel_entry

/* pad 55 snops to satisfy 0x100 code size for Preload Backward Compatibility Prologue */
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_load_dword s39, s[sgprKernArgAddress:sgprKernArgAddress+1], 0x2c
s_load_dwordx8 s[40:47], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x30
s_load_dwordx2 s[48:49], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x50
s_load_dword s50, s[sgprKernArgAddress:sgprKernArgAddress+1], 0x58
s_mov_b32 s28, s2                                  // move preload data to correct sgpr
s_mov_b32 s29, s3                                  // move preload data to correct sgpr
s_mov_b32 s30, s4                                  // move preload data to correct sgpr
s_mov_b32 s31, s5                                  // move preload data to correct sgpr
s_mov_b32 s32, s6                                  // move preload data to correct sgpr
s_mov_b32 s33, s7                                  // move preload data to correct sgpr
s_mov_b32 s34, s8                                  // move preload data to correct sgpr
s_mov_b32 s35, s9                                  // move preload data to correct sgpr
s_mov_b32 s36, s10                                 // move preload data to correct sgpr
s_mov_b32 s37, s11                                 // move preload data to correct sgpr
s_mov_b32 s38, s12                                 // move preload data to correct sgpr
label_common_kernel_entry:  /// for both preload/non-preload common code
s_mov_b32 s[sgprWorkGroup0+0], s13                 // restore workgroup id
s_mov_b32 s[sgprWorkGroup0+1], s14                 // restore workgroup id
s_mov_b32 s[sgprWorkGroup0+2], s15                 // restore workgroup id
s_mov_b32 s[sgprPackKForV0], 0x05040100
s_mov_b32 s[sgprPackKForV1], 0x07060302
s_mov_b32 m0, 0x9a00                               // LDS clamp at 39424 bytes
v_mov_b32 v[vgprSerial], v0                        // thread serial id
/* init: add vgpr [0...48) to pool */
/* init: add vgpr [0...0) to pool */
/* init: add agpr [0...16) to pool */

/******************************************/
/* Local Read Addresses                   */
/******************************************/

/* local read addresses: tile assignments a/b */
/* lr0I */
v_and_b32 v1, 63, v[vgprSerial]                    // 0. thread id in wave: wtid = tid % wavelength(64)
v_and_b32 v0, 15, v1                               // 1. N offset: nIdx = wtid % MI_N(16)
                                                   // 1. N offset: nOffset = nIdx * nStride(1) (multiplier is 1, do nothing)
v_lshrrev_b32 v1, 4, v1                            // 2. block offset: bnIdx = wtid / dividedForBlkId(16)
v_and_b32 v1, 0, v1                                // 2. block offset: bnIdx = bnIdx % num1DBlocks(1)
v_lshlrev_b32 v1, 0x4, v1                          // 2. block offset: bnOffset = bnIdx * strideBlock(16)
v_add_u32 v0, v1, v0                               // 3. add N and block offset: bnOffset = block and N offset
v_lshlrev_b32 v0, 0x2, v0                          // 4. apply VectorWidth: bnOffset = bnOffset * vw(4)
v_and_b32 v1, 63, v[vgprSerial]                    // 5. thread id in wave: wtid = tid % wavelength(64)
v_lshrrev_b32 v1, 4, v1                            // 5. K offset: kIdx = wtid / (MIN(16) * MIBB(1))
v_lshlrev_b32 v1, 0xb, v1                          // 5. K offset: lrKOffset = kIdx * mStride(2048)
v_add_u32 v0, v1, v0                               // 6. offset in wave: lrOffset = bnOffset + lrKOffset
v_lshrrev_b32 v1, 6, v[vgprSerial]                 // 7. wave offset in N dimen: wtid = tid / dividedForWaveId(64)
v_and_b32 v1, 3, v1                                // 7. wave offset in M dimen: wtid0 = wtid / num1DWaves(4)
v_lshlrev_b32 v1, 0x6, v1                          // 7. wave offset in M dimen: wOffset = wtid0 * W0Stride(64)
v_add_u32 v0, v1, v0                               // 7. final local read offset: flrOffset = lrOffset + WOffset
/* lr1J */
v_and_b32 v2, 63, v[vgprSerial]                    // 0. thread id in wave: wtid = tid % wavelength(64)
v_and_b32 v1, 15, v2                               // 1. N offset: nIdx = wtid % MI_N(16)
v_lshlrev_b32 v1, 0x6, v1                          // 1. N offset: nOffset = nIdx * nStride(64)
v_lshrrev_b32 v2, 4, v2                            // 2. block offset: bnIdx = wtid / dividedForBlkId(16)
v_and_b32 v2, 0, v2                                // 2. block offset: bnIdx = bnIdx % num1DBlocks(1)
v_lshlrev_b32 v2, 0xa, v2                          // 2. block offset: bnOffset = bnIdx * strideBlock(1024)
v_add_u32 v1, v2, v1                               // 3. add N and block offset: bnOffset = block and N offset
                                                   // 4. apply VectorWidth: bnOffset = bnOffset * vw(1) (multiplier is 1, do nothing)
v_and_b32 v2, 63, v[vgprSerial]                    // 5. thread id in wave: wtid = tid % wavelength(64)
v_lshrrev_b32 v2, 4, v2                            // 5. K offset: kIdx = wtid / (MIN(16) * MIBB(1))
v_lshlrev_b32 v2, 0x3, v2                          // 5. K offset: lrKOffset = kIdx * mStride(8)
v_add_u32 v1, v2, v1                               // 6. offset in wave: lrOffset = bnOffset + lrKOffset

/* local read addresses: final offsets a */
v_lshrrev_b32 v2, 8, v[vgprSerial]                 // LSU offset: sgid = Serial / subGroup(256)
s_mov_b32 s51, 256                                 // LSU offset: stride = MT0(256) + PAD0(0)
v_mul_lo_u32 v2, s51, v2                           // LSU offset: lsuoffset = sgid*(MT0+PAD)
v_add_lshl_u32 v[vgprLocalReadAddrA], v2, v0, 0x1  // Final Offset: offset = (lro0*VW+lsuoffset)*bpe
v_lshrrev_b32 v3, 7, v[vgprLocalReadAddrA]         // Final Offset: padding 8 per block 128
v_lshlrev_b32 v3, 0x4, v3                          // Final Offset: padding 8 per block 128
v_add_u32 v[vgprLocalReadAddrA], v3, v[vgprLocalReadAddrA] // Final Offset: add padding 8 per block 128

/* local read addresses: final offsets b */
v_lshrrev_b32 v0, 8, v[vgprSerial]                 // LSU offset: sgid = Serial / subGroup(256)
s_mov_b32 s51, 16                                  // LSU offset: stride = MT1(16) + PAD1(0)
v_mul_lo_u32 v0, s51, v0                           // LSU offset: lsuoffset = sgid*(MT1+PAD)
v_add_lshl_u32 v[vgprLocalReadAddrB], v0, v1, 0x1  // Final Offset: offset = (lro1*VW+lsuoffset)*bpe
v_lshrrev_b32 v2, 7, v[vgprLocalReadAddrB]         // Final Offset: padding 16 per block 128
v_lshlrev_b32 v2, 0x5, v2                          // Final Offset: padding 16 per block 128
v_add_u32 v[vgprLocalReadAddrB], v2, v[vgprLocalReadAddrB] // Final Offset: add padding 16 per block 128

/* local read addresses: declare addresses a */
/* N/A */

/* local read addresses: declare addresses b */
v_add_co_u32 v[vgprLocalReadAddrB+0], vcc, 0x9000, v[vgprLocalReadAddrB+0] //  += LdsOffsetB (lower)

/******************************************/
/* Local Write Addresses                  */
/******************************************/
/* LVCA = 16 */
/* v1 = A-unroll = serial/LVCA */
v_and_b32 v4, 63, v[vgprSerial]                    // v4 = v[vgprSerial] % 64
v_lshrrev_b32 v1, 4, v4                            // v1 = v4 / 16
v_and_b32 v0, 15, v4                               // v0 = v4 % 16
v_lshrrev_b32 v4, 0x6, v[vgprSerial]               // WaveID
v_mov_b32 v5, 16                                   // Global Read Wave: add back to cloumn index
v_mul_lo_u32 v1, v5, v1                            // Global Read Wave: add back to cloumn index
v_add_u32 v1, v4, v1                               // Global Read Wave: add back to cloumn index
/* tile *= glvw */
v_lshlrev_b32 v0, 0x4, v0                          // v0 = v0 * 16
v_mov_b32 v4, v1                                   // copy for GlobalSplitU
/* LVCB = 16 */
/* v3 = B-unroll = serial%LVCB */
v_and_b32 v5, 63, v[vgprSerial]                    // v5 = v[vgprSerial] % 64
v_lshrrev_b32 v2, 4, v5                            // v2 = v5 / 16
v_and_b32 v3, 15, v5                               // v3 = v5 % 16
v_readfirstlane_b32 s51, v[vgprSerial]             // WaveIdxWavefrontWidth
s_lshr_b32 s51, s51, 0x6                           // WaveId
s_mul_i32 s51, s51, 4                              // Each wave loads continuous lsp(4)*nrp(1) columns
v_add_u32 v2, s51, v2                              // Add back to column index
/* unroll *= glvw */
v_lshlrev_b32 v3, 0x2, v3                          // v3 = v3 * 4
v_mov_b32 v5, v3                                   // copy for GlobalSplitU
/* lwaUnrollAssignmentA = v4 */
/* lwaUnrollAssignmentB = v5 */

/* local write addresses: first offset a */
v_mul_u32_u24 v[vgprLocalWriteAddrA], 0x100, v4    // lwAL**(MTA + PAD)
v_add_lshl_u32 v[vgprLocalWriteAddrA], v0, v[vgprLocalWriteAddrA], 0x1 // lwFOA = (lwAA + lwAL*(MT0I+PAD))*bpe
v_lshrrev_b32 v6, 7, v[vgprLocalWriteAddrA]        // padding 8 per block 128
v_lshlrev_b32 v6, 0x4, v6                          // padding 8 per block 128
v_add_u32 v[vgprLocalWriteAddrA], v6, v[vgprLocalWriteAddrA] // add padding 8 per block 128

/* local write addresses: first offset b */
v_mul_u32_u24 v[vgprLocalWriteAddrB], 0x40, v2     // lwBL**(DepthU_Compute + PAD)
v_add_lshl_u32 v[vgprLocalWriteAddrB], v5, v[vgprLocalWriteAddrB], 0x1 // lwFOB = (lwBB + lwBL*(DepthU+PAD))*bpe
v_lshrrev_b32 v6, 7, v[vgprLocalWriteAddrB]        // padding 16 per block 128
v_lshlrev_b32 v6, 0x5, v6                          // padding 16 per block 128
v_add_u32 v[vgprLocalWriteAddrB], v6, v[vgprLocalWriteAddrB] // add padding 16 per block 128
v_add_co_u32 v[vgprLocalWriteAddrB], vcc, 0x9000, v[vgprLocalWriteAddrB] // lwFOB = lwB1J + lwBL*MT1J + LDS_OFFSET_B=18432*2
s_waitcnt lgkmcnt(0)                               // wait for 48 bytes of kern args
s_and_b32 s[sgprWGM], s[sgprGSU], 0xff00
s_lshr_b32 s[sgprWGM], s[sgprWGM], 0x8
s_and_b32 s[sgprGSU], s[sgprGSU], 0xff
label_stop:
v_mov_b32 v8, MT0                                  // set MT0 into sgpr
v_mov_b32 v7, s[sgprSizesFree+0]                   // set Free0 size
v_cvt_f32_u32 v6, v8                               // v6 = ceil(v7 / v8)
v_rcp_iflag_f32 v6, v6                             // v6 = ceil(v7 / v8)
v_cvt_f32_u32 v9, v7                               // v6 = ceil(v7 / v8)
v_mul_f32 v6, v6, v9                               // v6 = ceil(v7 / v8)
v_cvt_u32_f32 v6, v6                               // v6 = ceil(v7 / v8)
v_mul_u32_u24 v9, v6, v8                           // v6 = ceil(v7 / v8)
v_sub_u32 v9, v7, v9                               // v6 = ceil(v7 / v8)
v_cmp_ne_u32 vcc, v9, 0                            // v6 = ceil(v7 / v8)
v_addc_co_u32 v6, vcc, v6, 0, vcc                  // ceil
v_mov_b32 v8, MT1                                  // set MT1 into sgpr
v_mov_b32 v7, s[sgprSizesFree+1]                   // set Free1 size
v_readfirstlane_b32 s[sgprNumWorkGroups0], v6      // set back to numWorkGroup0
v_cvt_f32_u32 v6, v8                               // v6 = ceil(v7 / v8)
v_rcp_iflag_f32 v6, v6                             // v6 = ceil(v7 / v8)
v_cvt_f32_u32 v9, v7                               // v6 = ceil(v7 / v8)
v_mul_f32 v6, v6, v9                               // v6 = ceil(v7 / v8)
v_cvt_u32_f32 v6, v6                               // v6 = ceil(v7 / v8)
v_mul_u32_u24 v9, v6, v8                           // v6 = ceil(v7 / v8)
v_sub_u32 v9, v7, v9                               // v6 = ceil(v7 / v8)
v_cmp_ne_u32 vcc, v9, 0                            // v6 = ceil(v7 / v8)
v_addc_co_u32 v6, vcc, v6, 0, vcc                  // ceil
s_nop 0                                            // 1 wait states
v_readfirstlane_b32 s[sgprNumWorkGroups1], v6      // set back to numWorkGroup1
s_sub_u32 s[sgprAddressA+0], s[sgprAddressA+0], 16 // pre-pad to make room for possible pointer shift
s_subb_u32 s[sgprAddressA+1], s[sgprAddressA+1], 0 // pre-pad to make room for possible pointer shift
s_sub_u32 s[sgprAddressB+0], s[sgprAddressB+0], 8  // pre-pad to make room for possible pointer shift
s_subb_u32 s[sgprAddressB+1], s[sgprAddressB+1], 0 // pre-pad to make room for possible pointer shift

/* Short circuit condition if Alpha == 0, then sumDims=0 */
v_cmp_eq_f32 vcc, s[sgprAlpha], 0.0                // s[Alpha] == 0.0f ?
s_cbranch_vccz label_AlphaNonZero                  // branch if s[Alpha] != 0
s_mov_b32 s[sgprSizesSum+0], 0x0                   // Set summation dim=0 if Alpha == 0
label_AlphaNonZero:

/******************************************/
/* Begin setupNewTile                     */
/******************************************/

/* global read addresses: work-group */
/* graWorkGroup mapping */
s_cmp_eq_u32 s[sgprGSU], 1                         // GSU == 1 ?
s_cbranch_scc1 label_GSU                           // branch if GSU == 1
// GSU-not-WGMapRR :nwg1 = (size1J + MT1J - 1) / MT1J;
v_cvt_f32_u32 v6, s[sgprGSU]                       // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s[sgprGSU]
v_rcp_iflag_f32 v6, v6                             // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s[sgprGSU]
v_cvt_f32_u32 v7, s[sgprWorkGroup1]                // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s[sgprGSU]
v_mul_f32 v6, v6, v7                               // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s[sgprGSU]
v_cvt_u32_f32 v6, v6                               // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s[sgprGSU]
v_mul_u32_u24 v7, v6, s[sgprGSU]                   // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s[sgprGSU]
v_sub_u32 v7, s[sgprWorkGroup1], v7                // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s[sgprGSU]
v_cmpx_eq_u32 exec, v7, s[sgprGSU]                 // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s[sgprGSU]
v_add_u32 v6, 1, v6                                // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s[sgprGSU]
v_mov_b32 v7, 0                                    // s[sgprGSUSumIdx] = s[sgprWorkGroup1] % s[sgprGSU]
s_mov_b64 exec, -1                                 // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s[sgprGSU]
v_readfirstlane_b32 s[sgprWorkGroup1], v6
v_readfirstlane_b32 s[sgprGSUSumIdx], v7
s_mov_b32 s[sgprGSULog2BpeC], 1
s_mov_b32 s[sgprGSULog2BpeD], 2
s_branch label_GSU_End
label_GSU:
s_mov_b64 s[sgprGSUSumIdx:sgprGSUSumIdx+1], 0      // Set GSUSumIdx to 0
s_mov_b32 s[sgprGSULog2BpeC], 1
s_mov_b32 s[sgprGSULog2BpeD], 1
label_GSU_End:
s_cmp_le_u32 s[sgprWGM], 1                         // WGM <= 1 ?
s_cbranch_scc1 label_WGM                           // branch if WGM <= 1
v_cvt_f32_u32 v6, s[sgprWGM]                       // WGM
v_rcp_iflag_f32 v6, v6                             // WGM
v_cvt_f32_u32 v7, s[sgprWorkGroup1]                // WGM
v_mul_f32 v6, v6, v7                               // WGM
v_cvt_u32_f32 v6, v6                               // WGM
v_mul_u32_u24 v7, v6, s[sgprWGM]                   // WGM
v_sub_u32 v7, s[sgprWorkGroup1], v7                // WGM
v_cmpx_eq_u32 exec, v7, s[sgprWGM]                 // WGM
v_add_u32 v6, 1, v6                                // WGM
s_mov_b64 exec, -1                                 // WGM
v_readfirstlane_b32 s74, v6
s_mul_i32 s75, s74, s[sgprWGM]                     // quotient * non-magic divisor
s_sub_u32 s75, s[sgprWorkGroup1], s75              // WorkGroup1=remainder
s_mul_i32 s75, s75, s[sgprNumWorkGroups0]          // (wg1 % WGM)*nwg0
s_add_u32 s75, s75, s[sgprWorkGroup0]              // wgSerial = wg0 + (wg1 % WGM)*nwg0
v_cvt_f32_u32 v6, s[sgprWGM]                       // WGM
v_rcp_iflag_f32 v6, v6                             // WGM
v_cvt_f32_u32 v7, s[sgprNumWorkGroups1]            // WGM
v_mul_f32 v6, v6, v7                               // WGM
v_cvt_u32_f32 v6, v6                               // WGM
v_mul_u32_u24 v7, v6, s[sgprWGM]                   // WGM
v_sub_u32 v7, s[sgprNumWorkGroups1], v7            // WGM
v_cmpx_eq_u32 exec, v7, s[sgprWGM]                 // WGM
v_add_u32 v6, 1, v6                                // WGM
s_mov_b64 exec, -1                                 // WGM
v_readfirstlane_b32 s72, v6
s_mul_i32 s73, s[sgprWGM], s72                     // quotient * non-magic divisor
s_sub_u32 s76, s[sgprNumWorkGroups1], s73          // WorkGroup1=remainder
s_cmp_eq_u32 s76, 0                                // remainder == 0 ?
s_cmov_b32 s76, s[sgprWGM]                         // remainder = WGM if remainder == 0
s_cmp_ge_u32 s74, s72                              // blockId >= numFullBlocks ?
s_cselect_b32 s72, s76, s[sgprWGM]
v_cvt_f32_u32 v6, s72                              // s[sgprWorkGroup0] = s75 / s72
v_rcp_iflag_f32 v6, v6                             // s[sgprWorkGroup0] = s75 / s72
v_cvt_f32_u32 v7, s75                              // s[sgprWorkGroup0] = s75 / s72
v_mul_f32 v6, v6, v7                               // s[sgprWorkGroup0] = s75 / s72
v_cvt_u32_f32 v6, v6                               // s[sgprWorkGroup0] = s75 / s72
v_mul_u32_u24 v7, v6, s72                          // s[sgprWorkGroup0] = s75 / s72
v_sub_u32 v7, s75, v7                              // s[sgprWorkGroup0] = s75 / s72
v_cmpx_eq_u32 exec, v7, s72                        // s[sgprWorkGroup0] = s75 / s72
v_add_u32 v6, 1, v6                                // s[sgprWorkGroup0] = s75 / s72
v_mov_b32 v7, 0                                    // s[sgprWorkGroup1] = s75 % s72
s_mov_b64 exec, -1                                 // s[sgprWorkGroup0] = s75 / s72
v_readfirstlane_b32 s[sgprWorkGroup0], v6
v_readfirstlane_b32 s[sgprWorkGroup1], v7
s_mul_i32 s74, s74, s[sgprWGM]                     // blockId * WGM
s_add_u32 s[sgprWorkGroup1], s[sgprWorkGroup1], s74 // wg1 += blockId * WGM
label_WGM:

/* global read addresses: tile offset assignment a */
/* graTileAssignmentA = v0 */

/* global read addresses: tile offset assignment b */
/* graTileAssignmentB = v2 */

/* global read addresses: unroll assignment a */
/* v1 */

/* global read addresses: unroll assignment b */
/* v3 */

/* global read addresses: other free assignments */
/* s[sgprWorkGroup2] */

/* global read addresses: tile offsets a */
v_mov_b32 v6, v0                                   // groA0I_0

/* global read addresses: tile offsets b */
v_mov_b32 v7, v2                                   // groB1J_0

/* global read addresses: unroll offsets a */
v_mov_b32 v8, v1                                   // groAL_0
v_add_co_u32 v9, vcc, 4, v8                        // groAL_1 + LSPA
v_add_co_u32 v10, vcc, 4, v9                       // groAL_2 + LSPA
v_add_co_u32 v11, vcc, 4, v10                      // groAL_3 + LSPA

/* global read addresses: unroll offsets b */
v_mov_b32 v12, v3                                  // groBL_0

/* global read addresses: shift a */
s_mul_i32 s72, s[sgprWorkGroup0], 256              // WorkGroup[01] * MT
s_sub_u32 s72, s[sgprSizeI], s72                   // edge = Size0I - WG*MT
s_sub_u32 s72, s72, 16                             // edge -= margin(16)
v_mov_b32 v13, s72                                 // edge vgpr = Size0I- WG*MT - margin(16)
v_min_i32 v6, v13, v6                              // offset = (offset < edge) ? offset(v6) : edge(v13)

/* global read addresses: final offsets a */
GLOBAL_OFFSET_A vgprGlobalReadOffsetA+0,  6,  8, 13 // gROA_0_0_0_0
GLOBAL_OFFSET_A vgprGlobalReadOffsetA+1,  6,  9, 13 // gROA_0_0_1_0
GLOBAL_OFFSET_A vgprGlobalReadOffsetA+2,  6, 10, 13 // gROA_0_0_2_0
GLOBAL_OFFSET_A vgprGlobalReadOffsetA+3,  6, 11, 13 // gROA_0_0_3_0

/* global read addresses: final offsets b */
GLOBAL_OFFSET_B vgprGlobalReadOffsetB+0, 12,  7, 8 // gROB_0_0_0_0

/* global read addresses: addresses a */
/* max read offset = size[n] * stride[n-1] */
s_mul_hi_u32 s75, s[sgprWorkGroup0], 256           // WorkGroup[01] * MT
s_mul_i32 s74, s[sgprWorkGroup0], 256              // WorkGroup[01] * MT
s_mul_hi_u32 s73, 64, s[sgprGSUSumIdx]             // gsuOffset = DepthU*bpeGR*GSUSumIdx
s_mul_i32 s72, 64, s[sgprGSUSumIdx]                // gsuOffset = DepthU*bpeGR*GSUSumIdx
s_mul_hi_u32 s73, s72, s[sgprStrideAL]             // tlu=1, scaled unroll-offset by stride
s_mul_i32 s72, s72, s[sgprStrideAL]                // tlu=1, scaled unroll-offset by stride
s_add_u32 s74, s74, s72                            // accum GsuOffset term to tilestart
s_addc_u32 s75, s75, s73                           // accum GsuOffset term to tilestart
s_mov_b32 s[sgprShadowLimitA+0], 1                 // Init tensor size
s_mov_b32 s[sgprShadowLimitA+1], 0                 // init tensor size
s_sub_u32 s72, s[sgprSizeI], 1                     // (size-1)
s_mul_hi_u32 s73, constStrideA0I, s72              // stride x (size-1)
s_mul_i32 s72, constStrideA0I, s72                 // stride x (size-1)
s_add_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s72 // sum tensor size
s_addc_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s73 // sum tensor size
s_sub_u32 s72, s[sgprSizeL], 1                     // (size-1)
s_mul_hi_u32 s73, s[sgprStrideAL], s72             // stride x (size-1)
s_mul_i32 s72, s[sgprStrideAL], s72                // stride x (size-1)
s_add_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s72 // sum tensor size
s_addc_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s73 // sum tensor size
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s74 // sub tileStart
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s75 // sub tileStart
                                                   // Set limit to use bytes (byte is 1, do nothing)
s_add_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], 16 // extend limit for pre-pad
s_addc_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], 0 // extend limit for pre-pad
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
s_mul_hi_u32 s73, s[sgprStrideAK], s[sgprWorkGroup2] // Stride*WG
s_mul_i32 s72, s[sgprStrideAK], s[sgprWorkGroup2]  // Stride*WG
s_add_u32 s74, s74, s72                            // accum wg term to tilestart
s_addc_u32 s75, s75, s73                           // accum wg term to tilestart
                                                   // tileStart *= BPE (multiplier is 1, do nothing)
s_add_u32 s[sgprSrdA+0], s[sgprAddressA+0], s74    // SRD base = Address+ tileStart0
s_addc_u32 s[sgprSrdA+1], s[sgprAddressA+1], s75   // SRD base = Address+ tileStart1
s_mov_b32 s[sgprSrdA+3], Srd127_96                 // Set bits 127_96 in SRD

/* global read addresses: addresses b */
/* max read offset = size[n] * stride[n-1] */
s_mul_hi_u32 s75, s[sgprWorkGroup1], 16            // WorkGroup[01] * MT
s_mul_i32 s74, s[sgprWorkGroup1], 16               // WorkGroup[01] * MT
s_mul_hi_u32 s75, s74, s[sgprStrideB1J]            // tlu=0, scaled tile-offset by stride
s_mul_i32 s74, s74, s[sgprStrideB1J]               // tlu=0, scaled tile-offset by stride
s_mul_hi_u32 s73, 64, s[sgprGSUSumIdx]             // gsuOffset = DepthU*bpeGR*GSUSumIdx
s_mul_i32 s72, 64, s[sgprGSUSumIdx]                // gsuOffset = DepthU*bpeGR*GSUSumIdx
s_add_u32 s74, s74, s72                            // accum GsuOffset term to tilestart
s_addc_u32 s75, s75, s73                           // accum GsuOffset term to tilestart
s_mov_b32 s[sgprShadowLimitB+0], 1                 // Init tensor size
s_mov_b32 s[sgprShadowLimitB+1], 0                 // init tensor size
s_sub_u32 s72, s[sgprSizeL], 1                     // (size-1)
s_mul_hi_u32 s73, constStrideBL, s72               // stride x (size-1)
s_mul_i32 s72, constStrideBL, s72                  // stride x (size-1)
s_add_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s72 // sum tensor size
s_addc_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s73 // sum tensor size
s_sub_u32 s72, s[sgprSizeJ], 1                     // (size-1)
s_mul_hi_u32 s73, s[sgprStrideB1J], s72            // stride x (size-1)
s_mul_i32 s72, s[sgprStrideB1J], s72               // stride x (size-1)
s_add_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s72 // sum tensor size
s_addc_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s73 // sum tensor size
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s74 // sub tileStart
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s75 // sub tileStart
s_lshl_b64 s[sgprShadowLimitB:sgprShadowLimitB+1], s[sgprShadowLimitB:sgprShadowLimitB+1], 0x1 // Set limit to use bytes
s_add_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], 8 // extend limit for pre-pad
s_addc_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], 0 // extend limit for pre-pad
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32
s_mul_hi_u32 s73, s[sgprStrideBK], s[sgprWorkGroup2] // Stride*WG
s_mul_i32 s72, s[sgprStrideBK], s[sgprWorkGroup2]  // Stride*WG
s_add_u32 s74, s74, s72                            // accum wg term to tilestart
s_addc_u32 s75, s75, s73                           // accum wg term to tilestart
s_lshl_b64 s[74:75], s[74:75], 0x1                 // tileStart *= BPE
s_add_u32 s[sgprSrdB+0], s[sgprAddressB+0], s74    // SRD base = Address+ tileStart0
s_addc_u32 s[sgprSrdB+1], s[sgprAddressB+1], s75   // SRD base = Address+ tileStart1
s_mov_b32 s[sgprSrdB+3], Srd127_96                 // Set bits 127_96 in SRD
s_mul_i32 s72, s[sgprGSU], DepthU*BpeAGR
s_mul_i32 s[sgprGlobalReadIncsA+0], s72, s[sgprStrideAL] // incrA unrollIdx)

/* global read addresses: increments b */
s_mul_i32 s72, s[sgprGSU], DepthU*BpeBGR
s_mov_b32 s[sgprGlobalReadIncsB+0], s72            // incrB (unrollIdx)
/* declare loop num iterations */
s_lshr_b32 s[sgprLoopCounterL], s[sgprSizesSum+0], 6 // s[sgprLoopCounterL] = s[sgprSizesSum+0] / 64
s_cmp_eq_u32 s[sgprGSU], 1                         // GSU == 1 ?
s_cbranch_scc1 label_GSU_1                         // branch if GSU == 1
v_cvt_f32_u32 v0, s[sgprGSU]                       // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSU]
v_rcp_iflag_f32 v0, v0                             // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSU]
v_cvt_f32_u32 v1, s[sgprLoopCounterL]              // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSU]
v_mul_f32 v0, v0, v1                               // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSU]
v_cvt_u32_f32 v0, v0                               // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSU]
v_mul_u32_u24 v1, v0, s[sgprGSU]                   // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSU]
v_sub_u32 v1, s[sgprLoopCounterL], v1              // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSU]
v_cmpx_eq_u32 exec, v1, s[sgprGSU]                 // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSU]
v_add_u32 v0, 1, v0                                // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSU]
v_mov_b32 v1, 0                                    // s[sgprGSUSumIdx+1] = s[sgprLoopCounterL] % s[sgprGSU]
s_mov_b64 exec, -1                                 // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSU]
v_readfirstlane_b32 s[sgprLoopCounterL], v0
v_readfirstlane_b32 s[sgprGSUSumIdx+1], v1
s_add_u32 s72, 1, s[sgprLoopCounterL]              // tmp<-numIterMyWg+
s_cmp_lt_u32 s[sgprGSUSumIdx], s[sgprGSUSumIdx+1]  // gsuSumIdx < numIterPerWgRemainder
s_cmov_b32 s[sgprLoopCounterL], s72                // numIterMyWg++ if needed
label_GSU_1:
s_mov_b32 s[sgprOrigLoopCounter], s[sgprLoopCounterL] // copy loop counter
s_mov_b32 s72, 0x4                                 // init staggerU
label_beginStaggerUIter:
s_lshl_b32 s73, s72, 2                             // shift by StaggerUStride
s_cmp_ge_u32 s[sgprOrigLoopCounter], s73           // loopCount >= current shift Count
s_cbranch_scc1 label_endStaggerUIter               // jump to end
s_lshr_b32 s72, s72, 1                             // step down to smaller stagger
s_branch label_beginStaggerUIter                   // jump to begin
label_endStaggerUIter:
s_sub_u32 s73, s72, 1                              // staggerU mask
s_cmp_ge_u32 s72, 1                                // if current staggerU >= 1
s_cselect_b32 s[sgprStaggerUIter], s73, 0          // set Mask
s_and_b32 s[sgprStaggerUIter], s[sgprStaggerUIter], s[sgprWorkGroup1] // Compute actual stagger start for this tile
s_lshl_b32 s[sgprStaggerUIter], s[sgprStaggerUIter], 2 // shift by StaggerUStride

/* SRDs += (StaggerUIter) * GlobalReadIncsA+0 */
s_mul_hi_i32 s73, s[sgprStaggerUIter], s[sgprGlobalReadIncsA+0] //  stagger byte offset
s_mul_i32 s72, s[sgprStaggerUIter], s[sgprGlobalReadIncsA+0] //  stagger byte offset
s_mul_hi_i32 s[sgprWrapUA+1], s[sgprLoopCounterL], s[sgprGlobalReadIncsA+0] // Number of bytes accessed by the unroll loop
s_mul_i32 s[sgprWrapUA+0], s[sgprLoopCounterL], s[sgprGlobalReadIncsA+0] // Number of bytes accessed by the unroll loop
s_sub_u32 s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0], s[sgprWrapUA+0] // remove one iteration
s_subb_u32 s[sgprWrapUA+1], 0, s[sgprWrapUA+1]     // remove one iteration
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s72        // gra SRD += inc(lower)
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s73       // gra SRD += inc(upper)
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s72 // limit -= inc)
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s73 // limit -= inc)
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32

/* SRDs += (StaggerUIter) * GlobalReadIncsB+0 */
s_mul_hi_i32 s73, s[sgprStaggerUIter], s[sgprGlobalReadIncsB+0] //  stagger byte offset
s_mul_i32 s72, s[sgprStaggerUIter], s[sgprGlobalReadIncsB+0] //  stagger byte offset
s_mul_hi_i32 s[sgprWrapUB+1], s[sgprLoopCounterL], s[sgprGlobalReadIncsB+0] // Number of bytes accessed by the unroll loop
s_mul_i32 s[sgprWrapUB+0], s[sgprLoopCounterL], s[sgprGlobalReadIncsB+0] // Number of bytes accessed by the unroll loop
s_sub_u32 s[sgprWrapUB+0], s[sgprGlobalReadIncsB+0], s[sgprWrapUB+0] // remove one iteration
s_subb_u32 s[sgprWrapUB+1], 0, s[sgprWrapUB+1]     // remove one iteration
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s72        // gra SRD += inc(lower)
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s73       // gra SRD += inc(upper)
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s72 // limit -= inc)
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s73 // limit -= inc)
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32
s_add_u32 s[sgprStaggerUIter], s[sgprStaggerUIter], 2 // Subtract (PGR-1); StaggerUIter now contains target iteration to wrap
/* local read addresses: init pointers a */

/* localReadInitPointers */
/* local read addresses: init pointers b */

/* localReadInitPointers */

/* prefetch: global -> local */
s_cmp_eq_u32 s[sgprLoopCounterL], 0                // at last iteration?
s_cbranch_scc1 label_ShadowInitStart               // skip to ShadowInitStart iter b/c numIter==0
buffer_load_dwordx4 v[vgprG2LA+4:vgprG2LA+4+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0, sc0 // G -> Reg 0_0_0_0
buffer_load_dwordx4 v[vgprG2LA+12:vgprG2LA+12+3], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0, sc0 // G -> Reg 0_0_1_0
buffer_load_dwordx4 v[vgprG2LA+20:vgprG2LA+20+3], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0, sc0 // G -> Reg 0_0_2_0
buffer_load_dwordx4 v[vgprG2LA+28:vgprG2LA+28+3], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0, sc0 // G -> Reg 0_0_3_0
buffer_load_dwordx2 v[vgprG2LB+0:vgprG2LB+0+1], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_0_0

/* global read inc A loopL */
s_add_u32 s74, s[sgprLoopCounterL], 1              // remove pf(1)
s_cmp_eq_u32 s[sgprStaggerUIter], s74              // Is this wrapIter? (pf)
s_cselect_b32 s72, s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0] // incLower <- ?
s_cselect_b32 s73, s[sgprWrapUA+1], 0              // incUpper <- ?
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s72        // gra SRD += inc(lower)
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s73       // gra SRD += inc(upper)
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s72 // limit -= inc)
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s73 // limit -= inc)
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32

/* global read inc B loopL */
s_add_u32 s74, s[sgprLoopCounterL], 1              // remove pf(1)
s_cmp_eq_u32 s[sgprStaggerUIter], s74              // Is this wrapIter? (pf)
s_cselect_b32 s72, s[sgprWrapUB+0], s[sgprGlobalReadIncsB+0] // incLower <- ?
s_cselect_b32 s73, s[sgprWrapUB+1], 0              // incUpper <- ?
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s72        // gra SRD += inc(lower)
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s73       // gra SRD += inc(upper)
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s72 // limit -= inc)
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s73 // limit -= inc)
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32

/******************************************/
/* End setupNewTile                       */
/******************************************/
label_ShadowInitStart:
s_mov_b32 s[sgprSrdD+0], s[sgprAddressD+0]         // init SRD base address (lower)
s_mov_b32 s[sgprSrdD+1], s[sgprAddressD+1]         // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdD+2], 0x80000000
s_mov_b32 s[sgprSrdD+3], Srd127_96                 // Set bits 127_96 in post-loop SRD

s_mov_b32 s[sgprSrdC+0], s[sgprAddressC+0]         // init SRD base address (lower)
s_mov_b32 s[sgprSrdC+1], s[sgprAddressC+1]         // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdC+2], 0x80000000
s_mov_b32 s[sgprSrdC+3], Srd127_96                 // Set bits 127_96 in post-loop SRD


s_mul_i32 s74, MT1, s[sgprWorkGroup1]              // <- wg1*MT1
s_mul_hi_u32 s73, s74, s[sgprStrideC1J]            // ScaleC s74 by Stride
s_mul_i32 s72, s74, s[sgprStrideC1J]               // ScaleC s74 by Stride
s_lshl_b64 s[72:73], s[72:73], s[sgprGSULog2BpeC]  // scale by bpe
s_add_u32 s[sgprSrdC+0], s[sgprAddressC+0], s72    // add lo to SRD
s_addc_u32 s[sgprSrdC+1], s[sgprAddressC+1], s73   // add hi to SRD
s_mul_hi_u32 s73, s74, s[sgprStrideD1J]            // ScaleD s74 by Stride
s_mul_i32 s72, s74, s[sgprStrideD1J]               // ScaleD s74 by Stride
s_lshl_b64 s[72:73], s[72:73], s[sgprGSULog2BpeD]  // scale by bpe
s_add_u32 s[sgprSrdD+0], s[sgprAddressD+0], s72    // add lo to SRD
s_addc_u32 s[sgprSrdD+1], s[sgprAddressD+1], s73   // add hi to SRD

s_mul_hi_u32 s73, s[sgprWorkGroup2], s[sgprStrideCK] // ScaleC s[sgprWorkGroup2] by Stride
s_mul_i32 s72, s[sgprWorkGroup2], s[sgprStrideCK]  // ScaleC s[sgprWorkGroup2] by Stride
s_lshl_b64 s[72:73], s[72:73], s[sgprGSULog2BpeC]  // scale by bpe
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s72        // add lo to SRD
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], s73       // add hi to SRD
s_mul_hi_u32 s73, s[sgprWorkGroup2], s[sgprStrideDK] // ScaleD s[sgprWorkGroup2] by Stride
s_mul_i32 s72, s[sgprWorkGroup2], s[sgprStrideDK]  // ScaleD s[sgprWorkGroup2] by Stride
s_lshl_b64 s[72:73], s[72:73], s[sgprGSULog2BpeD]  // scale by bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s72        // add lo to SRD
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], s73       // add hi to SRD


s_mov_b32 s[sgprWSDstart+0], s[sgprSrdD+0]         // recode workspace start
s_mov_b32 s[sgprWSDstart+1], s[sgprSrdD+1]         // recode workspace start
s_cmp_eq_u32 s[sgprGSU], 1                         // GSU == 1 ?
s_cbranch_scc1 label_GSU_2                         // branch if GSU == 1
// GSU Output Buffer offset: Free0 + (Free1-1)*StrideC1J + (Free2-1)*StrideCK * GSUIdx * bpe%s
s_mul_hi_u32 s73, s[sgprSizesFree+0], s[sgprGSUSumIdx] // Free0
s_mul_i32 s72, s[sgprSizesFree+0], s[sgprGSUSumIdx] // Free0
s_sub_u32 s76, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s76, s76, s[sgprGSUSumIdx]               // Free1
s_mul_hi_u32 s75, s76, s[sgprStrideC1J]            // Free1
s_mul_i32 s74, s76, s[sgprStrideC1J]               // Free1
s_add_u32 s72, s72, s74                            // Free1
s_addc_u32 s73, s73, s75                           // Free1
s_sub_u32 s76, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s76, s76, s[sgprGSUSumIdx]               // Free2
s_mul_hi_u32 s75, s76, s[sgprStrideCK]             // Free2
s_mul_i32 s74, s76, s[sgprStrideCK]                // Free2
s_add_u32 s72, s72, s74                            // Free2
s_addc_u32 s73, s73, s75                           // Free2
s_lshl_b64 s[72:73], s[72:73], 2                   // scale by bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s72        // add lo GSU offset to SRD
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], s73       // add hi GSU offset to SRD
label_GSU_2:
.set sgprGSULog2BpeC, UNDEF

/* initC: remove ValuC vgpr buffer [0...0) from pool */

/* initC: remove acc vgpr buffer [0...16) from pool */

/* initC: remove ValuA/B vgpr buffer [0...48) from pool */
v_accvgpr_write acc0, 0x0                          // initC
v_accvgpr_write acc1, 0x0                          // initC
v_accvgpr_write acc2, 0x0                          // initC
v_accvgpr_write acc3, 0x0                          // initC
v_accvgpr_write acc4, 0x0                          // initC
v_accvgpr_write acc5, 0x0                          // initC
v_accvgpr_write acc6, 0x0                          // initC
v_accvgpr_write acc7, 0x0                          // initC
v_accvgpr_write acc8, 0x0                          // initC
v_accvgpr_write acc9, 0x0                          // initC
v_accvgpr_write acc10, 0x0                         // initC
v_accvgpr_write acc11, 0x0                         // initC
v_accvgpr_write acc12, 0x0                         // initC
v_accvgpr_write acc13, 0x0                         // initC
v_accvgpr_write acc14, 0x0                         // initC
v_accvgpr_write acc15, 0x0                         // initC
s_cmp_eq_u32 s[sgprLoopCounterL], 0                // at last iteration?

/* after InitC, skip to end of prefetch last iter if numIter==0 */
s_cbranch_scc0 label_NoBranch_BEWDQPBB8XPQW3T8_0   // Only branch on scc1
s_getpc_b64 s[72:73]                               // addr of next instr
s_add_i32 s74, label_PrefetchGlobalLastIterEnd, 0x4 // target branch offset
s_add_u32 s72, s72, s74                            // add target branch offset
s_addc_u32 s73, s73, 0                             // add high and carry
s_setpc_b64 s[72:73]                               // branch to label_PrefetchGlobalLastIterEnd
label_NoBranch_BEWDQPBB8XPQW3T8_0:
s_waitcnt vmcnt(0)                                 // 8wait for global read

/* local write a */
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+4+0] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+0], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+0], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+4+0] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+1], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+1], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+4+1] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+2], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+2], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+4+1] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+3], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+3], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+4+2] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+4], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+4], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+4+2] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+5], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+5], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+4+3] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+6], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+6], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+4+3] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+7], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+7], v95 dst_sel:WORD_1  // Convert to FP16
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+0:vgprG2LA+0+3] offset:0 // lwoA_0_0_0_0 = (0*LSCA) + (0*LSPA)(*MT0I+PAD) = 0
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+4:vgprG2LA+4+3] offset:16 // lwoA_0_0_0_0 = (0*LSCA) + (0*LSPA)(*MT0I+PAD) = 0
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+12+0] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+0], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+0], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+12+0] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+1], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+1], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+12+1] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+2], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+2], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+12+1] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+3], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+3], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+12+2] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+4], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+4], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+12+2] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+5], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+5], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+12+3] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+6], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+6], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+12+3] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+7], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+7], v95 dst_sel:WORD_1  // Convert to FP16
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+8:vgprG2LA+8+3] offset:2304 // lwoA_0_0_1_0 = (0*LSCA) + (1*LSPA)(*MT0I+PAD) = 2304
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+12:vgprG2LA+12+3] offset:2320 // lwoA_0_0_1_0 = (0*LSCA) + (1*LSPA)(*MT0I+PAD) = 2304
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+20+0] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+0], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+0], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+20+0] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+1], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+1], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+20+1] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+2], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+2], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+20+1] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+3], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+3], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+20+2] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+4], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+4], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+20+2] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+5], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+5], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+20+3] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+6], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+6], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+20+3] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+7], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+7], v95 dst_sel:WORD_1 // Convert to FP16
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+16:vgprG2LA+16+3] offset:4608 // lwoA_0_0_2_0 = (0*LSCA) + (2*LSPA)(*MT0I+PAD) = 4608
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+20:vgprG2LA+20+3] offset:4624 // lwoA_0_0_2_0 = (0*LSCA) + (2*LSPA)(*MT0I+PAD) = 4608
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+28+0] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+0], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+0], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+28+0] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+1], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+1], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+28+1] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+2], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+2], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+28+1] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+3], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+3], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+28+2] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+4], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+4], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+28+2] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+5], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+5], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+28+3] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+6], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+6], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+28+3] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+7], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+7], v95 dst_sel:WORD_1 // Convert to FP16
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+24:vgprG2LA+24+3] offset:6912 // lwoA_0_0_3_0 = (0*LSCA) + (3*LSPA)(*MT0I+PAD) = 6912
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+28:vgprG2LA+28+3] offset:6928 // lwoA_0_0_3_0 = (0*LSCA) + (3*LSPA)(*MT0I+PAD) = 6912

/* local write b */
ds_write_b64 v[vgprLocalWriteAddrB], v[vgprG2LB+0:vgprG2LB+0+1] offset:0 // lwoB_0_0_0_0 = (0*LSCB)*(MT1J+PAD) + (0*LSPB) = 0

/* local write swap a */

/* local write swap b */
s_cmp_eq_u32 s[sgprLoopCounterL], 0x1              // PGR=2 but only 1 loop
s_cbranch_scc1 label_skipPGR2_0                    // PGR=2 but only 1 loop
buffer_load_dwordx4 v[vgprG2LA+4:vgprG2LA+4+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0, sc0 // G -> Reg 0_0_0_0
buffer_load_dwordx4 v[vgprG2LA+12:vgprG2LA+12+3], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0, sc0 // G -> Reg 0_0_1_0
buffer_load_dwordx4 v[vgprG2LA+20:vgprG2LA+20+3], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0, sc0 // G -> Reg 0_0_2_0
buffer_load_dwordx4 v[vgprG2LA+28:vgprG2LA+28+3], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0, sc0 // G -> Reg 0_0_3_0
buffer_load_dwordx2 v[vgprG2LB+0:vgprG2LB+0+1], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_0_0
label_skipPGR2_0:
s_waitcnt lgkmcnt(0)                               // 0prefetch wait for local write
// Skip force waitcnt0
s_barrier

/* local read prefetch a */
ds_read_b64 v[vgprValuA_X0_I0_D0+0:vgprValuA_X0_I0_D0+0+1], v[vgprLocalReadAddrA] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b64 v[vgprValuA_X0_I0_D1+0:vgprValuA_X0_I0_D1+0+1], v[vgprLocalReadAddrA] offset:576 // L -> Reg lro=0 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=1 oIdx=0 buffer=0 iui=0
ds_read_b64 v[vgprValuA_X0_I0_D2+0:vgprValuA_X0_I0_D2+0+1], v[vgprLocalReadAddrA] offset:1152 // L -> Reg lro=0 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=2 oIdx=0 buffer=0 iui=0
ds_read_b64 v[vgprValuA_X0_I0_D3+0:vgprValuA_X0_I0_D3+0+1], v[vgprLocalReadAddrA] offset:1728 // L -> Reg lro=0 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=3 oIdx=0 buffer=0 iui=0

/* local read prefetch b */
ds_read_b128 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+3], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0

/* local read inc a */
/* N/A, lro->1024 */
/* self.localReadDoCntA 1 self.localReadDoCntB 1 */

/* local read inc b */
/* N/A, lro->32 */
/* self.localReadDoCntA 1 self.localReadDoCntB 1 */

/******************************************/
/* Unrolled Loop(s) - Begin               */
/******************************************/
label_openLoopL:
s_cmp_eq_u32 s[sgprLoopCounterL], 0x1              // LoopCounterL < EndCounter
s_cbranch_scc1 label_toPGR1_0                      // PGR=2 but only 1 loop, toPGR1
s_cmp_le_u32 s[sgprLoopCounterL], 0x2              // LoopCounterL < EndCounter
s_cbranch_scc1 label_LoopEndL                      // do not enter LoopL
label_LoopBeginL:

/******************************************/
/* Unrolled Loop 1/1 - Begin              */
/******************************************/

/* Begin Each Unroll: Check VGPR.checkin for INT8 LW */

/* iter 0 */
/*  grEndMfmaIndex:9, lwStartMfmaIndex:9, lwEndMfmaIndex:9  */
/*  numMfmaForLR:3, syncPlrMfmaIndex:12  */
/*  mfmaIndex:0  */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
/* pack scheduling: packAIdx:2, packBIdx:0 */
v_perm_b32 v[vgprValuA_X0_I0+0], v[vgprValuA_X0_I0_D1+0], v[vgprValuA_X0_I0_D0+0], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X0_I0+1], v[vgprValuA_X0_I0_D3+0], v[vgprValuA_X0_I0_D2+0], s[sgprPackKForV0] // select K=23 for vector=0
v_perm_b32 v[vgprValuA_X0_I0+2], v[vgprValuA_X0_I0_D1+0], v[vgprValuA_X0_I0_D0+0], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X0_I0+3], v[vgprValuA_X0_I0_D3+0], v[vgprValuA_X0_I0_D2+0], s[sgprPackKForV1] // select K=23 for vector=1
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
ds_read_b64 v[vgprValuA_X1_I0_D0+0:vgprValuA_X1_I0_D0+0+1], v[vgprLocalReadAddrA] offset:2304 // L -> Reg lro=1024 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=1 iui=0
ds_read_b64 v[vgprValuA_X1_I0_D1+0:vgprValuA_X1_I0_D1+0+1], v[vgprLocalReadAddrA] offset:2880 // L -> Reg lro=1024 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=1 oIdx=0 buffer=1 iui=0

/* global read inc A loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIter] // Is this the wrapIter?
/* pack scheduling: packAIdx:4, packBIdx:0 */
v_perm_b32 v[vgprValuA_X0_I0+4], v[vgprValuA_X0_I0_D1+1], v[vgprValuA_X0_I0_D0+1], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X0_I0+5], v[vgprValuA_X0_I0_D3+1], v[vgprValuA_X0_I0_D2+1], s[sgprPackKForV0] // select K=23 for vector=0
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+2+0+0:vgprValuA_X0_I0+2+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
ds_read_b64 v[vgprValuA_X1_I0_D2+0:vgprValuA_X1_I0_D2+0+1], v[vgprLocalReadAddrA] offset:3456 // L -> Reg lro=1024 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=2 oIdx=0 buffer=1 iui=0
ds_read_b64 v[vgprValuA_X1_I0_D3+0:vgprValuA_X1_I0_D3+0+1], v[vgprLocalReadAddrA] offset:4032 // L -> Reg lro=1024 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=3 oIdx=0 buffer=1 iui=0
s_cselect_b32 s72, s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0] // incLower <- ?
/* pack scheduling: packAIdx:6, packBIdx:0 */
v_perm_b32 v[vgprValuA_X0_I0+6], v[vgprValuA_X0_I0_D1+1], v[vgprValuA_X0_I0_D0+1], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X0_I0+7], v[vgprValuA_X0_I0_D3+1], v[vgprValuA_X0_I0_D2+1], s[sgprPackKForV1] // select K=23 for vector=1
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
ds_read_b128 v[vgprValuB_X2_I0+0:vgprValuB_X2_I0+0+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
s_cselect_b32 s73, s[sgprWrapUA+1], 0              // incUpper <- ?
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s72        // gra SRD += inc(lower)
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s73       // gra SRD += inc(upper)
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s72 // limit -= inc)
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s73 // limit -= inc)
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32

/* global read inc B loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIter] // Is this the wrapIter?
s_cselect_b32 s72, s[sgprWrapUB+0], s[sgprGlobalReadIncsB+0] // incLower <- ?
s_cselect_b32 s73, s[sgprWrapUB+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+6+0+0:vgprValuA_X0_I0+6+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=1 */

/* iter 1 */
/*  grEndMfmaIndex:9, lwStartMfmaIndex:9, lwEndMfmaIndex:9  */
/*  numMfmaForLR:3, syncPlrMfmaIndex:12  */
/*  mfmaIndex:4  */
ds_read_b64 v[vgprValuA_X2_I0_D0+0:vgprValuA_X2_I0_D0+0+1], v[vgprLocalReadAddrA] offset:18432 // L -> Reg lro=8192 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s72        // gra SRD += inc(lower)
s_waitcnt lgkmcnt(1)                               // wait for prior local read local write old=0, new=1 newLW=0 newLR=1
/* pack scheduling: packAIdx:2, packBIdx:0 */
v_perm_b32 v[vgprValuA_X1_I0+0], v[vgprValuA_X1_I0_D1+0], v[vgprValuA_X1_I0_D0+0], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X1_I0+1], v[vgprValuA_X1_I0_D3+0], v[vgprValuA_X1_I0_D2+0], s[sgprPackKForV0] // select K=23 for vector=0
v_perm_b32 v[vgprValuA_X1_I0+2], v[vgprValuA_X1_I0_D1+0], v[vgprValuA_X1_I0_D0+0], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X1_I0+3], v[vgprValuA_X1_I0_D3+0], v[vgprValuA_X1_I0_D2+0], s[sgprPackKForV1] // select K=23 for vector=1
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X1_I0+0+0+0:vgprValuA_X1_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:5  */
ds_read_b64 v[vgprValuA_X2_I0_D1+0:vgprValuA_X2_I0_D1+0+1], v[vgprLocalReadAddrA] offset:19008 // L -> Reg lro=8192 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=1 oIdx=0 buffer=2 iui=0
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s73       // gra SRD += inc(upper)
/* pack scheduling: packAIdx:4, packBIdx:0 */
v_perm_b32 v[vgprValuA_X1_I0+4], v[vgprValuA_X1_I0_D1+1], v[vgprValuA_X1_I0_D0+1], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X1_I0+5], v[vgprValuA_X1_I0_D3+1], v[vgprValuA_X1_I0_D2+1], s[sgprPackKForV0] // select K=23 for vector=0
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X1_I0+2+0+0:vgprValuA_X1_I0+2+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:6  */
ds_read_b64 v[vgprValuA_X2_I0_D2+0:vgprValuA_X2_I0_D2+0+1], v[vgprLocalReadAddrA] offset:19584 // L -> Reg lro=8192 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=2 oIdx=0 buffer=2 iui=0
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s72 // limit -= inc)
/* pack scheduling: packAIdx:6, packBIdx:0 */
v_perm_b32 v[vgprValuA_X1_I0+6], v[vgprValuA_X1_I0_D1+1], v[vgprValuA_X1_I0_D0+1], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X1_I0+7], v[vgprValuA_X1_I0_D3+1], v[vgprValuA_X1_I0_D2+1], s[sgprPackKForV1] // select K=23 for vector=1
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X1_I0+4+0+0:vgprValuA_X1_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:7  */
ds_read_b64 v[vgprValuA_X2_I0_D3+0:vgprValuA_X2_I0_D3+0+1], v[vgprLocalReadAddrA] offset:20160 // L -> Reg lro=8192 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=3 oIdx=0 buffer=2 iui=0
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s73 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X1_I0+6+0+0:vgprValuA_X1_I0+6+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=2 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=1 */

/* iter 2 (reset local read pointers iteration)  (swap and reset local write pointers iteration)  (swap local read pointers iteration)  */
/*  grEndMfmaIndex:9, lwStartMfmaIndex:9, lwEndMfmaIndex:9  */
/*  numMfmaForLR:3, syncPlrMfmaIndex:12  */
/*  mfmaIndex:8  */
/* schedule remaining localreads for 1LDSB */
ds_read_b64 v[vgprValuA_X3_I0_D0+0:vgprValuA_X3_I0_D0+0+1], v[vgprLocalReadAddrA] offset:20736 // L -> Reg lro=9216 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=3 iui=0
ds_read_b64 v[vgprValuA_X3_I0_D1+0:vgprValuA_X3_I0_D1+0+1], v[vgprLocalReadAddrA] offset:21312 // L -> Reg lro=9216 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=1 oIdx=0 buffer=3 iui=0
ds_read_b64 v[vgprValuA_X3_I0_D2+0:vgprValuA_X3_I0_D2+0+1], v[vgprLocalReadAddrA] offset:21888 // L -> Reg lro=9216 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=2 oIdx=0 buffer=3 iui=0
ds_read_b64 v[vgprValuA_X3_I0_D3+0:vgprValuA_X3_I0_D3+0+1], v[vgprLocalReadAddrA] offset:22464 // L -> Reg lro=9216 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=3 oIdx=0 buffer=3 iui=0
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
/* 1 LDS buffer: read-sync-write */
s_waitcnt lgkmcnt(0)
s_barrier
s_waitcnt lgkmcnt(4)                               // wait for prior local read local write old=0, new=4 newLW=0 newLR=4
/* pack scheduling: packAIdx:2, packBIdx:0 */
v_perm_b32 v[vgprValuA_X2_I0+0], v[vgprValuA_X2_I0_D1+0], v[vgprValuA_X2_I0_D0+0], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X2_I0+1], v[vgprValuA_X2_I0_D3+0], v[vgprValuA_X2_I0_D2+0], s[sgprPackKForV0] // select K=23 for vector=0
v_perm_b32 v[vgprValuA_X2_I0+2], v[vgprValuA_X2_I0_D1+0], v[vgprValuA_X2_I0_D0+0], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X2_I0+3], v[vgprValuA_X2_I0_D3+0], v[vgprValuA_X2_I0_D2+0], s[sgprPackKForV1] // select K=23 for vector=1
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:9  */
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(4)                                 // wait for global read before writing to local
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+4+0] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+0], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+0], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+4+0] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+1], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+1], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+4+1] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+2], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+2], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+4+1] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+3], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+3], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+4+2] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+4], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+4], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+4+2] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+5], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+5], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+4+3] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+6], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+6], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+4+3] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+7], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+7], v95 dst_sel:WORD_1  // Convert to FP16
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+0:vgprG2LA+0+3] offset:0 // lwoA_0_0_0_0 = (0*LSCA) + (0*LSPA)(*MT0I+PAD) = 0
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+4:vgprG2LA+4+3] offset:16 // lwoA_0_0_0_0 = (0*LSCA) + (0*LSPA)(*MT0I+PAD) = 0
buffer_load_dwordx4 v[vgprG2LA+4:vgprG2LA+4+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0, sc0 // G -> Reg 0_0_0_0
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(4)                                 // wait for global read before writing to local
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+12+0] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+0], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+0], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+12+0] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+1], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+1], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+12+1] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+2], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+2], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+12+1] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+3], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+3], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+12+2] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+4], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+4], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+12+2] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+5], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+5], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+12+3] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+6], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+6], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+12+3] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+7], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+7], v95 dst_sel:WORD_1  // Convert to FP16
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+8:vgprG2LA+8+3] offset:2304 // lwoA_0_0_1_0 = (0*LSCA) + (1*LSPA)(*MT0I+PAD) = 2304
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+12:vgprG2LA+12+3] offset:2320 // lwoA_0_0_1_0 = (0*LSCA) + (1*LSPA)(*MT0I+PAD) = 2304
buffer_load_dwordx4 v[vgprG2LA+12:vgprG2LA+12+3], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0, sc0 // G -> Reg 0_0_1_0
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(4)                                 // wait for global read before writing to local
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+20+0] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+0], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+0], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+20+0] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+1], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+1], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+20+1] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+2], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+2], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+20+1] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+3], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+3], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+20+2] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+4], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+4], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+20+2] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+5], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+5], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+20+3] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+6], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+6], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+20+3] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+7], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+7], v95 dst_sel:WORD_1 // Convert to FP16
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+16:vgprG2LA+16+3] offset:4608 // lwoA_0_0_2_0 = (0*LSCA) + (2*LSPA)(*MT0I+PAD) = 4608
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+20:vgprG2LA+20+3] offset:4624 // lwoA_0_0_2_0 = (0*LSCA) + (2*LSPA)(*MT0I+PAD) = 4608
buffer_load_dwordx4 v[vgprG2LA+20:vgprG2LA+20+3], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0, sc0 // G -> Reg 0_0_2_0
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(4)                                 // wait for global read before writing to local
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+28+0] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+0], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+0], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+28+0] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+1], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+1], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+28+1] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+2], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+2], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+28+1] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+3], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+3], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+28+2] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+4], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+4], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+28+2] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+5], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+5], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+28+3] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+6], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+6], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+28+3] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+7], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+7], v95 dst_sel:WORD_1 // Convert to FP16
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+24:vgprG2LA+24+3] offset:6912 // lwoA_0_0_3_0 = (0*LSCA) + (3*LSPA)(*MT0I+PAD) = 6912
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+28:vgprG2LA+28+3] offset:6928 // lwoA_0_0_3_0 = (0*LSCA) + (3*LSPA)(*MT0I+PAD) = 6912
buffer_load_dwordx4 v[vgprG2LA+28:vgprG2LA+28+3], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0, sc0 // G -> Reg 0_0_3_0
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(4)                                 // wait for global read before writing to local
ds_write_b64 v[vgprLocalWriteAddrB], v[vgprG2LB+0:vgprG2LB+0+1] offset:0 // lwoB_0_0_0_0 = (0*LSCB)*(MT1J+PAD) + (0*LSPB) = 0
buffer_load_dwordx2 v[vgprG2LB+0:vgprG2LB+0+1], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_0_0

/* local write swap offsets a */

/* local write swap offsets b */
/* pack scheduling: packAIdx:4, packBIdx:0 */
v_perm_b32 v[vgprValuA_X2_I0+4], v[vgprValuA_X2_I0_D1+1], v[vgprValuA_X2_I0_D0+1], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X2_I0+5], v[vgprValuA_X2_I0_D3+1], v[vgprValuA_X2_I0_D2+1], s[sgprPackKForV0] // select K=23 for vector=0
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+2+0+0:vgprValuA_X2_I0+2+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:10  */
/* pack scheduling: packAIdx:6, packBIdx:0 */
v_perm_b32 v[vgprValuA_X2_I0+6], v[vgprValuA_X2_I0_D1+1], v[vgprValuA_X2_I0_D0+1], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X2_I0+7], v[vgprValuA_X2_I0_D3+1], v[vgprValuA_X2_I0_D2+1], s[sgprPackKForV1] // select K=23 for vector=1
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:11  */

/* local read swap offsets a */

/* local read swap offsets b */

/* local read init pointers a */

/* localReadInitPointers */

/* local read init pointers b */

/* localReadInitPointers */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+6+0+0:vgprValuA_X2_I0+6+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/* numPrefetchIter=0 */
/* dataAtIterA=1 numReadsIterA=3 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=1 */

/* iter 3 */
/*  grEndMfmaIndex:9, lwStartMfmaIndex:9, lwEndMfmaIndex:9  */
/*  numMfmaForLR:3, syncPlrMfmaIndex:12  */
/*  mfmaIndex:12  */
s_waitcnt lgkmcnt(0)                               // 3wait for local write
// Skip force waitcnt0
s_barrier
s_waitcnt lgkmcnt(9)                               // wait for prior local read local write old=0, new=9 newLW=9 newLR=0
/* pack scheduling: packAIdx:2, packBIdx:0 */
v_perm_b32 v[vgprValuA_X3_I0+0], v[vgprValuA_X3_I0_D1+0], v[vgprValuA_X3_I0_D0+0], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X3_I0+1], v[vgprValuA_X3_I0_D3+0], v[vgprValuA_X3_I0_D2+0], s[sgprPackKForV0] // select K=23 for vector=0
v_perm_b32 v[vgprValuA_X3_I0+2], v[vgprValuA_X3_I0_D1+0], v[vgprValuA_X3_I0_D0+0], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X3_I0+3], v[vgprValuA_X3_I0_D3+0], v[vgprValuA_X3_I0_D2+0], s[sgprPackKForV1] // select K=23 for vector=1
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X3_I0+0+0+0:vgprValuA_X3_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:13  */
ds_read_b64 v[vgprValuA_X0_I0_D0+0:vgprValuA_X0_I0_D0+0+1], v[vgprLocalReadAddrA] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b64 v[vgprValuA_X0_I0_D1+0:vgprValuA_X0_I0_D1+0+1], v[vgprLocalReadAddrA] offset:576 // L -> Reg lro=0 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=1 oIdx=0 buffer=0 iui=0
/* pack scheduling: packAIdx:4, packBIdx:0 */
v_perm_b32 v[vgprValuA_X3_I0+4], v[vgprValuA_X3_I0_D1+1], v[vgprValuA_X3_I0_D0+1], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X3_I0+5], v[vgprValuA_X3_I0_D3+1], v[vgprValuA_X3_I0_D2+1], s[sgprPackKForV0] // select K=23 for vector=0
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X3_I0+2+0+0:vgprValuA_X3_I0+2+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:14  */
ds_read_b64 v[vgprValuA_X0_I0_D2+0:vgprValuA_X0_I0_D2+0+1], v[vgprLocalReadAddrA] offset:1152 // L -> Reg lro=0 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=2 oIdx=0 buffer=0 iui=0
ds_read_b64 v[vgprValuA_X0_I0_D3+0:vgprValuA_X0_I0_D3+0+1], v[vgprLocalReadAddrA] offset:1728 // L -> Reg lro=0 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=3 oIdx=0 buffer=0 iui=0
/* pack scheduling: packAIdx:6, packBIdx:0 */
v_perm_b32 v[vgprValuA_X3_I0+6], v[vgprValuA_X3_I0_D1+1], v[vgprValuA_X3_I0_D0+1], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X3_I0+7], v[vgprValuA_X3_I0_D3+1], v[vgprValuA_X3_I0_D2+1], s[sgprPackKForV1] // select K=23 for vector=1
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X3_I0+4+0+0:vgprValuA_X3_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:15  */
ds_read_b128 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+3], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X3_I0+6+0+0:vgprValuA_X3_I0+6+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/* numPrefetchIter=1 */
/* dataAtIterA=2 numReadsIterA=3 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=1 */

/******************************************/
/* Unrolled Loop - End                    */
/******************************************/

/* closeLoop loopL finalLoop=1 tailLoop=0 */
s_sub_u32 s[sgprLoopCounterL], s[sgprLoopCounterL], 1 // dec counterL
s_cmp_eq_i32 s[sgprLoopCounterL], 0x2              // counterL==2
s_cbranch_scc0 label_LoopBeginL                    // restart LoopL
label_LoopEndL:

/* Before NLL: Check VGPR.checkin for INT8 LW */

/******************************************/
/* Ord. NoGlobalLoadLoop - Begin          */
/******************************************/

/* iter 0 */
/*  grEndMfmaIndex:9, lwStartMfmaIndex:9, lwEndMfmaIndex:9  */
/*  numMfmaForLR:3, syncPlrMfmaIndex:12  */
/*  mfmaIndex:0  */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
/* pack scheduling: packAIdx:2, packBIdx:0 */
v_perm_b32 v[vgprValuA_X0_I0+0], v[vgprValuA_X0_I0_D1+0], v[vgprValuA_X0_I0_D0+0], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X0_I0+1], v[vgprValuA_X0_I0_D3+0], v[vgprValuA_X0_I0_D2+0], s[sgprPackKForV0] // select K=23 for vector=0
v_perm_b32 v[vgprValuA_X0_I0+2], v[vgprValuA_X0_I0_D1+0], v[vgprValuA_X0_I0_D0+0], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X0_I0+3], v[vgprValuA_X0_I0_D3+0], v[vgprValuA_X0_I0_D2+0], s[sgprPackKForV1] // select K=23 for vector=1
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
ds_read_b64 v[vgprValuA_X1_I0_D0+0:vgprValuA_X1_I0_D0+0+1], v[vgprLocalReadAddrA] offset:2304 // L -> Reg lro=1024 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=1 iui=0
ds_read_b64 v[vgprValuA_X1_I0_D1+0:vgprValuA_X1_I0_D1+0+1], v[vgprLocalReadAddrA] offset:2880 // L -> Reg lro=1024 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=1 oIdx=0 buffer=1 iui=0

/* global read inc A loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIter] // Is this the wrapIter?
/* pack scheduling: packAIdx:4, packBIdx:0 */
v_perm_b32 v[vgprValuA_X0_I0+4], v[vgprValuA_X0_I0_D1+1], v[vgprValuA_X0_I0_D0+1], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X0_I0+5], v[vgprValuA_X0_I0_D3+1], v[vgprValuA_X0_I0_D2+1], s[sgprPackKForV0] // select K=23 for vector=0
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+2+0+0:vgprValuA_X0_I0+2+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
ds_read_b64 v[vgprValuA_X1_I0_D2+0:vgprValuA_X1_I0_D2+0+1], v[vgprLocalReadAddrA] offset:3456 // L -> Reg lro=1024 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=2 oIdx=0 buffer=1 iui=0
ds_read_b64 v[vgprValuA_X1_I0_D3+0:vgprValuA_X1_I0_D3+0+1], v[vgprLocalReadAddrA] offset:4032 // L -> Reg lro=1024 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=3 oIdx=0 buffer=1 iui=0
s_cselect_b32 s72, s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0] // incLower <- ?
/* pack scheduling: packAIdx:6, packBIdx:0 */
v_perm_b32 v[vgprValuA_X0_I0+6], v[vgprValuA_X0_I0_D1+1], v[vgprValuA_X0_I0_D0+1], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X0_I0+7], v[vgprValuA_X0_I0_D3+1], v[vgprValuA_X0_I0_D2+1], s[sgprPackKForV1] // select K=23 for vector=1
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
ds_read_b128 v[vgprValuB_X2_I0+0:vgprValuB_X2_I0+0+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
s_cselect_b32 s73, s[sgprWrapUA+1], 0              // incUpper <- ?
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s72        // gra SRD += inc(lower)
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s73       // gra SRD += inc(upper)
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s72 // limit -= inc)
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s73 // limit -= inc)
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32

/* global read inc B loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIter] // Is this the wrapIter?
s_cselect_b32 s72, s[sgprWrapUB+0], s[sgprGlobalReadIncsB+0] // incLower <- ?
s_cselect_b32 s73, s[sgprWrapUB+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+6+0+0:vgprValuA_X0_I0+6+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=1 */

/* iter 1 */
/*  grEndMfmaIndex:9, lwStartMfmaIndex:9, lwEndMfmaIndex:9  */
/*  numMfmaForLR:3, syncPlrMfmaIndex:12  */
/*  mfmaIndex:4  */
ds_read_b64 v[vgprValuA_X2_I0_D0+0:vgprValuA_X2_I0_D0+0+1], v[vgprLocalReadAddrA] offset:18432 // L -> Reg lro=8192 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s72        // gra SRD += inc(lower)
s_waitcnt lgkmcnt(1)                               // wait for prior local read local write old=0, new=1 newLW=0 newLR=1
/* pack scheduling: packAIdx:2, packBIdx:0 */
v_perm_b32 v[vgprValuA_X1_I0+0], v[vgprValuA_X1_I0_D1+0], v[vgprValuA_X1_I0_D0+0], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X1_I0+1], v[vgprValuA_X1_I0_D3+0], v[vgprValuA_X1_I0_D2+0], s[sgprPackKForV0] // select K=23 for vector=0
v_perm_b32 v[vgprValuA_X1_I0+2], v[vgprValuA_X1_I0_D1+0], v[vgprValuA_X1_I0_D0+0], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X1_I0+3], v[vgprValuA_X1_I0_D3+0], v[vgprValuA_X1_I0_D2+0], s[sgprPackKForV1] // select K=23 for vector=1
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X1_I0+0+0+0:vgprValuA_X1_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:5  */
ds_read_b64 v[vgprValuA_X2_I0_D1+0:vgprValuA_X2_I0_D1+0+1], v[vgprLocalReadAddrA] offset:19008 // L -> Reg lro=8192 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=1 oIdx=0 buffer=2 iui=0
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s73       // gra SRD += inc(upper)
/* pack scheduling: packAIdx:4, packBIdx:0 */
v_perm_b32 v[vgprValuA_X1_I0+4], v[vgprValuA_X1_I0_D1+1], v[vgprValuA_X1_I0_D0+1], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X1_I0+5], v[vgprValuA_X1_I0_D3+1], v[vgprValuA_X1_I0_D2+1], s[sgprPackKForV0] // select K=23 for vector=0
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X1_I0+2+0+0:vgprValuA_X1_I0+2+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:6  */
ds_read_b64 v[vgprValuA_X2_I0_D2+0:vgprValuA_X2_I0_D2+0+1], v[vgprLocalReadAddrA] offset:19584 // L -> Reg lro=8192 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=2 oIdx=0 buffer=2 iui=0
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s72 // limit -= inc)
/* pack scheduling: packAIdx:6, packBIdx:0 */
v_perm_b32 v[vgprValuA_X1_I0+6], v[vgprValuA_X1_I0_D1+1], v[vgprValuA_X1_I0_D0+1], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X1_I0+7], v[vgprValuA_X1_I0_D3+1], v[vgprValuA_X1_I0_D2+1], s[sgprPackKForV1] // select K=23 for vector=1
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X1_I0+4+0+0:vgprValuA_X1_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:7  */
ds_read_b64 v[vgprValuA_X2_I0_D3+0:vgprValuA_X2_I0_D3+0+1], v[vgprLocalReadAddrA] offset:20160 // L -> Reg lro=8192 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=3 oIdx=0 buffer=2 iui=0
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s73 // limit -= inc)
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X1_I0+6+0+0:vgprValuA_X1_I0+6+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=2 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=1 */

/* iter 2 (reset local read pointers iteration)  (swap and reset local write pointers iteration)  (swap local read pointers iteration)  */
/*  grEndMfmaIndex:9, lwStartMfmaIndex:9, lwEndMfmaIndex:9  */
/*  numMfmaForLR:3, syncPlrMfmaIndex:12  */
/*  mfmaIndex:8  */
/* schedule remaining localreads for 1LDSB */
ds_read_b64 v[vgprValuA_X3_I0_D0+0:vgprValuA_X3_I0_D0+0+1], v[vgprLocalReadAddrA] offset:20736 // L -> Reg lro=9216 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=3 iui=0
ds_read_b64 v[vgprValuA_X3_I0_D1+0:vgprValuA_X3_I0_D1+0+1], v[vgprLocalReadAddrA] offset:21312 // L -> Reg lro=9216 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=1 oIdx=0 buffer=3 iui=0
ds_read_b64 v[vgprValuA_X3_I0_D2+0:vgprValuA_X3_I0_D2+0+1], v[vgprLocalReadAddrA] offset:21888 // L -> Reg lro=9216 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=2 oIdx=0 buffer=3 iui=0
ds_read_b64 v[vgprValuA_X3_I0_D3+0:vgprValuA_X3_I0_D3+0+1], v[vgprLocalReadAddrA] offset:22464 // L -> Reg lro=9216 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=3 oIdx=0 buffer=3 iui=0
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
/* 1 LDS buffer: read-sync-write */
s_waitcnt lgkmcnt(0)
s_barrier
s_waitcnt lgkmcnt(4)                               // wait for prior local read local write old=0, new=4 newLW=0 newLR=4
/* pack scheduling: packAIdx:2, packBIdx:0 */
v_perm_b32 v[vgprValuA_X2_I0+0], v[vgprValuA_X2_I0_D1+0], v[vgprValuA_X2_I0_D0+0], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X2_I0+1], v[vgprValuA_X2_I0_D3+0], v[vgprValuA_X2_I0_D2+0], s[sgprPackKForV0] // select K=23 for vector=0
v_perm_b32 v[vgprValuA_X2_I0+2], v[vgprValuA_X2_I0_D1+0], v[vgprValuA_X2_I0_D0+0], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X2_I0+3], v[vgprValuA_X2_I0_D3+0], v[vgprValuA_X2_I0_D2+0], s[sgprPackKForV1] // select K=23 for vector=1
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:9  */
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(4)                                 // wait for global read before writing to local
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+4+0] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+0], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+0], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+4+0] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+1], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+1], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+4+1] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+2], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+2], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+4+1] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+3], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+3], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+4+2] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+4], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+4], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+4+2] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+5], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+5], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+4+3] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+6], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+6], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+4+3] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+7], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+7], v95 dst_sel:WORD_1  // Convert to FP16
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+0:vgprG2LA+0+3] offset:0 // lwoA_0_0_0_0 = (0*LSCA) + (0*LSPA)(*MT0I+PAD) = 0
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+4:vgprG2LA+4+3] offset:16 // lwoA_0_0_0_0 = (0*LSCA) + (0*LSPA)(*MT0I+PAD) = 0
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(3)                                 // wait for global read before writing to local
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+12+0] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+0], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+0], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+12+0] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+1], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+1], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+12+1] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+2], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+2], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+12+1] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+3], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+3], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+12+2] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+4], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+4], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+12+2] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+5], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+5], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+12+3] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+6], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+6], v95 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+12+3] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+7], v94 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+7], v95 dst_sel:WORD_1  // Convert to FP16
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+8:vgprG2LA+8+3] offset:2304 // lwoA_0_0_1_0 = (0*LSCA) + (1*LSPA)(*MT0I+PAD) = 2304
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+12:vgprG2LA+12+3] offset:2320 // lwoA_0_0_1_0 = (0*LSCA) + (1*LSPA)(*MT0I+PAD) = 2304
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(2)                                 // wait for global read before writing to local
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+20+0] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+0], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+0], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+20+0] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+1], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+1], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+20+1] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+2], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+2], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+20+1] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+3], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+3], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+20+2] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+4], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+4], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+20+2] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+5], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+5], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+20+3] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+6], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+6], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+20+3] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+7], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+7], v95 dst_sel:WORD_1 // Convert to FP16
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+16:vgprG2LA+16+3] offset:4608 // lwoA_0_0_2_0 = (0*LSCA) + (2*LSPA)(*MT0I+PAD) = 4608
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+20:vgprG2LA+20+3] offset:4624 // lwoA_0_0_2_0 = (0*LSCA) + (2*LSPA)(*MT0I+PAD) = 4608
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(1)                                 // wait for global read before writing to local
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+28+0] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+0], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+0], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+28+0] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+1], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+1], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+28+1] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+2], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+2], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+28+1] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+3], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+3], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+28+2] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+4], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+4], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+28+2] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+5], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+5], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+28+3] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+6], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+6], v95 dst_sel:WORD_1 // Convert to FP16
v_cvt_pk_f32_fp8 v[94:95], v[vgprG2LA+28+3] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+7], v94 dst_sel:WORD_0 // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+7], v95 dst_sel:WORD_1 // Convert to FP16
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+24:vgprG2LA+24+3] offset:6912 // lwoA_0_0_3_0 = (0*LSCA) + (3*LSPA)(*MT0I+PAD) = 6912
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+28:vgprG2LA+28+3] offset:6928 // lwoA_0_0_3_0 = (0*LSCA) + (3*LSPA)(*MT0I+PAD) = 6912
/* sched write - iter 2 writesPerItem=1 */
s_waitcnt vmcnt(0)                                 // wait for global read before writing to local
ds_write_b64 v[vgprLocalWriteAddrB], v[vgprG2LB+0:vgprG2LB+0+1] offset:0 // lwoB_0_0_0_0 = (0*LSCB)*(MT1J+PAD) + (0*LSPB) = 0

/* local write swap offsets a */

/* local write swap offsets b */
/* pack scheduling: packAIdx:4, packBIdx:0 */
v_perm_b32 v[vgprValuA_X2_I0+4], v[vgprValuA_X2_I0_D1+1], v[vgprValuA_X2_I0_D0+1], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X2_I0+5], v[vgprValuA_X2_I0_D3+1], v[vgprValuA_X2_I0_D2+1], s[sgprPackKForV0] // select K=23 for vector=0
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+2+0+0:vgprValuA_X2_I0+2+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:10  */
/* pack scheduling: packAIdx:6, packBIdx:0 */
v_perm_b32 v[vgprValuA_X2_I0+6], v[vgprValuA_X2_I0_D1+1], v[vgprValuA_X2_I0_D0+1], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X2_I0+7], v[vgprValuA_X2_I0_D3+1], v[vgprValuA_X2_I0_D2+1], s[sgprPackKForV1] // select K=23 for vector=1
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:11  */

/* local read swap offsets a */

/* local read swap offsets b */

/* local read init pointers a */

/* localReadInitPointers */

/* local read init pointers b */

/* localReadInitPointers */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+6+0+0:vgprValuA_X2_I0+6+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/* numPrefetchIter=0 */
/* dataAtIterA=1 numReadsIterA=3 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=1 */

/* iter 3 */
/*  grEndMfmaIndex:9, lwStartMfmaIndex:9, lwEndMfmaIndex:9  */
/*  numMfmaForLR:3, syncPlrMfmaIndex:12  */
/*  mfmaIndex:12  */
s_waitcnt lgkmcnt(0)                               // 3wait for local write
// Skip force waitcnt0
s_barrier
s_waitcnt lgkmcnt(9)                               // wait for prior local read local write old=0, new=9 newLW=9 newLR=0
/* pack scheduling: packAIdx:2, packBIdx:0 */
v_perm_b32 v[vgprValuA_X3_I0+0], v[vgprValuA_X3_I0_D1+0], v[vgprValuA_X3_I0_D0+0], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X3_I0+1], v[vgprValuA_X3_I0_D3+0], v[vgprValuA_X3_I0_D2+0], s[sgprPackKForV0] // select K=23 for vector=0
v_perm_b32 v[vgprValuA_X3_I0+2], v[vgprValuA_X3_I0_D1+0], v[vgprValuA_X3_I0_D0+0], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X3_I0+3], v[vgprValuA_X3_I0_D3+0], v[vgprValuA_X3_I0_D2+0], s[sgprPackKForV1] // select K=23 for vector=1
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X3_I0+0+0+0:vgprValuA_X3_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:13  */
ds_read_b64 v[vgprValuA_X0_I0_D0+0:vgprValuA_X0_I0_D0+0+1], v[vgprLocalReadAddrA] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b64 v[vgprValuA_X0_I0_D1+0:vgprValuA_X0_I0_D1+0+1], v[vgprLocalReadAddrA] offset:576 // L -> Reg lro=0 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=1 oIdx=0 buffer=0 iui=0
/* pack scheduling: packAIdx:4, packBIdx:0 */
v_perm_b32 v[vgprValuA_X3_I0+4], v[vgprValuA_X3_I0_D1+1], v[vgprValuA_X3_I0_D0+1], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X3_I0+5], v[vgprValuA_X3_I0_D3+1], v[vgprValuA_X3_I0_D2+1], s[sgprPackKForV0] // select K=23 for vector=0
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X3_I0+2+0+0:vgprValuA_X3_I0+2+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:14  */
ds_read_b64 v[vgprValuA_X0_I0_D2+0:vgprValuA_X0_I0_D2+0+1], v[vgprLocalReadAddrA] offset:1152 // L -> Reg lro=0 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=2 oIdx=0 buffer=0 iui=0
ds_read_b64 v[vgprValuA_X0_I0_D3+0:vgprValuA_X0_I0_D3+0+1], v[vgprLocalReadAddrA] offset:1728 // L -> Reg lro=0 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=3 oIdx=0 buffer=0 iui=0
/* pack scheduling: packAIdx:6, packBIdx:0 */
v_perm_b32 v[vgprValuA_X3_I0+6], v[vgprValuA_X3_I0_D1+1], v[vgprValuA_X3_I0_D0+1], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X3_I0+7], v[vgprValuA_X3_I0_D3+1], v[vgprValuA_X3_I0_D2+1], s[sgprPackKForV1] // select K=23 for vector=1
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X3_I0+4+0+0:vgprValuA_X3_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:15  */
ds_read_b128 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+3], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X3_I0+6+0+0:vgprValuA_X3_I0+6+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/* numPrefetchIter=1 */
/* dataAtIterA=2 numReadsIterA=3 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=1 */
label_toPGR1_0:
s_cmp_eq_u32 s[sgprGSU], 1                         // GSU == 1 ?
s_cbranch_scc0 label_GSU_3                         // branch if GSU != 1
label_GSU_3:

/******************************************/
/* Ord. NoLoadLoop - Begin                */
/******************************************/

/* iter 0 (last unrolled loop) */
/*  grEndMfmaIndex:0, lwStartMfmaIndex:9, lwEndMfmaIndex:9  */
/*  numMfmaForLR:3, syncPlrMfmaIndex:12  */
/*  mfmaIndex:0  */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
/* pack scheduling: packAIdx:2, packBIdx:0 */
v_perm_b32 v[vgprValuA_X0_I0+0], v[vgprValuA_X0_I0_D1+0], v[vgprValuA_X0_I0_D0+0], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X0_I0+1], v[vgprValuA_X0_I0_D3+0], v[vgprValuA_X0_I0_D2+0], s[sgprPackKForV0] // select K=23 for vector=0
v_perm_b32 v[vgprValuA_X0_I0+2], v[vgprValuA_X0_I0_D1+0], v[vgprValuA_X0_I0_D0+0], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X0_I0+3], v[vgprValuA_X0_I0_D3+0], v[vgprValuA_X0_I0_D2+0], s[sgprPackKForV1] // select K=23 for vector=1
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
ds_read_b64 v[vgprValuA_X1_I0_D0+0:vgprValuA_X1_I0_D0+0+1], v[vgprLocalReadAddrA] offset:2304 // L -> Reg lro=1024 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=1 iui=0
ds_read_b64 v[vgprValuA_X1_I0_D1+0:vgprValuA_X1_I0_D1+0+1], v[vgprLocalReadAddrA] offset:2880 // L -> Reg lro=1024 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=1 oIdx=0 buffer=1 iui=0
/* pack scheduling: packAIdx:4, packBIdx:0 */
v_perm_b32 v[vgprValuA_X0_I0+4], v[vgprValuA_X0_I0_D1+1], v[vgprValuA_X0_I0_D0+1], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X0_I0+5], v[vgprValuA_X0_I0_D3+1], v[vgprValuA_X0_I0_D2+1], s[sgprPackKForV0] // select K=23 for vector=0
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+2+0+0:vgprValuA_X0_I0+2+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
ds_read_b64 v[vgprValuA_X1_I0_D2+0:vgprValuA_X1_I0_D2+0+1], v[vgprLocalReadAddrA] offset:3456 // L -> Reg lro=1024 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=2 oIdx=0 buffer=1 iui=0
ds_read_b64 v[vgprValuA_X1_I0_D3+0:vgprValuA_X1_I0_D3+0+1], v[vgprLocalReadAddrA] offset:4032 // L -> Reg lro=1024 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=3 oIdx=0 buffer=1 iui=0
/* pack scheduling: packAIdx:6, packBIdx:0 */
v_perm_b32 v[vgprValuA_X0_I0+6], v[vgprValuA_X0_I0_D1+1], v[vgprValuA_X0_I0_D0+1], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X0_I0+7], v[vgprValuA_X0_I0_D3+1], v[vgprValuA_X0_I0_D2+1], s[sgprPackKForV1] // select K=23 for vector=1
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
ds_read_b128 v[vgprValuB_X2_I0+0:vgprValuB_X2_I0+0+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+6+0+0:vgprValuA_X0_I0+6+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=1 */

/* iter 1 (last unrolled loop) */
/*  grEndMfmaIndex:0, lwStartMfmaIndex:9, lwEndMfmaIndex:9  */
/*  numMfmaForLR:3, syncPlrMfmaIndex:12  */
/*  mfmaIndex:4  */
ds_read_b64 v[vgprValuA_X2_I0_D0+0:vgprValuA_X2_I0_D0+0+1], v[vgprLocalReadAddrA] offset:18432 // L -> Reg lro=8192 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
s_waitcnt lgkmcnt(1)                               // wait for prior local read local write old=0, new=1 newLW=0 newLR=1
/* pack scheduling: packAIdx:2, packBIdx:0 */
v_perm_b32 v[vgprValuA_X1_I0+0], v[vgprValuA_X1_I0_D1+0], v[vgprValuA_X1_I0_D0+0], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X1_I0+1], v[vgprValuA_X1_I0_D3+0], v[vgprValuA_X1_I0_D2+0], s[sgprPackKForV0] // select K=23 for vector=0
v_perm_b32 v[vgprValuA_X1_I0+2], v[vgprValuA_X1_I0_D1+0], v[vgprValuA_X1_I0_D0+0], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X1_I0+3], v[vgprValuA_X1_I0_D3+0], v[vgprValuA_X1_I0_D2+0], s[sgprPackKForV1] // select K=23 for vector=1
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X1_I0+0+0+0:vgprValuA_X1_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:5  */
ds_read_b64 v[vgprValuA_X2_I0_D1+0:vgprValuA_X2_I0_D1+0+1], v[vgprLocalReadAddrA] offset:19008 // L -> Reg lro=8192 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=1 oIdx=0 buffer=2 iui=0
/* pack scheduling: packAIdx:4, packBIdx:0 */
v_perm_b32 v[vgprValuA_X1_I0+4], v[vgprValuA_X1_I0_D1+1], v[vgprValuA_X1_I0_D0+1], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X1_I0+5], v[vgprValuA_X1_I0_D3+1], v[vgprValuA_X1_I0_D2+1], s[sgprPackKForV0] // select K=23 for vector=0
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X1_I0+2+0+0:vgprValuA_X1_I0+2+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:6  */
ds_read_b64 v[vgprValuA_X2_I0_D2+0:vgprValuA_X2_I0_D2+0+1], v[vgprLocalReadAddrA] offset:19584 // L -> Reg lro=8192 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=2 oIdx=0 buffer=2 iui=0
/* pack scheduling: packAIdx:6, packBIdx:0 */
v_perm_b32 v[vgprValuA_X1_I0+6], v[vgprValuA_X1_I0_D1+1], v[vgprValuA_X1_I0_D0+1], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X1_I0+7], v[vgprValuA_X1_I0_D3+1], v[vgprValuA_X1_I0_D2+1], s[sgprPackKForV1] // select K=23 for vector=1
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X1_I0+4+0+0:vgprValuA_X1_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:7  */
ds_read_b64 v[vgprValuA_X2_I0_D3+0:vgprValuA_X2_I0_D3+0+1], v[vgprLocalReadAddrA] offset:20160 // L -> Reg lro=8192 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=3 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X1_I0+6+0+0:vgprValuA_X1_I0+6+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=2 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=1 */

/* iter 2 (last unrolled loop) */
/*  grEndMfmaIndex:0, lwStartMfmaIndex:9, lwEndMfmaIndex:9  */
/*  numMfmaForLR:3, syncPlrMfmaIndex:12  */
/*  mfmaIndex:8  */
/* schedule remaining localreads for 1LDSB */
ds_read_b64 v[vgprValuA_X3_I0_D0+0:vgprValuA_X3_I0_D0+0+1], v[vgprLocalReadAddrA] offset:20736 // L -> Reg lro=9216 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=3 iui=0
ds_read_b64 v[vgprValuA_X3_I0_D1+0:vgprValuA_X3_I0_D1+0+1], v[vgprLocalReadAddrA] offset:21312 // L -> Reg lro=9216 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=1 oIdx=0 buffer=3 iui=0
ds_read_b64 v[vgprValuA_X3_I0_D2+0:vgprValuA_X3_I0_D2+0+1], v[vgprLocalReadAddrA] offset:21888 // L -> Reg lro=9216 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=2 oIdx=0 buffer=3 iui=0
ds_read_b64 v[vgprValuA_X3_I0_D3+0:vgprValuA_X3_I0_D3+0+1], v[vgprLocalReadAddrA] offset:22464 // L -> Reg lro=9216 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=3 oIdx=0 buffer=3 iui=0
/* 1 LDS buffer: read-sync-write */
s_waitcnt lgkmcnt(0)
s_barrier
s_waitcnt lgkmcnt(4)                               // wait for prior local read local write old=0, new=4 newLW=0 newLR=4
/* pack scheduling: packAIdx:2, packBIdx:0 */
v_perm_b32 v[vgprValuA_X2_I0+0], v[vgprValuA_X2_I0_D1+0], v[vgprValuA_X2_I0_D0+0], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X2_I0+1], v[vgprValuA_X2_I0_D3+0], v[vgprValuA_X2_I0_D2+0], s[sgprPackKForV0] // select K=23 for vector=0
v_perm_b32 v[vgprValuA_X2_I0+2], v[vgprValuA_X2_I0_D1+0], v[vgprValuA_X2_I0_D0+0], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X2_I0+3], v[vgprValuA_X2_I0_D3+0], v[vgprValuA_X2_I0_D2+0], s[sgprPackKForV1] // select K=23 for vector=1
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:9  */
/* pack scheduling: packAIdx:4, packBIdx:0 */
v_perm_b32 v[vgprValuA_X2_I0+4], v[vgprValuA_X2_I0_D1+1], v[vgprValuA_X2_I0_D0+1], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X2_I0+5], v[vgprValuA_X2_I0_D3+1], v[vgprValuA_X2_I0_D2+1], s[sgprPackKForV0] // select K=23 for vector=0
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+2+0+0:vgprValuA_X2_I0+2+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:10  */
/* pack scheduling: packAIdx:6, packBIdx:0 */
v_perm_b32 v[vgprValuA_X2_I0+6], v[vgprValuA_X2_I0_D1+1], v[vgprValuA_X2_I0_D0+1], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X2_I0+7], v[vgprValuA_X2_I0_D3+1], v[vgprValuA_X2_I0_D2+1], s[sgprPackKForV1] // select K=23 for vector=1
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:11  */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+6+0+0:vgprValuA_X2_I0+6+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/* numPrefetchIter=0 */
/* dataAtIterA=1 numReadsIterA=3 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=1 */

/* iter 3 (last unrolled loop) */
/*  grEndMfmaIndex:0, lwStartMfmaIndex:9, lwEndMfmaIndex:9  */
/*  numMfmaForLR:3, syncPlrMfmaIndex:12  */
/*  mfmaIndex:12  */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
/* pack scheduling: packAIdx:2, packBIdx:0 */
v_perm_b32 v[vgprValuA_X3_I0+0], v[vgprValuA_X3_I0_D1+0], v[vgprValuA_X3_I0_D0+0], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X3_I0+1], v[vgprValuA_X3_I0_D3+0], v[vgprValuA_X3_I0_D2+0], s[sgprPackKForV0] // select K=23 for vector=0
v_perm_b32 v[vgprValuA_X3_I0+2], v[vgprValuA_X3_I0_D1+0], v[vgprValuA_X3_I0_D0+0], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X3_I0+3], v[vgprValuA_X3_I0_D3+0], v[vgprValuA_X3_I0_D2+0], s[sgprPackKForV1] // select K=23 for vector=1
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X3_I0+0+0+0:vgprValuA_X3_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:13  */
/* pack scheduling: packAIdx:4, packBIdx:0 */
v_perm_b32 v[vgprValuA_X3_I0+4], v[vgprValuA_X3_I0_D1+1], v[vgprValuA_X3_I0_D0+1], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X3_I0+5], v[vgprValuA_X3_I0_D3+1], v[vgprValuA_X3_I0_D2+1], s[sgprPackKForV0] // select K=23 for vector=0
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X3_I0+2+0+0:vgprValuA_X3_I0+2+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:14  */
/* pack scheduling: packAIdx:6, packBIdx:0 */
v_perm_b32 v[vgprValuA_X3_I0+6], v[vgprValuA_X3_I0_D1+1], v[vgprValuA_X3_I0_D0+1], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X3_I0+7], v[vgprValuA_X3_I0_D3+1], v[vgprValuA_X3_I0_D2+1], s[sgprPackKForV1] // select K=23 for vector=1
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X3_I0+4+0+0:vgprValuA_X3_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:15  */
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X3_I0+6+0+0:vgprValuA_X3_I0+6+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/* numPrefetchIter=0 */
/* dataAtIterA=2 numReadsIterA=3 skipReadsIterA=0 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=1 */
label_PrefetchGlobalLastIterEnd:

/******************************************/
/* Tail Loop                              */
/******************************************/

/* Tail: add ValuA/B vgpr buffer [0...48) to pool */

/* local write reset offsets a */

/* local write reset offsets b */

// numIterL = (((sizeL % LOCAL_DEPTHU) + LOCAL_SPLITU - 1) / LOCAL_SPLITU)
s_and_b32 s[sgprLoopCounterL], 63, s[sgprSizesSum+0] // s[sgprLoopCounterL] = s[sgprSizesSum+0] % 64
s_cmp_lg_u32 s[sgprGSUSumIdx], s[sgprGSUSumIdx+1]  // gsuSumIdx == numIterPerWgRemainder
s_cmov_b32 s[sgprLoopCounterL], 0x0                // numIter=0 if gsuSimIdx!=remainder
s_cmp_eq_u32 s[sgprLoopCounterL], 0x0              // numIterL == 0
s_mov_b32 s[sgprOrigLoopCounter], 0                // repurpose to count each localRead increment
s_cbranch_scc1 label_SkipTailLoopL                 // skip to end of tail loop b/c numIter==0

/* remove stagger offsets for tail loop */
s_sub_i32 s72, 3, s[sgprStaggerUIter]
s_mul_hi_i32 s73, s72, s[sgprGlobalReadIncsA+0]    // start offset S in bytes
s_mul_i32 s72, s72, s[sgprGlobalReadIncsA+0]       // start offset S in bytes
s_sub_u32 s72, s72, s[sgprWrapUA]                  // S - WrapU
s_subb_u32 s73, s73, s[sgprWrapUA+1]               // S - WrapU
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s72        // gra SRD += inc(lower)
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s73       // gra SRD += inc(upper)
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s72 // limit -= inc)
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s73 // limit -= inc)
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
s_sub_i32 s72, 3, s[sgprStaggerUIter]
s_mul_hi_i32 s73, s72, s[sgprGlobalReadIncsB+0]    // start offset S in bytes
s_mul_i32 s72, s72, s[sgprGlobalReadIncsB+0]       // start offset S in bytes
s_sub_u32 s72, s72, s[sgprWrapUB]                  // S - WrapU
s_subb_u32 s73, s73, s[sgprWrapUB+1]               // S - WrapU
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s72        // gra SRD += inc(lower)
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s73       // gra SRD += inc(upper)
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s72 // limit -= inc)
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s73 // limit -= inc)
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32

/* Update M0 for DTLDS */

/* global read A */
/* g2l=0, load component 0 */
buffer_load_ubyte_d16 v[vgprG2LA+4+0], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0, sc0 // load one buffer value
/* g2l=0, load component 1 */
buffer_load_ubyte_d16 v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:1, sc0 // load one buffer value
/* g2l=0, load component 2 */
buffer_load_ubyte_d16_hi v1, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:2, sc0 // load one buffer value
/* g2l=0, load component 3 */
buffer_load_ubyte_d16_hi v2, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:3, sc0 // load one buffer value
/* g2l=0, load component 4 */
buffer_load_ubyte_d16 v[vgprG2LA+4+1], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:4, sc0 // load one buffer value
/* g2l=0, load component 5 */
buffer_load_ubyte_d16 v4, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:5, sc0 // load one buffer value
/* g2l=0, load component 6 */
buffer_load_ubyte_d16_hi v5, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:6, sc0 // load one buffer value
/* g2l=0, load component 7 */
buffer_load_ubyte_d16_hi v6, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:7, sc0 // load one buffer value
/* g2l=0, load component 8 */
buffer_load_ubyte_d16 v[vgprG2LA+4+2], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:8, sc0 // load one buffer value
/* g2l=0, load component 9 */
buffer_load_ubyte_d16 v8, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:9, sc0 // load one buffer value
/* g2l=0, load component 10 */
buffer_load_ubyte_d16_hi v9, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:10, sc0 // load one buffer value
/* g2l=0, load component 11 */
buffer_load_ubyte_d16_hi v10, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:11, sc0 // load one buffer value
/* g2l=0, load component 12 */
buffer_load_ubyte_d16 v[vgprG2LA+4+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:12, sc0 // load one buffer value
/* g2l=0, load component 13 */
buffer_load_ubyte_d16 v12, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:13, sc0 // load one buffer value
/* g2l=0, load component 14 */
buffer_load_ubyte_d16_hi v13, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:14, sc0 // load one buffer value
/* g2l=0, load component 15 */
buffer_load_ubyte_d16_hi v14, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:15, sc0 // load one buffer value
s_waitcnt vmcnt(14)
v_lshlrev_b32 v0, 0x8, v0                          // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+4+0], v[vgprG2LA+4+0], v0      // pack a sub 8-bit with dest
s_waitcnt vmcnt(13)
v_or_b32 v[vgprG2LA+4+0], v[vgprG2LA+4+0], v1      // pack a sub 8-bit with dest
s_waitcnt vmcnt(12)
v_lshlrev_b32 v2, 0x8, v2                          // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+4+0], v[vgprG2LA+4+0], v2      // pack a sub 8-bit with dest
s_waitcnt vmcnt(10)
v_lshlrev_b32 v4, 0x8, v4                          // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+4+1], v[vgprG2LA+4+1], v4      // pack a sub 8-bit with dest
s_waitcnt vmcnt(9)
v_or_b32 v[vgprG2LA+4+1], v[vgprG2LA+4+1], v5      // pack a sub 8-bit with dest
s_waitcnt vmcnt(8)
v_lshlrev_b32 v6, 0x8, v6                          // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+4+1], v[vgprG2LA+4+1], v6      // pack a sub 8-bit with dest
s_waitcnt vmcnt(6)
v_lshlrev_b32 v8, 0x8, v8                          // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+4+2], v[vgprG2LA+4+2], v8      // pack a sub 8-bit with dest
s_waitcnt vmcnt(5)
v_or_b32 v[vgprG2LA+4+2], v[vgprG2LA+4+2], v9      // pack a sub 8-bit with dest
s_waitcnt vmcnt(4)
v_lshlrev_b32 v10, 0x8, v10                        // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+4+2], v[vgprG2LA+4+2], v10     // pack a sub 8-bit with dest
s_waitcnt vmcnt(2)
v_lshlrev_b32 v12, 0x8, v12                        // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+4+3], v[vgprG2LA+4+3], v12     // pack a sub 8-bit with dest
s_waitcnt vmcnt(1)
v_or_b32 v[vgprG2LA+4+3], v[vgprG2LA+4+3], v13     // pack a sub 8-bit with dest
s_waitcnt vmcnt(0)
v_lshlrev_b32 v14, 0x8, v14                        // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+4+3], v[vgprG2LA+4+3], v14     // pack a sub 8-bit with dest
/* g2l=8, load component 0 */
buffer_load_ubyte_d16 v[vgprG2LA+12+0], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0, sc0 // load one buffer value
/* g2l=8, load component 1 */
buffer_load_ubyte_d16 v0, v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:1, sc0 // load one buffer value
/* g2l=8, load component 2 */
buffer_load_ubyte_d16_hi v1, v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:2, sc0 // load one buffer value
/* g2l=8, load component 3 */
buffer_load_ubyte_d16_hi v2, v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:3, sc0 // load one buffer value
/* g2l=8, load component 4 */
buffer_load_ubyte_d16 v[vgprG2LA+12+1], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:4, sc0 // load one buffer value
/* g2l=8, load component 5 */
buffer_load_ubyte_d16 v4, v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:5, sc0 // load one buffer value
/* g2l=8, load component 6 */
buffer_load_ubyte_d16_hi v5, v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:6, sc0 // load one buffer value
/* g2l=8, load component 7 */
buffer_load_ubyte_d16_hi v6, v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:7, sc0 // load one buffer value
/* g2l=8, load component 8 */
buffer_load_ubyte_d16 v[vgprG2LA+12+2], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:8, sc0 // load one buffer value
/* g2l=8, load component 9 */
buffer_load_ubyte_d16 v8, v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:9, sc0 // load one buffer value
/* g2l=8, load component 10 */
buffer_load_ubyte_d16_hi v9, v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:10, sc0 // load one buffer value
/* g2l=8, load component 11 */
buffer_load_ubyte_d16_hi v10, v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:11, sc0 // load one buffer value
/* g2l=8, load component 12 */
buffer_load_ubyte_d16 v[vgprG2LA+12+3], v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:12, sc0 // load one buffer value
/* g2l=8, load component 13 */
buffer_load_ubyte_d16 v12, v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:13, sc0 // load one buffer value
/* g2l=8, load component 14 */
buffer_load_ubyte_d16_hi v13, v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:14, sc0 // load one buffer value
/* g2l=8, load component 15 */
buffer_load_ubyte_d16_hi v14, v[vgprGlobalReadOffsetA+1], s[sgprSrdA:sgprSrdA+3], 0 offen offset:15, sc0 // load one buffer value
s_waitcnt vmcnt(14)
v_lshlrev_b32 v0, 0x8, v0                          // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+12+0], v[vgprG2LA+12+0], v0    // pack a sub 8-bit with dest
s_waitcnt vmcnt(13)
v_or_b32 v[vgprG2LA+12+0], v[vgprG2LA+12+0], v1    // pack a sub 8-bit with dest
s_waitcnt vmcnt(12)
v_lshlrev_b32 v2, 0x8, v2                          // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+12+0], v[vgprG2LA+12+0], v2    // pack a sub 8-bit with dest
s_waitcnt vmcnt(10)
v_lshlrev_b32 v4, 0x8, v4                          // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+12+1], v[vgprG2LA+12+1], v4    // pack a sub 8-bit with dest
s_waitcnt vmcnt(9)
v_or_b32 v[vgprG2LA+12+1], v[vgprG2LA+12+1], v5    // pack a sub 8-bit with dest
s_waitcnt vmcnt(8)
v_lshlrev_b32 v6, 0x8, v6                          // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+12+1], v[vgprG2LA+12+1], v6    // pack a sub 8-bit with dest
s_waitcnt vmcnt(6)
v_lshlrev_b32 v8, 0x8, v8                          // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+12+2], v[vgprG2LA+12+2], v8    // pack a sub 8-bit with dest
s_waitcnt vmcnt(5)
v_or_b32 v[vgprG2LA+12+2], v[vgprG2LA+12+2], v9    // pack a sub 8-bit with dest
s_waitcnt vmcnt(4)
v_lshlrev_b32 v10, 0x8, v10                        // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+12+2], v[vgprG2LA+12+2], v10   // pack a sub 8-bit with dest
s_waitcnt vmcnt(2)
v_lshlrev_b32 v12, 0x8, v12                        // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+12+3], v[vgprG2LA+12+3], v12   // pack a sub 8-bit with dest
s_waitcnt vmcnt(1)
v_or_b32 v[vgprG2LA+12+3], v[vgprG2LA+12+3], v13   // pack a sub 8-bit with dest
s_waitcnt vmcnt(0)
v_lshlrev_b32 v14, 0x8, v14                        // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+12+3], v[vgprG2LA+12+3], v14   // pack a sub 8-bit with dest
/* g2l=16, load component 0 */
buffer_load_ubyte_d16 v[vgprG2LA+20+0], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0, sc0 // load one buffer value
/* g2l=16, load component 1 */
buffer_load_ubyte_d16 v0, v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:1, sc0 // load one buffer value
/* g2l=16, load component 2 */
buffer_load_ubyte_d16_hi v1, v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:2, sc0 // load one buffer value
/* g2l=16, load component 3 */
buffer_load_ubyte_d16_hi v2, v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:3, sc0 // load one buffer value
/* g2l=16, load component 4 */
buffer_load_ubyte_d16 v[vgprG2LA+20+1], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:4, sc0 // load one buffer value
/* g2l=16, load component 5 */
buffer_load_ubyte_d16 v4, v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:5, sc0 // load one buffer value
/* g2l=16, load component 6 */
buffer_load_ubyte_d16_hi v5, v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:6, sc0 // load one buffer value
/* g2l=16, load component 7 */
buffer_load_ubyte_d16_hi v6, v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:7, sc0 // load one buffer value
/* g2l=16, load component 8 */
buffer_load_ubyte_d16 v[vgprG2LA+20+2], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:8, sc0 // load one buffer value
/* g2l=16, load component 9 */
buffer_load_ubyte_d16 v8, v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:9, sc0 // load one buffer value
/* g2l=16, load component 10 */
buffer_load_ubyte_d16_hi v9, v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:10, sc0 // load one buffer value
/* g2l=16, load component 11 */
buffer_load_ubyte_d16_hi v10, v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:11, sc0 // load one buffer value
/* g2l=16, load component 12 */
buffer_load_ubyte_d16 v[vgprG2LA+20+3], v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:12, sc0 // load one buffer value
/* g2l=16, load component 13 */
buffer_load_ubyte_d16 v12, v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:13, sc0 // load one buffer value
/* g2l=16, load component 14 */
buffer_load_ubyte_d16_hi v13, v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:14, sc0 // load one buffer value
/* g2l=16, load component 15 */
buffer_load_ubyte_d16_hi v14, v[vgprGlobalReadOffsetA+2], s[sgprSrdA:sgprSrdA+3], 0 offen offset:15, sc0 // load one buffer value
s_waitcnt vmcnt(14)
v_lshlrev_b32 v0, 0x8, v0                          // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+20+0], v[vgprG2LA+20+0], v0    // pack a sub 8-bit with dest
s_waitcnt vmcnt(13)
v_or_b32 v[vgprG2LA+20+0], v[vgprG2LA+20+0], v1    // pack a sub 8-bit with dest
s_waitcnt vmcnt(12)
v_lshlrev_b32 v2, 0x8, v2                          // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+20+0], v[vgprG2LA+20+0], v2    // pack a sub 8-bit with dest
s_waitcnt vmcnt(10)
v_lshlrev_b32 v4, 0x8, v4                          // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+20+1], v[vgprG2LA+20+1], v4    // pack a sub 8-bit with dest
s_waitcnt vmcnt(9)
v_or_b32 v[vgprG2LA+20+1], v[vgprG2LA+20+1], v5    // pack a sub 8-bit with dest
s_waitcnt vmcnt(8)
v_lshlrev_b32 v6, 0x8, v6                          // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+20+1], v[vgprG2LA+20+1], v6    // pack a sub 8-bit with dest
s_waitcnt vmcnt(6)
v_lshlrev_b32 v8, 0x8, v8                          // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+20+2], v[vgprG2LA+20+2], v8    // pack a sub 8-bit with dest
s_waitcnt vmcnt(5)
v_or_b32 v[vgprG2LA+20+2], v[vgprG2LA+20+2], v9    // pack a sub 8-bit with dest
s_waitcnt vmcnt(4)
v_lshlrev_b32 v10, 0x8, v10                        // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+20+2], v[vgprG2LA+20+2], v10   // pack a sub 8-bit with dest
s_waitcnt vmcnt(2)
v_lshlrev_b32 v12, 0x8, v12                        // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+20+3], v[vgprG2LA+20+3], v12   // pack a sub 8-bit with dest
s_waitcnt vmcnt(1)
v_or_b32 v[vgprG2LA+20+3], v[vgprG2LA+20+3], v13   // pack a sub 8-bit with dest
s_waitcnt vmcnt(0)
v_lshlrev_b32 v14, 0x8, v14                        // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+20+3], v[vgprG2LA+20+3], v14   // pack a sub 8-bit with dest
/* g2l=24, load component 0 */
buffer_load_ubyte_d16 v[vgprG2LA+28+0], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0, sc0 // load one buffer value
/* g2l=24, load component 1 */
buffer_load_ubyte_d16 v0, v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:1, sc0 // load one buffer value
/* g2l=24, load component 2 */
buffer_load_ubyte_d16_hi v1, v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:2, sc0 // load one buffer value
/* g2l=24, load component 3 */
buffer_load_ubyte_d16_hi v2, v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:3, sc0 // load one buffer value
/* g2l=24, load component 4 */
buffer_load_ubyte_d16 v[vgprG2LA+28+1], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:4, sc0 // load one buffer value
/* g2l=24, load component 5 */
buffer_load_ubyte_d16 v4, v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:5, sc0 // load one buffer value
/* g2l=24, load component 6 */
buffer_load_ubyte_d16_hi v5, v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:6, sc0 // load one buffer value
/* g2l=24, load component 7 */
buffer_load_ubyte_d16_hi v6, v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:7, sc0 // load one buffer value
/* g2l=24, load component 8 */
buffer_load_ubyte_d16 v[vgprG2LA+28+2], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:8, sc0 // load one buffer value
/* g2l=24, load component 9 */
buffer_load_ubyte_d16 v8, v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:9, sc0 // load one buffer value
/* g2l=24, load component 10 */
buffer_load_ubyte_d16_hi v9, v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:10, sc0 // load one buffer value
/* g2l=24, load component 11 */
buffer_load_ubyte_d16_hi v10, v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:11, sc0 // load one buffer value
/* g2l=24, load component 12 */
buffer_load_ubyte_d16 v[vgprG2LA+28+3], v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:12, sc0 // load one buffer value
/* g2l=24, load component 13 */
buffer_load_ubyte_d16 v12, v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:13, sc0 // load one buffer value
/* g2l=24, load component 14 */
buffer_load_ubyte_d16_hi v13, v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:14, sc0 // load one buffer value
/* g2l=24, load component 15 */
buffer_load_ubyte_d16_hi v14, v[vgprGlobalReadOffsetA+3], s[sgprSrdA:sgprSrdA+3], 0 offen offset:15, sc0 // load one buffer value
s_waitcnt vmcnt(14)
v_lshlrev_b32 v0, 0x8, v0                          // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+28+0], v[vgprG2LA+28+0], v0    // pack a sub 8-bit with dest
s_waitcnt vmcnt(13)
v_or_b32 v[vgprG2LA+28+0], v[vgprG2LA+28+0], v1    // pack a sub 8-bit with dest
s_waitcnt vmcnt(12)
v_lshlrev_b32 v2, 0x8, v2                          // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+28+0], v[vgprG2LA+28+0], v2    // pack a sub 8-bit with dest
s_waitcnt vmcnt(10)
v_lshlrev_b32 v4, 0x8, v4                          // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+28+1], v[vgprG2LA+28+1], v4    // pack a sub 8-bit with dest
s_waitcnt vmcnt(9)
v_or_b32 v[vgprG2LA+28+1], v[vgprG2LA+28+1], v5    // pack a sub 8-bit with dest
s_waitcnt vmcnt(8)
v_lshlrev_b32 v6, 0x8, v6                          // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+28+1], v[vgprG2LA+28+1], v6    // pack a sub 8-bit with dest
s_waitcnt vmcnt(6)
v_lshlrev_b32 v8, 0x8, v8                          // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+28+2], v[vgprG2LA+28+2], v8    // pack a sub 8-bit with dest
s_waitcnt vmcnt(5)
v_or_b32 v[vgprG2LA+28+2], v[vgprG2LA+28+2], v9    // pack a sub 8-bit with dest
s_waitcnt vmcnt(4)
v_lshlrev_b32 v10, 0x8, v10                        // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+28+2], v[vgprG2LA+28+2], v10   // pack a sub 8-bit with dest
s_waitcnt vmcnt(2)
v_lshlrev_b32 v12, 0x8, v12                        // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+28+3], v[vgprG2LA+28+3], v12   // pack a sub 8-bit with dest
s_waitcnt vmcnt(1)
v_or_b32 v[vgprG2LA+28+3], v[vgprG2LA+28+3], v13   // pack a sub 8-bit with dest
s_waitcnt vmcnt(0)
v_lshlrev_b32 v14, 0x8, v14                        // shift left to higher 8 bits
v_or_b32 v[vgprG2LA+28+3], v[vgprG2LA+28+3], v14   // pack a sub 8-bit with dest

/* Update M0 for DTLDS */

/* global read B */
/* g2l=0, load component 0 */
buffer_load_dword v[vgprG2LB+0+0], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // load packed 2X half buffer value
/* g2l=0, load component 2 */
buffer_load_dword v[vgprG2LB+0+1], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:4 // load packed 2X half buffer value
s_waitcnt vmcnt(0)                                 // 2wait for global read
// Skip force waitcnt0
s_barrier

/* local write a */
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+4+0] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+0], v0 dst_sel:WORD_0   // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+0], v1 dst_sel:WORD_1   // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+4+0] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+1], v0 dst_sel:WORD_0   // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+1], v1 dst_sel:WORD_1   // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+4+1] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+2], v0 dst_sel:WORD_0   // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+2], v1 dst_sel:WORD_1   // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+4+1] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+3], v0 dst_sel:WORD_0   // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+3], v1 dst_sel:WORD_1   // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+4+2] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+4], v0 dst_sel:WORD_0   // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+4], v1 dst_sel:WORD_1   // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+4+2] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+5], v0 dst_sel:WORD_0   // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+5], v1 dst_sel:WORD_1   // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+4+3] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+6], v0 dst_sel:WORD_0   // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+6], v1 dst_sel:WORD_1   // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+4+3] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+0+7], v0 dst_sel:WORD_0   // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+0+7], v1 dst_sel:WORD_1   // Convert to FP16
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+0:vgprG2LA+0+3] offset:0 // lwoA_0_0_0_0 = (0*LSCA) + (0*LSPA)(*MT0I+PAD) = 0
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+4:vgprG2LA+4+3] offset:16 // lwoA_0_0_0_0 = (0*LSCA) + (0*LSPA)(*MT0I+PAD) = 0
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+12+0] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+0], v0 dst_sel:WORD_0   // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+0], v1 dst_sel:WORD_1   // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+12+0] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+1], v0 dst_sel:WORD_0   // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+1], v1 dst_sel:WORD_1   // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+12+1] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+2], v0 dst_sel:WORD_0   // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+2], v1 dst_sel:WORD_1   // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+12+1] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+3], v0 dst_sel:WORD_0   // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+3], v1 dst_sel:WORD_1   // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+12+2] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+4], v0 dst_sel:WORD_0   // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+4], v1 dst_sel:WORD_1   // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+12+2] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+5], v0 dst_sel:WORD_0   // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+5], v1 dst_sel:WORD_1   // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+12+3] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+6], v0 dst_sel:WORD_0   // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+6], v1 dst_sel:WORD_1   // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+12+3] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+8+7], v0 dst_sel:WORD_0   // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+8+7], v1 dst_sel:WORD_1   // Convert to FP16
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+8:vgprG2LA+8+3] offset:2304 // lwoA_0_0_1_0 = (0*LSCA) + (1*LSPA)(*MT0I+PAD) = 2304
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+12:vgprG2LA+12+3] offset:2320 // lwoA_0_0_1_0 = (0*LSCA) + (1*LSPA)(*MT0I+PAD) = 2304
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+20+0] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+0], v0 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+0], v1 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+20+0] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+1], v0 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+1], v1 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+20+1] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+2], v0 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+2], v1 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+20+1] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+3], v0 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+3], v1 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+20+2] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+4], v0 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+4], v1 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+20+2] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+5], v0 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+5], v1 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+20+3] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+6], v0 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+6], v1 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+20+3] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+16+7], v0 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+16+7], v1 dst_sel:WORD_1  // Convert to FP16
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+16:vgprG2LA+16+3] offset:4608 // lwoA_0_0_2_0 = (0*LSCA) + (2*LSPA)(*MT0I+PAD) = 4608
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+20:vgprG2LA+20+3] offset:4624 // lwoA_0_0_2_0 = (0*LSCA) + (2*LSPA)(*MT0I+PAD) = 4608
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+28+0] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+0], v0 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+0], v1 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+28+0] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+1], v0 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+1], v1 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+28+1] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+2], v0 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+2], v1 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+28+1] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+3], v0 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+3], v1 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+28+2] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+4], v0 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+4], v1 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+28+2] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+5], v0 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+5], v1 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+28+3] src0_sel:WORD_0 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+6], v0 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+6], v1 dst_sel:WORD_1  // Convert to FP16
v_cvt_pk_f32_fp8 v[0:1], v[vgprG2LA+28+3] src0_sel:WORD_1 // convert to F32
v_cvt_f16_f32 v[vgprG2LA+24+7], v0 dst_sel:WORD_0  // Convert to FP16
v_cvt_f16_f32 v[vgprG2LA+24+7], v1 dst_sel:WORD_1  // Convert to FP16
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+24:vgprG2LA+24+3] offset:6912 // lwoA_0_0_3_0 = (0*LSCA) + (3*LSPA)(*MT0I+PAD) = 6912
ds_write_b128 v[vgprLocalWriteAddrA], v[vgprG2LA+28:vgprG2LA+28+3] offset:6928 // lwoA_0_0_3_0 = (0*LSCA) + (3*LSPA)(*MT0I+PAD) = 6912

/* local write b */
ds_write_b64 v[vgprLocalWriteAddrB], v[vgprG2LB+0:vgprG2LB+0+1] offset:0 // lwoB_0_0_0_0 = (0*LSCB)*(MT1J+PAD) + (0*LSPB) = 0

/* Recalc local read offsets */
/* lr0I */
v_and_b32 v1, 63, v[vgprSerial]                    // 0. thread id in wave: wtid = tid % wavelength(64)
v_and_b32 v0, 15, v1                               // 1. N offset: nIdx = wtid % MI_N(16)
                                                   // 1. N offset: nOffset = nIdx * nStride(1) (multiplier is 1, do nothing)
v_lshrrev_b32 v1, 4, v1                            // 2. block offset: bnIdx = wtid / dividedForBlkId(16)
v_and_b32 v1, 0, v1                                // 2. block offset: bnIdx = bnIdx % num1DBlocks(1)
v_lshlrev_b32 v1, 0x4, v1                          // 2. block offset: bnOffset = bnIdx * strideBlock(16)
v_add_u32 v0, v1, v0                               // 3. add N and block offset: bnOffset = block and N offset
v_lshlrev_b32 v0, 0x2, v0                          // 4. apply VectorWidth: bnOffset = bnOffset * vw(4)
v_and_b32 v1, 63, v[vgprSerial]                    // 5. thread id in wave: wtid = tid % wavelength(64)
v_lshrrev_b32 v1, 4, v1                            // 5. K offset: kIdx = wtid / (MIN(16) * MIBB(1))
v_lshlrev_b32 v1, 0xa, v1                          // 5. K offset: lrKOffset = kIdx * mStride(1024)
v_add_u32 v0, v1, v0                               // 6. offset in wave: lrOffset = bnOffset + lrKOffset
v_lshrrev_b32 v1, 6, v[vgprSerial]                 // 7. wave offset in N dimen: wtid = tid / dividedForWaveId(64)
v_and_b32 v1, 3, v1                                // 7. wave offset in M dimen: wtid0 = wtid / num1DWaves(4)
v_lshlrev_b32 v1, 0x6, v1                          // 7. wave offset in M dimen: wOffset = wtid0 * W0Stride(64)
v_add_u32 v0, v1, v0                               // 7. final local read offset: flrOffset = lrOffset + WOffset
/* lr1J */
v_and_b32 v2, 63, v[vgprSerial]                    // 0. thread id in wave: wtid = tid % wavelength(64)
v_and_b32 v1, 15, v2                               // 1. N offset: nIdx = wtid % MI_N(16)
v_lshlrev_b32 v1, 0x6, v1                          // 1. N offset: nOffset = nIdx * nStride(64)
v_lshrrev_b32 v2, 4, v2                            // 2. block offset: bnIdx = wtid / dividedForBlkId(16)
v_and_b32 v2, 0, v2                                // 2. block offset: bnIdx = bnIdx % num1DBlocks(1)
v_lshlrev_b32 v2, 0xa, v2                          // 2. block offset: bnOffset = bnIdx * strideBlock(1024)
v_add_u32 v1, v2, v1                               // 3. add N and block offset: bnOffset = block and N offset
                                                   // 4. apply VectorWidth: bnOffset = bnOffset * vw(1) (multiplier is 1, do nothing)
v_and_b32 v2, 63, v[vgprSerial]                    // 5. thread id in wave: wtid = tid % wavelength(64)
v_lshrrev_b32 v2, 4, v2                            // 5. K offset: kIdx = wtid / (MIN(16) * MIBB(1))
v_lshlrev_b32 v2, 0x2, v2                          // 5. K offset: lrKOffset = kIdx * mStride(4)
v_add_u32 v1, v2, v1                               // 6. offset in wave: lrOffset = bnOffset + lrKOffset
v_lshrrev_b32 v2, 8, v[vgprSerial]                 // LSU offset: sgid = Serial / subGroup(256)
s_mov_b32 s5, 256                                  // LSU offset: stride = MT0(256) + PAD0(0)
v_mul_lo_u32 v2, s5, v2                            // LSU offset: lsuoffset = sgid*(MT0+PAD)
v_add_lshl_u32 v[vgprLocalReadAddrA], v2, v0, 0x1  // Final Offset: offset = (lro0*VW+lsuoffset)*bpe
v_lshrrev_b32 v3, 7, v[vgprLocalReadAddrA]         // Final Offset: padding 8 per block 128
v_lshlrev_b32 v3, 0x4, v3                          // Final Offset: padding 8 per block 128
v_add_u32 v[vgprLocalReadAddrA], v3, v[vgprLocalReadAddrA] // Final Offset: add padding 8 per block 128
/* N/A */
v_lshrrev_b32 v0, 8, v[vgprSerial]                 // LSU offset: sgid = Serial / subGroup(256)
s_mov_b32 s5, 16                                   // LSU offset: stride = MT1(16) + PAD1(0)
v_mul_lo_u32 v0, s5, v0                            // LSU offset: lsuoffset = sgid*(MT1+PAD)
v_add_lshl_u32 v[vgprLocalReadAddrB], v0, v1, 0x1  // Final Offset: offset = (lro1*VW+lsuoffset)*bpe
v_lshrrev_b32 v2, 7, v[vgprLocalReadAddrB]         // Final Offset: padding 16 per block 128
v_lshlrev_b32 v2, 0x5, v2                          // Final Offset: padding 16 per block 128
v_add_u32 v[vgprLocalReadAddrB], v2, v[vgprLocalReadAddrB] // Final Offset: add padding 16 per block 128
v_add_co_u32 v[vgprLocalReadAddrB+0], vcc, 0x9000, v[vgprLocalReadAddrB+0] //  += LdsOffsetB (lower)
s_waitcnt lgkmcnt(0)                               // 5wait for local write
// Skip force waitcnt0
s_barrier

/* local read reset offsets a */

/* local read reset offsets b */

/* local read init pointers a */

/* localReadInitPointers */

/* local read init pointers b */

/* localReadInitPointers */

/* tail loop: macs */
label_TailLoopBeginL:

/* Tail: remove ValuA/B vgpr buffer [0...48) from pool */

/* Tail: add address/G2L vgpr [48...92) to pool */

/* local read a */
ds_read_b64 v[vgprValuA_X0_I0_D0+0:vgprValuA_X0_I0_D0+0+1], v[vgprLocalReadAddrA] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b64 v[vgprValuA_X0_I0_D1+0:vgprValuA_X0_I0_D1+0+1], v[vgprLocalReadAddrA] offset:576 // L -> Reg lro=0 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=1 oIdx=0 buffer=0 iui=0
ds_read_b64 v[vgprValuA_X0_I0_D2+0:vgprValuA_X0_I0_D2+0+1], v[vgprLocalReadAddrA] offset:1152 // L -> Reg lro=0 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=2 oIdx=0 buffer=0 iui=0
ds_read_b64 v[vgprValuA_X0_I0_D3+0:vgprValuA_X0_I0_D3+0+1], v[vgprLocalReadAddrA] offset:1728 // L -> Reg lro=0 swapByteOffset=0 ti=256 vIdx=0 eIdx=0 rIdx=3 oIdx=0 buffer=0 iui=0

/* local read b */
ds_read_b64 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+1], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0

/* local read inc a */
s_mov_b32 s5, 0x2400                               // inc
v_add_co_u32 v[vgprLocalReadAddrA], vcc, s5, v[vgprLocalReadAddrA] // lrA += 9216 (LSU*(MT+PAD)*bpe)

/* local read inc b */
s_mov_b32 s5, 0x20                                 // inc
v_add_co_u32 v[vgprLocalReadAddrB], vcc, s5, v[vgprLocalReadAddrB] // lrB += 32 (LSU*bpe)
s_waitcnt lgkmcnt(0)                               // 4wait for local read
v_perm_b32 v[vgprValuA_X0_I0+0], v[vgprValuA_X0_I0_D1+0], v[vgprValuA_X0_I0_D0+0], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X0_I0+1], v[vgprValuA_X0_I0_D3+0], v[vgprValuA_X0_I0_D2+0], s[sgprPackKForV0] // select K=23 for vector=0
v_perm_b32 v[vgprValuA_X0_I0+2], v[vgprValuA_X0_I0_D1+0], v[vgprValuA_X0_I0_D0+0], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X0_I0+3], v[vgprValuA_X0_I0_D3+0], v[vgprValuA_X0_I0_D2+0], s[sgprPackKForV1] // select K=23 for vector=1
v_perm_b32 v[vgprValuA_X0_I0+4], v[vgprValuA_X0_I0_D1+1], v[vgprValuA_X0_I0_D0+1], s[sgprPackKForV0] // select K=01 for vector=0
v_perm_b32 v[vgprValuA_X0_I0+5], v[vgprValuA_X0_I0_D3+1], v[vgprValuA_X0_I0_D2+1], s[sgprPackKForV0] // select K=23 for vector=0
v_perm_b32 v[vgprValuA_X0_I0+6], v[vgprValuA_X0_I0_D1+1], v[vgprValuA_X0_I0_D0+1], s[sgprPackKForV1] // select K=01 for vector=1
v_perm_b32 v[vgprValuA_X0_I0+7], v[vgprValuA_X0_I0_D3+1], v[vgprValuA_X0_I0_D2+1], s[sgprPackKForV1] // select K=23 for vector=1
v_and_b32 v48, 63, v[vgprSerial]                   // v48 = v[vgprSerial] % 64
v_lshrrev_b32 v48, 4, v48                          // v48 = v48 / 16
v_lshlrev_b32 v48, 0x2, v48                        // v48 = v48 * 4
v_cmp_ge_i32 s[72:73], v48, s[sgprLoopCounterL]    // check K index >= Size L
v_cndmask_b32 v[vgprValuA_X0_I0+0+0], v[vgprValuA_X0_I0+0+0], 0x0, s[72:73] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuA_X0_I0+2+0], v[vgprValuA_X0_I0+2+0], 0x0, s[72:73] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuA_X0_I0+4+0], v[vgprValuA_X0_I0+4+0], 0x0, s[72:73] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuA_X0_I0+6+0], v[vgprValuA_X0_I0+6+0], 0x0, s[72:73] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuA_X0_I0+0+1], v[vgprValuA_X0_I0+0+1], 0x0, s[72:73] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuA_X0_I0+2+1], v[vgprValuA_X0_I0+2+1], 0x0, s[72:73] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuA_X0_I0+4+1], v[vgprValuA_X0_I0+4+1], 0x0, s[72:73] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuA_X0_I0+6+1], v[vgprValuA_X0_I0+6+1], 0x0, s[72:73] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+0+0], v[vgprValuB_X0_I0+0+0], 0x0, s[72:73] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+0+1], v[vgprValuB_X0_I0+0+1], 0x0, s[72:73] // set 0 if K_idx >= sizeL
s_nop 1
v_mfma_f32_16x16x16_f16 acc[0:3], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
v_mfma_f32_16x16x16_f16 acc[4:7], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+2+0+0:vgprValuA_X0_I0+2+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
v_mfma_f32_16x16x16_f16 acc[8:11], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
v_mfma_f32_16x16x16_f16 acc[12:15], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+6+0+0:vgprValuA_X0_I0+6+0+0+1], acc[12:15] // left value = acc[12+0:15+0]

/* closeLoop loopL finalLoop=1 tailLoop=1 */
s_sub_i32 s[sgprLoopCounterL], s[sgprLoopCounterL], 0x10 // dec counterL (tailLoop)
s_add_u32 s[sgprOrigLoopCounter], s[sgprOrigLoopCounter], 0x10 // inc counterL
s_cmp_le_i32 s[sgprLoopCounterL], 0x0              // counterL<=0
s_cbranch_scc0 label_TailLoopBeginL                // restart LoopL
label_TailLoopEndL:
label_SkipTailLoopL:

/* Tail: remove address/G2L [48...92) from pool */
label_Summation_End_PU5V5JFVOS8NMBTM_0:
/* endSummation: add vgpr [0...92) to pool */
.set sgprStaggerUIter, UNDEF
.set sgprSrdA, UNDEF
.set sgprSrdB, UNDEF
.set sgprShadowLimitA, UNDEF
.set sgprShadowLimitB, UNDEF
.set sgprWrapUA, UNDEF
.set sgprWrapUB, UNDEF
.set sgprGlobalReadIncsA, UNDEF
.set sgprGlobalReadIncsB, UNDEF
.set sgprPackKForV0, UNDEF
.set sgprPackKForV1, UNDEF
/* load store sgprs */
.set sgprAddressScaleA, 52
.set sgprAddressScaleB, 54
.set sgprAddressScaleAlphaVec, 56
.set sgprAddressBias, 58
.set sgprBiasType, 60
.set sgprBiasStride, 61
.set sgpractivationAlpha, 62
.set sgpractivationBeta, 63
.set sgprActivationType, 64
s_cmp_eq_u32 s[sgprGSU], 1                         // GSU == 1 ?
s_load_dwordx8 s[52:59], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x5c
s_load_dwordx4 s[60:63], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x7c
s_load_dword s64, s[sgprKernArgAddress:sgprKernArgAddress+1], 0x8c
label_GSU_4:
/* load store sgprs2 */
.set sgprAddressTC, 68
.set sgprSynchronizer, 70
s_load_dwordx4 s[68:71], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x90
.set sgprSrdScaleAlphaVec, 72
.set sgprSrdBias, 76
.set sgprSrdTD, 80
.set sgprGSUSync, 5

/* self.SrdTC(kernel) */

/* Mapping of Acc register -> C Vgpr register */

/* shift vector components d0 */
v_mov_b32 v3, s[sgprWorkGroup0]
v_mul_i32_i24 v3, -0x100, v3                       // wg*MT
v_add_co_u32 v3, vcc, s[sgprSizesFree+0], v3       // wgMT = Size - wg*MT
v_mov_b32 v4, 0x100                                // MT
v_cmp_lt_u32 s[66:67], v3, v4                      // wgMT < MT
v_cndmask_b32 v3, v4, v3, s[66:67]                 // wgMT = (wgMT < MT) ? wgMT : MT
v_lshrrev_b32 v5, 6, v[vgprSerial]                 // v5 = v[vgprSerial] / 64
v_and_b32 v5, 3, v5                                // v5 = v5 % 4
v_lshrrev_b32 v6, 6, v3                            // v6 = v3 / 64
v_and_b32 v6, 3, v6                                // v6 = v6 % 4
v_cmp_eq_u32 s[66:67], v6, v5                      // wave_id == block_belong_to_wave?
v_cndmask_b32 v3, v4, v3, s[66:67]                 // wgMT = (wgMT < MT) ? wgMT : MT

/* mbReg: which mb block need to shift, mb(matrixInstCoal(16) * VectorWidth(4)) */
v_lshrrev_b32 v4, 6, v3                            // v4 = v3 / 64
v_lshlrev_b32 v6, 0x0, v5                          // v6 = v5 * 1
v_sub_u32 v4, v4, v6

/* gbReg: glvw block id */
v_lshrrev_b32 v6, 4, v3                            // v6 = v3 / 16

/* tgbReg: glvw block id */
v_lshrrev_b32 v7, 0, v[vgprSerial]                 // v7 = v[vgprSerial] / 1
v_and_b32 v7, 15, v7                               // v7 = v7 % 16
v_lshlrev_b32 v7, 0x2, v7                          // v7 = v7 * 4
v_lshrrev_b32 v7, 4, v7                            // v7 = v7 / 16
v_lshlrev_b32 v5, 0x2, v5                          // v5 = v5 * 4
v_add_co_u32 v7, vcc, v5, v7                       // tgbReg = (tid_coal * continOut) / GLVW
v_sub_u32 v6, v6, v7

/* vwReg: glvw in which vw block? */
v_and_b32 v5, 3, v3                                // permute register between threads
v_lshrrev_b32 v5, 4, v5                            // permute register between threads

/* rReg : reminder of M_size % GlobalReadVectorWidth */
v_and_b32 v7, 15, v3                               // v7 = v3 % 16
v_cmp_eq_u32 vcc, v7, 0x1                          // wgMT%VW == 1
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW1_0 // branch to shift d0 r=1
v_cmp_eq_u32 vcc, v7, 0x2                          // wgMT%VW == 2
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW2_0 // branch to shift d0 r=2
v_cmp_eq_u32 vcc, v7, 0x3                          // wgMT%VW == 3
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW3_0 // branch to shift d0 r=3
v_cmp_eq_u32 vcc, v7, 0x4                          // wgMT%VW == 4
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW4_0 // branch to shift d0 r=4
v_cmp_eq_u32 vcc, v7, 0x5                          // wgMT%VW == 5
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW5_0 // branch to shift d0 r=5
v_cmp_eq_u32 vcc, v7, 0x6                          // wgMT%VW == 6
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW6_0 // branch to shift d0 r=6
v_cmp_eq_u32 vcc, v7, 0x7                          // wgMT%VW == 7
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW7_0 // branch to shift d0 r=7
v_cmp_eq_u32 vcc, v7, 0x8                          // wgMT%VW == 8
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW8_0 // branch to shift d0 r=8
v_cmp_eq_u32 vcc, v7, 0x9                          // wgMT%VW == 9
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW9_0 // branch to shift d0 r=9
v_cmp_eq_u32 vcc, v7, 0xa                          // wgMT%VW == 10
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW10_0 // branch to shift d0 r=10
v_cmp_eq_u32 vcc, v7, 0xb                          // wgMT%VW == 11
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW11_0 // branch to shift d0 r=11
v_cmp_eq_u32 vcc, v7, 0xc                          // wgMT%VW == 12
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW12_0 // branch to shift d0 r=12
v_cmp_eq_u32 vcc, v7, 0xd                          // wgMT%VW == 13
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW13_0 // branch to shift d0 r=13
v_cmp_eq_u32 vcc, v7, 0xe                          // wgMT%VW == 14
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW14_0 // branch to shift d0 r=14
v_cmp_eq_u32 vcc, v7, 0xf                          // wgMT%VW == 15
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW15_0 // branch to shift d0 r=15
s_branch label_ShiftVectorComponents0_GLVW0_0      // no shifting

/******************************************/
/* shift d0 r=1                           */
/******************************************/
label_ShiftVectorComponents0_GLVW1_0:
v_cmp_eq_u32 vcc, v4, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW1_BM0_0 // branch to shift d0 r1 mb0

/******************************************/
/* shift d0 r=2                           */
/******************************************/
label_ShiftVectorComponents0_GLVW2_0:
v_cmp_eq_u32 vcc, v4, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW2_BM0_0 // branch to shift d0 r2 mb0

/******************************************/
/* shift d0 r=3                           */
/******************************************/
label_ShiftVectorComponents0_GLVW3_0:
v_cmp_eq_u32 vcc, v4, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW3_BM0_0 // branch to shift d0 r3 mb0

/******************************************/
/* shift d0 r=4                           */
/******************************************/
label_ShiftVectorComponents0_GLVW4_0:
v_cmp_eq_u32 vcc, v4, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW4_BM0_0 // branch to shift d0 r4 mb0

/******************************************/
/* shift d0 r=5                           */
/******************************************/
label_ShiftVectorComponents0_GLVW5_0:
v_cmp_eq_u32 vcc, v4, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW5_BM0_0 // branch to shift d0 r5 mb0

/******************************************/
/* shift d0 r=6                           */
/******************************************/
label_ShiftVectorComponents0_GLVW6_0:
v_cmp_eq_u32 vcc, v4, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW6_BM0_0 // branch to shift d0 r6 mb0

/******************************************/
/* shift d0 r=7                           */
/******************************************/
label_ShiftVectorComponents0_GLVW7_0:
v_cmp_eq_u32 vcc, v4, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW7_BM0_0 // branch to shift d0 r7 mb0

/******************************************/
/* shift d0 r=8                           */
/******************************************/
label_ShiftVectorComponents0_GLVW8_0:
v_cmp_eq_u32 vcc, v4, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW8_BM0_0 // branch to shift d0 r8 mb0

/******************************************/
/* shift d0 r=9                           */
/******************************************/
label_ShiftVectorComponents0_GLVW9_0:
v_cmp_eq_u32 vcc, v4, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW9_BM0_0 // branch to shift d0 r9 mb0

/******************************************/
/* shift d0 r=10                          */
/******************************************/
label_ShiftVectorComponents0_GLVW10_0:
v_cmp_eq_u32 vcc, v4, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW10_BM0_0 // branch to shift d0 r10 mb0

/******************************************/
/* shift d0 r=11                          */
/******************************************/
label_ShiftVectorComponents0_GLVW11_0:
v_cmp_eq_u32 vcc, v4, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW11_BM0_0 // branch to shift d0 r11 mb0

/******************************************/
/* shift d0 r=12                          */
/******************************************/
label_ShiftVectorComponents0_GLVW12_0:
v_cmp_eq_u32 vcc, v4, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW12_BM0_0 // branch to shift d0 r12 mb0

/******************************************/
/* shift d0 r=13                          */
/******************************************/
label_ShiftVectorComponents0_GLVW13_0:
v_cmp_eq_u32 vcc, v4, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW13_BM0_0 // branch to shift d0 r13 mb0

/******************************************/
/* shift d0 r=14                          */
/******************************************/
label_ShiftVectorComponents0_GLVW14_0:
v_cmp_eq_u32 vcc, v4, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW14_BM0_0 // branch to shift d0 r14 mb0

/******************************************/
/* shift d0 r=15                          */
/******************************************/
label_ShiftVectorComponents0_GLVW15_0:
v_cmp_eq_u32 vcc, v4, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW15_BM0_0 // branch to shift d0 r15 mb0

/******************************************/
/* shift d0 r=1 mb=0                      */
/******************************************/
label_ShiftVectorComponents0_GLVW1_BM0_0:  /// r1 mb0
v_cmp_eq_u32 vcc, v5, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW1_BM0_VW0_0 // branch to shift d0 r1 mb0 vw0

/******************************************/
/* shift d0 r=2 mb=0                      */
/******************************************/
label_ShiftVectorComponents0_GLVW2_BM0_0:  /// r2 mb0
v_cmp_eq_u32 vcc, v5, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW2_BM0_VW0_0 // branch to shift d0 r2 mb0 vw0

/******************************************/
/* shift d0 r=3 mb=0                      */
/******************************************/
label_ShiftVectorComponents0_GLVW3_BM0_0:  /// r3 mb0
v_cmp_eq_u32 vcc, v5, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW3_BM0_VW0_0 // branch to shift d0 r3 mb0 vw0

/******************************************/
/* shift d0 r=4 mb=0                      */
/******************************************/
label_ShiftVectorComponents0_GLVW4_BM0_0:  /// r4 mb0
v_cmp_eq_u32 vcc, v5, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW4_BM0_VW0_0 // branch to shift d0 r4 mb0 vw0

/******************************************/
/* shift d0 r=5 mb=0                      */
/******************************************/
label_ShiftVectorComponents0_GLVW5_BM0_0:  /// r5 mb0
v_cmp_eq_u32 vcc, v5, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW5_BM0_VW0_0 // branch to shift d0 r5 mb0 vw0

/******************************************/
/* shift d0 r=6 mb=0                      */
/******************************************/
label_ShiftVectorComponents0_GLVW6_BM0_0:  /// r6 mb0
v_cmp_eq_u32 vcc, v5, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW6_BM0_VW0_0 // branch to shift d0 r6 mb0 vw0

/******************************************/
/* shift d0 r=7 mb=0                      */
/******************************************/
label_ShiftVectorComponents0_GLVW7_BM0_0:  /// r7 mb0
v_cmp_eq_u32 vcc, v5, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW7_BM0_VW0_0 // branch to shift d0 r7 mb0 vw0

/******************************************/
/* shift d0 r=8 mb=0                      */
/******************************************/
label_ShiftVectorComponents0_GLVW8_BM0_0:  /// r8 mb0
v_cmp_eq_u32 vcc, v5, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW8_BM0_VW0_0 // branch to shift d0 r8 mb0 vw0

/******************************************/
/* shift d0 r=9 mb=0                      */
/******************************************/
label_ShiftVectorComponents0_GLVW9_BM0_0:  /// r9 mb0
v_cmp_eq_u32 vcc, v5, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW9_BM0_VW0_0 // branch to shift d0 r9 mb0 vw0

/******************************************/
/* shift d0 r=10 mb=0                     */
/******************************************/
label_ShiftVectorComponents0_GLVW10_BM0_0:  /// r10 mb0
v_cmp_eq_u32 vcc, v5, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW10_BM0_VW0_0 // branch to shift d0 r10 mb0 vw0

/******************************************/
/* shift d0 r=11 mb=0                     */
/******************************************/
label_ShiftVectorComponents0_GLVW11_BM0_0:  /// r11 mb0
v_cmp_eq_u32 vcc, v5, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW11_BM0_VW0_0 // branch to shift d0 r11 mb0 vw0

/******************************************/
/* shift d0 r=12 mb=0                     */
/******************************************/
label_ShiftVectorComponents0_GLVW12_BM0_0:  /// r12 mb0
v_cmp_eq_u32 vcc, v5, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW12_BM0_VW0_0 // branch to shift d0 r12 mb0 vw0

/******************************************/
/* shift d0 r=13 mb=0                     */
/******************************************/
label_ShiftVectorComponents0_GLVW13_BM0_0:  /// r13 mb0
v_cmp_eq_u32 vcc, v5, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW13_BM0_VW0_0 // branch to shift d0 r13 mb0 vw0

/******************************************/
/* shift d0 r=14 mb=0                     */
/******************************************/
label_ShiftVectorComponents0_GLVW14_BM0_0:  /// r14 mb0
v_cmp_eq_u32 vcc, v5, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW14_BM0_VW0_0 // branch to shift d0 r14 mb0 vw0

/******************************************/
/* shift d0 r=15 mb=0                     */
/******************************************/
label_ShiftVectorComponents0_GLVW15_BM0_0:  /// r15 mb0
v_cmp_eq_u32 vcc, v5, 0x0
s_cbranch_vccnz label_ShiftVectorComponents0_GLVW15_BM0_VW0_0 // branch to shift d0 r15 mb0 vw0

/******************************************/
/* shift d0 r=1 mb=0 vw0                  */
/******************************************/
label_ShiftVectorComponents0_GLVW1_BM0_VW0_0:  /// r1 mb0 vw0
s_mov_b32 s66, 0
v_cmpx_eq_u32 s[66:67], v6, s66                    // is thread in edge glvw region
v_and_b32 v0, 63, v[vgprSerial]                    // permute register between threads
v_lshlrev_b32 v0, 2, v0                            // permute register between threads
v_accvgpr_read_b32 v7, acc12                       // glvw 1 mb 0 tt1 0 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:12               // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc0, v7
v_accvgpr_read_b32 v7, acc13                       // glvw 1 mb 0 tt1 1 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:12               // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc1, v7
v_accvgpr_read_b32 v7, acc14                       // glvw 1 mb 0 tt1 2 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:12               // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc2, v7
v_accvgpr_read_b32 v7, acc15                       // glvw 1 mb 0 tt1 3 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:12               // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc3, v7
s_mov_b64 s[66:67], 0xFFFFFFFFFFFFFFFF             // to restore all threads active
s_or_saveexec_b64 vcc, s[66:67]                    // all threads active
s_branch label_ShiftVectorComponents0_GLVW0_0      // done shifting


/******************************************/
/* shift d0 r=2 mb=0 vw0                  */
/******************************************/
label_ShiftVectorComponents0_GLVW2_BM0_VW0_0:  /// r2 mb0 vw0
s_mov_b32 s66, 0
v_cmpx_eq_u32 s[66:67], v6, s66                    // is thread in edge glvw region
v_and_b32 v0, 63, v[vgprSerial]                    // permute register between threads
v_lshlrev_b32 v0, 2, v0                            // permute register between threads
v_accvgpr_read_b32 v7, acc8                        // glvw 2 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v8, acc12                       // glvw 2 mb 0 tt1 0 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:12               // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:12               // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc0, v7
v_accvgpr_write_b32 acc4, v8
v_accvgpr_read_b32 v7, acc9                        // glvw 2 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v8, acc13                       // glvw 2 mb 0 tt1 1 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:12               // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:12               // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc1, v7
v_accvgpr_write_b32 acc5, v8
v_accvgpr_read_b32 v7, acc10                       // glvw 2 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v8, acc14                       // glvw 2 mb 0 tt1 2 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:12               // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:12               // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc2, v7
v_accvgpr_write_b32 acc6, v8
v_accvgpr_read_b32 v7, acc11                       // glvw 2 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v8, acc15                       // glvw 2 mb 0 tt1 3 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:12               // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:12               // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc3, v7
v_accvgpr_write_b32 acc7, v8
s_mov_b64 s[66:67], 0xFFFFFFFFFFFFFFFF             // to restore all threads active
s_or_saveexec_b64 vcc, s[66:67]                    // all threads active
s_branch label_ShiftVectorComponents0_GLVW0_0      // done shifting


/******************************************/
/* shift d0 r=3 mb=0 vw0                  */
/******************************************/
label_ShiftVectorComponents0_GLVW3_BM0_VW0_0:  /// r3 mb0 vw0
s_mov_b32 s66, 0
v_cmpx_eq_u32 s[66:67], v6, s66                    // is thread in edge glvw region
v_and_b32 v0, 63, v[vgprSerial]                    // permute register between threads
v_lshlrev_b32 v0, 2, v0                            // permute register between threads
v_accvgpr_read_b32 v7, acc4                        // glvw 3 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v8, acc8                        // glvw 3 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v9, acc12                       // glvw 3 mb 0 tt1 0 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:12               // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:12               // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:12               // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc0, v7
v_accvgpr_write_b32 acc4, v8
v_accvgpr_write_b32 acc8, v9
v_accvgpr_read_b32 v7, acc5                        // glvw 3 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v8, acc9                        // glvw 3 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v9, acc13                       // glvw 3 mb 0 tt1 1 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:12               // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:12               // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:12               // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc1, v7
v_accvgpr_write_b32 acc5, v8
v_accvgpr_write_b32 acc9, v9
v_accvgpr_read_b32 v7, acc6                        // glvw 3 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v8, acc10                       // glvw 3 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v9, acc14                       // glvw 3 mb 0 tt1 2 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:12               // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:12               // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:12               // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc2, v7
v_accvgpr_write_b32 acc6, v8
v_accvgpr_write_b32 acc10, v9
v_accvgpr_read_b32 v7, acc7                        // glvw 3 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v8, acc11                       // glvw 3 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v9, acc15                       // glvw 3 mb 0 tt1 3 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:12               // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:12               // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:12               // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc3, v7
v_accvgpr_write_b32 acc7, v8
v_accvgpr_write_b32 acc11, v9
s_mov_b64 s[66:67], 0xFFFFFFFFFFFFFFFF             // to restore all threads active
s_or_saveexec_b64 vcc, s[66:67]                    // all threads active
s_branch label_ShiftVectorComponents0_GLVW0_0      // done shifting


/******************************************/
/* shift d0 r=4 mb=0 vw0                  */
/******************************************/
label_ShiftVectorComponents0_GLVW4_BM0_VW0_0:  /// r4 mb0 vw0
s_mov_b32 s66, 0
v_cmpx_eq_u32 s[66:67], v6, s66                    // is thread in edge glvw region
v_and_b32 v0, 63, v[vgprSerial]                    // permute register between threads
v_lshlrev_b32 v0, 2, v0                            // permute register between threads
v_accvgpr_read_b32 v7, acc0                        // glvw 4 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v8, acc4                        // glvw 4 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v9, acc8                        // glvw 4 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v10, acc12                      // glvw 4 mb 0 tt1 0 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:12               // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:12               // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:12               // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:12             // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc0, v7
v_accvgpr_write_b32 acc4, v8
v_accvgpr_write_b32 acc8, v9
v_accvgpr_write_b32 acc12, v10
v_accvgpr_read_b32 v7, acc1                        // glvw 4 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v8, acc5                        // glvw 4 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v9, acc9                        // glvw 4 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v10, acc13                      // glvw 4 mb 0 tt1 1 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:12               // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:12               // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:12               // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:12             // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc1, v7
v_accvgpr_write_b32 acc5, v8
v_accvgpr_write_b32 acc9, v9
v_accvgpr_write_b32 acc13, v10
v_accvgpr_read_b32 v7, acc2                        // glvw 4 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v8, acc6                        // glvw 4 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v9, acc10                       // glvw 4 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v10, acc14                      // glvw 4 mb 0 tt1 2 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:12               // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:12               // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:12               // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:12             // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc2, v7
v_accvgpr_write_b32 acc6, v8
v_accvgpr_write_b32 acc10, v9
v_accvgpr_write_b32 acc14, v10
v_accvgpr_read_b32 v7, acc3                        // glvw 4 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v8, acc7                        // glvw 4 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v9, acc11                       // glvw 4 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v10, acc15                      // glvw 4 mb 0 tt1 3 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:12               // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:12               // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:12               // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:12             // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc3, v7
v_accvgpr_write_b32 acc7, v8
v_accvgpr_write_b32 acc11, v9
v_accvgpr_write_b32 acc15, v10
s_mov_b64 s[66:67], 0xFFFFFFFFFFFFFFFF             // to restore all threads active
s_or_saveexec_b64 vcc, s[66:67]                    // all threads active
s_branch label_ShiftVectorComponents0_GLVW0_0      // done shifting


/******************************************/
/* shift d0 r=5 mb=0 vw0                  */
/******************************************/
label_ShiftVectorComponents0_GLVW5_BM0_VW0_0:  /// r5 mb0 vw0
s_mov_b32 s66, 0
v_cmpx_eq_u32 s[66:67], v6, s66                    // is thread in edge glvw region
v_and_b32 v0, 63, v[vgprSerial]                    // permute register between threads
v_lshlrev_b32 v0, 2, v0                            // permute register between threads
v_accvgpr_read_b32 v7, acc12                       // glvw 5 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v8, acc0                        // glvw 5 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v9, acc4                        // glvw 5 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v10, acc8                       // glvw 5 mb 0 tt1 0 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:8                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:12               // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:12               // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:12             // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc0, v7
v_accvgpr_write_b32 acc4, v8
v_accvgpr_write_b32 acc8, v9
v_accvgpr_write_b32 acc12, v10
v_accvgpr_read_b32 v7, acc13                       // glvw 5 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v8, acc1                        // glvw 5 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v9, acc5                        // glvw 5 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v10, acc9                       // glvw 5 mb 0 tt1 1 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:8                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:12               // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:12               // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:12             // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc1, v7
v_accvgpr_write_b32 acc5, v8
v_accvgpr_write_b32 acc9, v9
v_accvgpr_write_b32 acc13, v10
v_accvgpr_read_b32 v7, acc14                       // glvw 5 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v8, acc2                        // glvw 5 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v9, acc6                        // glvw 5 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v10, acc10                      // glvw 5 mb 0 tt1 2 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:8                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:12               // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:12               // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:12             // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc2, v7
v_accvgpr_write_b32 acc6, v8
v_accvgpr_write_b32 acc10, v9
v_accvgpr_write_b32 acc14, v10
v_accvgpr_read_b32 v7, acc15                       // glvw 5 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v8, acc3                        // glvw 5 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v9, acc7                        // glvw 5 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v10, acc11                      // glvw 5 mb 0 tt1 3 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:8                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:12               // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:12               // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:12             // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc3, v7
v_accvgpr_write_b32 acc7, v8
v_accvgpr_write_b32 acc11, v9
v_accvgpr_write_b32 acc15, v10
s_mov_b64 s[66:67], 0xFFFFFFFFFFFFFFFF             // to restore all threads active
s_or_saveexec_b64 vcc, s[66:67]                    // all threads active
s_branch label_ShiftVectorComponents0_GLVW0_0      // done shifting


/******************************************/
/* shift d0 r=6 mb=0 vw0                  */
/******************************************/
label_ShiftVectorComponents0_GLVW6_BM0_VW0_0:  /// r6 mb0 vw0
s_mov_b32 s66, 0
v_cmpx_eq_u32 s[66:67], v6, s66                    // is thread in edge glvw region
v_and_b32 v0, 63, v[vgprSerial]                    // permute register between threads
v_lshlrev_b32 v0, 2, v0                            // permute register between threads
v_accvgpr_read_b32 v7, acc8                        // glvw 6 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v8, acc12                       // glvw 6 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v9, acc0                        // glvw 6 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v10, acc4                       // glvw 6 mb 0 tt1 0 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:8                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:8                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:12               // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:12             // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc0, v7
v_accvgpr_write_b32 acc4, v8
v_accvgpr_write_b32 acc8, v9
v_accvgpr_write_b32 acc12, v10
v_accvgpr_read_b32 v7, acc9                        // glvw 6 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v8, acc13                       // glvw 6 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v9, acc1                        // glvw 6 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v10, acc5                       // glvw 6 mb 0 tt1 1 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:8                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:8                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:12               // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:12             // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc1, v7
v_accvgpr_write_b32 acc5, v8
v_accvgpr_write_b32 acc9, v9
v_accvgpr_write_b32 acc13, v10
v_accvgpr_read_b32 v7, acc10                       // glvw 6 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v8, acc14                       // glvw 6 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v9, acc2                        // glvw 6 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v10, acc6                       // glvw 6 mb 0 tt1 2 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:8                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:8                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:12               // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:12             // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc2, v7
v_accvgpr_write_b32 acc6, v8
v_accvgpr_write_b32 acc10, v9
v_accvgpr_write_b32 acc14, v10
v_accvgpr_read_b32 v7, acc11                       // glvw 6 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v8, acc15                       // glvw 6 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v9, acc3                        // glvw 6 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v10, acc7                       // glvw 6 mb 0 tt1 3 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:8                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:8                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:12               // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:12             // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc3, v7
v_accvgpr_write_b32 acc7, v8
v_accvgpr_write_b32 acc11, v9
v_accvgpr_write_b32 acc15, v10
s_mov_b64 s[66:67], 0xFFFFFFFFFFFFFFFF             // to restore all threads active
s_or_saveexec_b64 vcc, s[66:67]                    // all threads active
s_branch label_ShiftVectorComponents0_GLVW0_0      // done shifting


/******************************************/
/* shift d0 r=7 mb=0 vw0                  */
/******************************************/
label_ShiftVectorComponents0_GLVW7_BM0_VW0_0:  /// r7 mb0 vw0
s_mov_b32 s66, 0
v_cmpx_eq_u32 s[66:67], v6, s66                    // is thread in edge glvw region
v_and_b32 v0, 63, v[vgprSerial]                    // permute register between threads
v_lshlrev_b32 v0, 2, v0                            // permute register between threads
v_accvgpr_read_b32 v7, acc4                        // glvw 7 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v8, acc8                        // glvw 7 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v9, acc12                       // glvw 7 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v10, acc0                       // glvw 7 mb 0 tt1 0 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:8                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:8                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:8                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:12             // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc0, v7
v_accvgpr_write_b32 acc4, v8
v_accvgpr_write_b32 acc8, v9
v_accvgpr_write_b32 acc12, v10
v_accvgpr_read_b32 v7, acc5                        // glvw 7 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v8, acc9                        // glvw 7 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v9, acc13                       // glvw 7 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v10, acc1                       // glvw 7 mb 0 tt1 1 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:8                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:8                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:8                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:12             // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc1, v7
v_accvgpr_write_b32 acc5, v8
v_accvgpr_write_b32 acc9, v9
v_accvgpr_write_b32 acc13, v10
v_accvgpr_read_b32 v7, acc6                        // glvw 7 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v8, acc10                       // glvw 7 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v9, acc14                       // glvw 7 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v10, acc2                       // glvw 7 mb 0 tt1 2 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:8                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:8                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:8                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:12             // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc2, v7
v_accvgpr_write_b32 acc6, v8
v_accvgpr_write_b32 acc10, v9
v_accvgpr_write_b32 acc14, v10
v_accvgpr_read_b32 v7, acc7                        // glvw 7 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v8, acc11                       // glvw 7 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v9, acc15                       // glvw 7 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v10, acc3                       // glvw 7 mb 0 tt1 3 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:8                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:8                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:8                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:12             // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc3, v7
v_accvgpr_write_b32 acc7, v8
v_accvgpr_write_b32 acc11, v9
v_accvgpr_write_b32 acc15, v10
s_mov_b64 s[66:67], 0xFFFFFFFFFFFFFFFF             // to restore all threads active
s_or_saveexec_b64 vcc, s[66:67]                    // all threads active
s_branch label_ShiftVectorComponents0_GLVW0_0      // done shifting


/******************************************/
/* shift d0 r=8 mb=0 vw0                  */
/******************************************/
label_ShiftVectorComponents0_GLVW8_BM0_VW0_0:  /// r8 mb0 vw0
s_mov_b32 s66, 0
v_cmpx_eq_u32 s[66:67], v6, s66                    // is thread in edge glvw region
v_and_b32 v0, 63, v[vgprSerial]                    // permute register between threads
v_lshlrev_b32 v0, 2, v0                            // permute register between threads
v_accvgpr_read_b32 v7, acc0                        // glvw 8 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v8, acc4                        // glvw 8 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v9, acc8                        // glvw 8 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v10, acc12                      // glvw 8 mb 0 tt1 0 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:8                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:8                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:8                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:8              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc0, v7
v_accvgpr_write_b32 acc4, v8
v_accvgpr_write_b32 acc8, v9
v_accvgpr_write_b32 acc12, v10
v_accvgpr_read_b32 v7, acc1                        // glvw 8 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v8, acc5                        // glvw 8 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v9, acc9                        // glvw 8 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v10, acc13                      // glvw 8 mb 0 tt1 1 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:8                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:8                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:8                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:8              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc1, v7
v_accvgpr_write_b32 acc5, v8
v_accvgpr_write_b32 acc9, v9
v_accvgpr_write_b32 acc13, v10
v_accvgpr_read_b32 v7, acc2                        // glvw 8 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v8, acc6                        // glvw 8 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v9, acc10                       // glvw 8 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v10, acc14                      // glvw 8 mb 0 tt1 2 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:8                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:8                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:8                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:8              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc2, v7
v_accvgpr_write_b32 acc6, v8
v_accvgpr_write_b32 acc10, v9
v_accvgpr_write_b32 acc14, v10
v_accvgpr_read_b32 v7, acc3                        // glvw 8 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v8, acc7                        // glvw 8 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v9, acc11                       // glvw 8 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v10, acc15                      // glvw 8 mb 0 tt1 3 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:8                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:8                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:8                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:8              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc3, v7
v_accvgpr_write_b32 acc7, v8
v_accvgpr_write_b32 acc11, v9
v_accvgpr_write_b32 acc15, v10
s_mov_b64 s[66:67], 0xFFFFFFFFFFFFFFFF             // to restore all threads active
s_or_saveexec_b64 vcc, s[66:67]                    // all threads active
s_branch label_ShiftVectorComponents0_GLVW0_0      // done shifting


/******************************************/
/* shift d0 r=9 mb=0 vw0                  */
/******************************************/
label_ShiftVectorComponents0_GLVW9_BM0_VW0_0:  /// r9 mb0 vw0
s_mov_b32 s66, 0
v_cmpx_eq_u32 s[66:67], v6, s66                    // is thread in edge glvw region
v_and_b32 v0, 63, v[vgprSerial]                    // permute register between threads
v_lshlrev_b32 v0, 2, v0                            // permute register between threads
v_accvgpr_read_b32 v7, acc12                       // glvw 9 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v8, acc0                        // glvw 9 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v9, acc4                        // glvw 9 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v10, acc8                       // glvw 9 mb 0 tt1 0 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:4                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:8                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:8                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:8              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc0, v7
v_accvgpr_write_b32 acc4, v8
v_accvgpr_write_b32 acc8, v9
v_accvgpr_write_b32 acc12, v10
v_accvgpr_read_b32 v7, acc13                       // glvw 9 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v8, acc1                        // glvw 9 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v9, acc5                        // glvw 9 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v10, acc9                       // glvw 9 mb 0 tt1 1 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:4                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:8                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:8                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:8              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc1, v7
v_accvgpr_write_b32 acc5, v8
v_accvgpr_write_b32 acc9, v9
v_accvgpr_write_b32 acc13, v10
v_accvgpr_read_b32 v7, acc14                       // glvw 9 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v8, acc2                        // glvw 9 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v9, acc6                        // glvw 9 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v10, acc10                      // glvw 9 mb 0 tt1 2 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:4                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:8                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:8                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:8              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc2, v7
v_accvgpr_write_b32 acc6, v8
v_accvgpr_write_b32 acc10, v9
v_accvgpr_write_b32 acc14, v10
v_accvgpr_read_b32 v7, acc15                       // glvw 9 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v8, acc3                        // glvw 9 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v9, acc7                        // glvw 9 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v10, acc11                      // glvw 9 mb 0 tt1 3 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:4                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:8                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:8                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:8              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc3, v7
v_accvgpr_write_b32 acc7, v8
v_accvgpr_write_b32 acc11, v9
v_accvgpr_write_b32 acc15, v10
s_mov_b64 s[66:67], 0xFFFFFFFFFFFFFFFF             // to restore all threads active
s_or_saveexec_b64 vcc, s[66:67]                    // all threads active
s_branch label_ShiftVectorComponents0_GLVW0_0      // done shifting


/******************************************/
/* shift d0 r=10 mb=0 vw0                 */
/******************************************/
label_ShiftVectorComponents0_GLVW10_BM0_VW0_0:  /// r10 mb0 vw0
s_mov_b32 s66, 0
v_cmpx_eq_u32 s[66:67], v6, s66                    // is thread in edge glvw region
v_and_b32 v0, 63, v[vgprSerial]                    // permute register between threads
v_lshlrev_b32 v0, 2, v0                            // permute register between threads
v_accvgpr_read_b32 v7, acc8                        // glvw 10 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v8, acc12                       // glvw 10 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v9, acc0                        // glvw 10 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v10, acc4                       // glvw 10 mb 0 tt1 0 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:4                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:4                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:8                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:8              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc0, v7
v_accvgpr_write_b32 acc4, v8
v_accvgpr_write_b32 acc8, v9
v_accvgpr_write_b32 acc12, v10
v_accvgpr_read_b32 v7, acc9                        // glvw 10 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v8, acc13                       // glvw 10 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v9, acc1                        // glvw 10 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v10, acc5                       // glvw 10 mb 0 tt1 1 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:4                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:4                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:8                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:8              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc1, v7
v_accvgpr_write_b32 acc5, v8
v_accvgpr_write_b32 acc9, v9
v_accvgpr_write_b32 acc13, v10
v_accvgpr_read_b32 v7, acc10                       // glvw 10 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v8, acc14                       // glvw 10 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v9, acc2                        // glvw 10 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v10, acc6                       // glvw 10 mb 0 tt1 2 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:4                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:4                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:8                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:8              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc2, v7
v_accvgpr_write_b32 acc6, v8
v_accvgpr_write_b32 acc10, v9
v_accvgpr_write_b32 acc14, v10
v_accvgpr_read_b32 v7, acc11                       // glvw 10 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v8, acc15                       // glvw 10 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v9, acc3                        // glvw 10 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v10, acc7                       // glvw 10 mb 0 tt1 3 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:4                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:4                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:8                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:8              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc3, v7
v_accvgpr_write_b32 acc7, v8
v_accvgpr_write_b32 acc11, v9
v_accvgpr_write_b32 acc15, v10
s_mov_b64 s[66:67], 0xFFFFFFFFFFFFFFFF             // to restore all threads active
s_or_saveexec_b64 vcc, s[66:67]                    // all threads active
s_branch label_ShiftVectorComponents0_GLVW0_0      // done shifting


/******************************************/
/* shift d0 r=11 mb=0 vw0                 */
/******************************************/
label_ShiftVectorComponents0_GLVW11_BM0_VW0_0:  /// r11 mb0 vw0
s_mov_b32 s66, 0
v_cmpx_eq_u32 s[66:67], v6, s66                    // is thread in edge glvw region
v_and_b32 v0, 63, v[vgprSerial]                    // permute register between threads
v_lshlrev_b32 v0, 2, v0                            // permute register between threads
v_accvgpr_read_b32 v7, acc4                        // glvw 11 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v8, acc8                        // glvw 11 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v9, acc12                       // glvw 11 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v10, acc0                       // glvw 11 mb 0 tt1 0 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:4                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:4                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:4                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:8              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc0, v7
v_accvgpr_write_b32 acc4, v8
v_accvgpr_write_b32 acc8, v9
v_accvgpr_write_b32 acc12, v10
v_accvgpr_read_b32 v7, acc5                        // glvw 11 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v8, acc9                        // glvw 11 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v9, acc13                       // glvw 11 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v10, acc1                       // glvw 11 mb 0 tt1 1 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:4                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:4                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:4                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:8              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc1, v7
v_accvgpr_write_b32 acc5, v8
v_accvgpr_write_b32 acc9, v9
v_accvgpr_write_b32 acc13, v10
v_accvgpr_read_b32 v7, acc6                        // glvw 11 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v8, acc10                       // glvw 11 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v9, acc14                       // glvw 11 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v10, acc2                       // glvw 11 mb 0 tt1 2 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:4                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:4                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:4                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:8              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc2, v7
v_accvgpr_write_b32 acc6, v8
v_accvgpr_write_b32 acc10, v9
v_accvgpr_write_b32 acc14, v10
v_accvgpr_read_b32 v7, acc7                        // glvw 11 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v8, acc11                       // glvw 11 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v9, acc15                       // glvw 11 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v10, acc3                       // glvw 11 mb 0 tt1 3 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:4                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:4                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:4                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:8              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc3, v7
v_accvgpr_write_b32 acc7, v8
v_accvgpr_write_b32 acc11, v9
v_accvgpr_write_b32 acc15, v10
s_mov_b64 s[66:67], 0xFFFFFFFFFFFFFFFF             // to restore all threads active
s_or_saveexec_b64 vcc, s[66:67]                    // all threads active
s_branch label_ShiftVectorComponents0_GLVW0_0      // done shifting


/******************************************/
/* shift d0 r=12 mb=0 vw0                 */
/******************************************/
label_ShiftVectorComponents0_GLVW12_BM0_VW0_0:  /// r12 mb0 vw0
s_mov_b32 s66, 0
v_cmpx_eq_u32 s[66:67], v6, s66                    // is thread in edge glvw region
v_and_b32 v0, 63, v[vgprSerial]                    // permute register between threads
v_lshlrev_b32 v0, 2, v0                            // permute register between threads
v_accvgpr_read_b32 v7, acc0                        // glvw 12 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v8, acc4                        // glvw 12 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v9, acc8                        // glvw 12 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v10, acc12                      // glvw 12 mb 0 tt1 0 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:4                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:4                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:4                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:4              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc0, v7
v_accvgpr_write_b32 acc4, v8
v_accvgpr_write_b32 acc8, v9
v_accvgpr_write_b32 acc12, v10
v_accvgpr_read_b32 v7, acc1                        // glvw 12 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v8, acc5                        // glvw 12 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v9, acc9                        // glvw 12 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v10, acc13                      // glvw 12 mb 0 tt1 1 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:4                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:4                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:4                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:4              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc1, v7
v_accvgpr_write_b32 acc5, v8
v_accvgpr_write_b32 acc9, v9
v_accvgpr_write_b32 acc13, v10
v_accvgpr_read_b32 v7, acc2                        // glvw 12 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v8, acc6                        // glvw 12 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v9, acc10                       // glvw 12 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v10, acc14                      // glvw 12 mb 0 tt1 2 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:4                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:4                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:4                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:4              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc2, v7
v_accvgpr_write_b32 acc6, v8
v_accvgpr_write_b32 acc10, v9
v_accvgpr_write_b32 acc14, v10
v_accvgpr_read_b32 v7, acc3                        // glvw 12 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v8, acc7                        // glvw 12 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v9, acc11                       // glvw 12 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v10, acc15                      // glvw 12 mb 0 tt1 3 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v7, v0, v7 offset:4                // permute edge values
ds_bpermute_b32 v8, v0, v8 offset:4                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:4                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:4              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc3, v7
v_accvgpr_write_b32 acc7, v8
v_accvgpr_write_b32 acc11, v9
v_accvgpr_write_b32 acc15, v10
s_mov_b64 s[66:67], 0xFFFFFFFFFFFFFFFF             // to restore all threads active
s_or_saveexec_b64 vcc, s[66:67]                    // all threads active
s_branch label_ShiftVectorComponents0_GLVW0_0      // done shifting


/******************************************/
/* shift d0 r=13 mb=0 vw0                 */
/******************************************/
label_ShiftVectorComponents0_GLVW13_BM0_VW0_0:  /// r13 mb0 vw0
s_mov_b32 s66, 0
v_cmpx_eq_u32 s[66:67], v6, s66                    // is thread in edge glvw region
v_and_b32 v0, 63, v[vgprSerial]                    // permute register between threads
v_lshlrev_b32 v0, 2, v0                            // permute register between threads
v_accvgpr_read_b32 v7, acc12                       // glvw 13 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v8, acc0                        // glvw 13 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v9, acc4                        // glvw 13 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v10, acc8                       // glvw 13 mb 0 tt1 0 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v8, v0, v8 offset:4                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:4                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:4              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc0, v7
v_accvgpr_write_b32 acc4, v8
v_accvgpr_write_b32 acc8, v9
v_accvgpr_write_b32 acc12, v10
v_accvgpr_read_b32 v7, acc13                       // glvw 13 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v8, acc1                        // glvw 13 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v9, acc5                        // glvw 13 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v10, acc9                       // glvw 13 mb 0 tt1 1 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v8, v0, v8 offset:4                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:4                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:4              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc1, v7
v_accvgpr_write_b32 acc5, v8
v_accvgpr_write_b32 acc9, v9
v_accvgpr_write_b32 acc13, v10
v_accvgpr_read_b32 v7, acc14                       // glvw 13 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v8, acc2                        // glvw 13 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v9, acc6                        // glvw 13 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v10, acc10                      // glvw 13 mb 0 tt1 2 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v8, v0, v8 offset:4                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:4                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:4              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc2, v7
v_accvgpr_write_b32 acc6, v8
v_accvgpr_write_b32 acc10, v9
v_accvgpr_write_b32 acc14, v10
v_accvgpr_read_b32 v7, acc15                       // glvw 13 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v8, acc3                        // glvw 13 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v9, acc7                        // glvw 13 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v10, acc11                      // glvw 13 mb 0 tt1 3 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v8, v0, v8 offset:4                // permute edge values
ds_bpermute_b32 v9, v0, v9 offset:4                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:4              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc3, v7
v_accvgpr_write_b32 acc7, v8
v_accvgpr_write_b32 acc11, v9
v_accvgpr_write_b32 acc15, v10
s_mov_b64 s[66:67], 0xFFFFFFFFFFFFFFFF             // to restore all threads active
s_or_saveexec_b64 vcc, s[66:67]                    // all threads active
s_branch label_ShiftVectorComponents0_GLVW0_0      // done shifting


/******************************************/
/* shift d0 r=14 mb=0 vw0                 */
/******************************************/
label_ShiftVectorComponents0_GLVW14_BM0_VW0_0:  /// r14 mb0 vw0
s_mov_b32 s66, 0
v_cmpx_eq_u32 s[66:67], v6, s66                    // is thread in edge glvw region
v_and_b32 v0, 63, v[vgprSerial]                    // permute register between threads
v_lshlrev_b32 v0, 2, v0                            // permute register between threads
v_accvgpr_read_b32 v7, acc8                        // glvw 14 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v8, acc12                       // glvw 14 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v9, acc0                        // glvw 14 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v10, acc4                       // glvw 14 mb 0 tt1 0 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v9, v0, v9 offset:4                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:4              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc0, v7
v_accvgpr_write_b32 acc4, v8
v_accvgpr_write_b32 acc8, v9
v_accvgpr_write_b32 acc12, v10
v_accvgpr_read_b32 v7, acc9                        // glvw 14 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v8, acc13                       // glvw 14 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v9, acc1                        // glvw 14 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v10, acc5                       // glvw 14 mb 0 tt1 1 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v9, v0, v9 offset:4                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:4              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc1, v7
v_accvgpr_write_b32 acc5, v8
v_accvgpr_write_b32 acc9, v9
v_accvgpr_write_b32 acc13, v10
v_accvgpr_read_b32 v7, acc10                       // glvw 14 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v8, acc14                       // glvw 14 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v9, acc2                        // glvw 14 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v10, acc6                       // glvw 14 mb 0 tt1 2 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v9, v0, v9 offset:4                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:4              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc2, v7
v_accvgpr_write_b32 acc6, v8
v_accvgpr_write_b32 acc10, v9
v_accvgpr_write_b32 acc14, v10
v_accvgpr_read_b32 v7, acc11                       // glvw 14 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v8, acc15                       // glvw 14 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v9, acc3                        // glvw 14 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v10, acc7                       // glvw 14 mb 0 tt1 3 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v9, v0, v9 offset:4                // permute edge values
ds_bpermute_b32 v10, v0, v10 offset:4              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc3, v7
v_accvgpr_write_b32 acc7, v8
v_accvgpr_write_b32 acc11, v9
v_accvgpr_write_b32 acc15, v10
s_mov_b64 s[66:67], 0xFFFFFFFFFFFFFFFF             // to restore all threads active
s_or_saveexec_b64 vcc, s[66:67]                    // all threads active
s_branch label_ShiftVectorComponents0_GLVW0_0      // done shifting


/******************************************/
/* shift d0 r=15 mb=0 vw0                 */
/******************************************/
label_ShiftVectorComponents0_GLVW15_BM0_VW0_0:  /// r15 mb0 vw0
s_mov_b32 s66, 0
v_cmpx_eq_u32 s[66:67], v6, s66                    // is thread in edge glvw region
v_and_b32 v0, 63, v[vgprSerial]                    // permute register between threads
v_lshlrev_b32 v0, 2, v0                            // permute register between threads
v_accvgpr_read_b32 v7, acc4                        // glvw 15 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v8, acc8                        // glvw 15 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v9, acc12                       // glvw 15 mb 0 tt1 0 r 0
v_accvgpr_read_b32 v10, acc0                       // glvw 15 mb 0 tt1 0 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v10, v0, v10 offset:4              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc0, v7
v_accvgpr_write_b32 acc4, v8
v_accvgpr_write_b32 acc8, v9
v_accvgpr_write_b32 acc12, v10
v_accvgpr_read_b32 v7, acc5                        // glvw 15 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v8, acc9                        // glvw 15 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v9, acc13                       // glvw 15 mb 0 tt1 1 r 0
v_accvgpr_read_b32 v10, acc1                       // glvw 15 mb 0 tt1 1 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v10, v0, v10 offset:4              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc1, v7
v_accvgpr_write_b32 acc5, v8
v_accvgpr_write_b32 acc9, v9
v_accvgpr_write_b32 acc13, v10
v_accvgpr_read_b32 v7, acc6                        // glvw 15 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v8, acc10                       // glvw 15 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v9, acc14                       // glvw 15 mb 0 tt1 2 r 0
v_accvgpr_read_b32 v10, acc2                       // glvw 15 mb 0 tt1 2 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v10, v0, v10 offset:4              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc2, v7
v_accvgpr_write_b32 acc6, v8
v_accvgpr_write_b32 acc10, v9
v_accvgpr_write_b32 acc14, v10
v_accvgpr_read_b32 v7, acc7                        // glvw 15 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v8, acc11                       // glvw 15 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v9, acc15                       // glvw 15 mb 0 tt1 3 r 0
v_accvgpr_read_b32 v10, acc3                       // glvw 15 mb 0 tt1 3 r 0
s_nop 1                                            // v_accvgpr read vgpr after write vgpr: 2 wait states
ds_bpermute_b32 v10, v0, v10 offset:4              // permute edge values
s_waitcnt 0                                        // (Wait all)
v_accvgpr_write_b32 acc3, v7
v_accvgpr_write_b32 acc7, v8
v_accvgpr_write_b32 acc11, v9
v_accvgpr_write_b32 acc15, v10
s_mov_b64 s[66:67], 0xFFFFFFFFFFFFFFFF             // to restore all threads active
s_or_saveexec_b64 vcc, s[66:67]                    // all threads active
s_branch label_ShiftVectorComponents0_GLVW0_0      // done shifting

label_ShiftVectorComponents0_GLVW0_0:  /// end shift0

/* not-LocalSplitU: global write indices */
/* computeStoreVgprs */
v_lshrrev_b32 v4, 6, v[vgprSerial]                 // v4 = v[vgprSerial] / 64
v_lshrrev_b32 v5, 2, v4                            // v5 = v4 / 4
v_mul_lo_u32 v5, 0x10, v5                          // wave coordination offset 1
v_and_b32 v1, 63, v[vgprSerial]                    // v1 = v[vgprSerial] % 64
v_lshrrev_b32 v1, 4, v1                            // v1 = v1 / 16
v_lshlrev_b32 v1, 0x2, v1                          // thread0 * continuous_output
v_add_lshl_u32 v1, v5, v1, 0                       // coordination 1 = vwB *(wave_id1 + tid1)
v_mul_lo_u32 v2, v1, s[sgprStrideC1J]              //  offset 1
v_mul_lo_u32 v3, v1, s[sgprStrideD1J]              //  offset 1
v_and_b32 v0, 3, v4                                // v0 = v4 % 4
v_mul_lo_u32 v0, 0x10, v0                          // wave coordination offset 0
v_and_b32 v5, 15, v[vgprSerial]                    // v5 = v[vgprSerial] % 16
v_add_lshl_u32 v0, v5, v0, 2                       // coordination 0 = vwA * (wave_id0 + tid0)
s_mul_i32 s51, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_add_u32 v0, s51, v0                              // coord 0 = (tid0/MI_m)*4 + waveG0*MIB_m + MT0*SG0
s_mul_i32 s51, 16, s[sgprWorkGroup1]               // wgp1 * MT1
v_add_u32 v1, s51, v1                              // coord 1 = (tid0%MI_m) + waveG1*MIB_n + MT1*SG1

/* not-LocalSplitU: global write */

/******************************************/
/* Global Write Elements                  */
/******************************************/
//s_endpgm
s_waitcnt lgkmcnt(0)                               // wait for 68 bytes of kern args.
s_cmp_eq_u32 s[sgprGSU], 1                         // GSU == 1 ?
s_cbranch_scc0 label_NoBranch_3Q7PU5R2DVI6B06E_0   // Only branch on scc1
// long branch if GSU == 1
s_getpc_b64 s[84:85]                               // addr of next instr
s_add_i32 s86, label_GSU_5, 0x4                    // target branch offset
s_add_u32 s84, s84, s86                            // add target branch offset
s_addc_u32 s85, s85, 0                             // add high and carry
s_setpc_b64 s[84:85]                               // branch to label_GSU_5
label_NoBranch_3Q7PU5R2DVI6B06E_0:
s_mov_b32 s51, 1.0                                 // init as 1
s_cmp_eq_u64 s[sgprAddressScaleA:sgprAddressScaleA+1], 0 // s[AddressScaleA] == 0 ?
s_cbranch_scc1 label_ScaleAValid                   // branch if s[AddressScaleA] == 0
s_load_dword s51, s[sgprAddressScaleA:sgprAddressScaleA+1], 0 // load scaleA
label_ScaleAValid:
s_mov_b32 s65, 1.0                                 // init as 1
s_cmp_eq_u64 s[sgprAddressScaleB:sgprAddressScaleB+1], 0 // s[AddressScaleB] == 0 ?
s_cbranch_scc1 label_ScaleBValid                   // branch if s[AddressScaleB] == 0
s_load_dword s65, s[sgprAddressScaleB:sgprAddressScaleB+1], 0 // load scaleB
label_ScaleBValid:
s_mov_b32 s[sgprSrdScaleAlphaVec+0], s[sgprAddressScaleAlphaVec+0] // init SRD base address (lower)
s_mov_b32 s[sgprSrdScaleAlphaVec+1], s[sgprAddressScaleAlphaVec+1] // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdScaleAlphaVec+3], Srd127_96     // Set bits 127_96 in post-loop SRD
s_cmp_eq_u64 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], 0 // s[AddressScaleAlphaVec] == 0 ?
s_cbranch_scc0 label_ScaleAlphaVecAddrValid        // branch if s[AddressScaleAlphaVec] != 0
s_mov_b32 s[sgprSrdScaleAlphaVec+2], 0
s_branch label_ScaleAlphaVecAddrValid_End
label_ScaleAlphaVecAddrValid:
s_mov_b32 s[sgprSrdScaleAlphaVec+2], s[sgprSizeI]
label_ScaleAlphaVecAddrValid_End:

s_mul_i32 s[sgprSrdScaleAlphaVec+2], 0x4, s[sgprSrdScaleAlphaVec+2] // ScaleAlphaVec scaled by BPE

s_mov_b32 s[sgprSrdTD+3], Srd127_96                // Set bits 127_96 in post-loop SRD
s_mov_b32 s[sgprSrdTD+2], 0x80000000
s_mul_i32 s66, MT1, s[sgprWorkGroup1]
s_mul_hi_u32 s85, s66, s[sgprStrideC1J]
s_mul_i32 s84, s66, s[sgprStrideC1J]
s_lshl_b64 s[84:85], s[84:85], 1                   // scale by bpe
s_add_u32 s[sgprSrdTD+0], s[sgprAddressTC+0], s84
s_addc_u32 s[sgprSrdTD+1], s[sgprAddressTC+1], s85
s_mul_hi_u32 s85, s[sgprStrideCK], s[sgprWorkGroup2]
s_mul_i32 s84, s[sgprStrideCK], s[sgprWorkGroup2]
s_lshl_b64 s[84:85], s[84:85], 1                   // scale by bpe
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s84
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], s85
s_add_u32 s66, s[sgprWorkGroup2], 0x1
s_mul_i32 s66, s[sgprBiasStride], s66              // stride * (wg+1)
s_cmp_eq_u32 s66, 0x0                              // bias stride = 0?
s_cselect_b32 s66, s[sgprSizeI], s66
s_mov_b32 s[sgprSrdBias+0], s[sgprAddressBias+0]   // init SRD base address (lower)
s_mov_b32 s[sgprSrdBias+1], s[sgprAddressBias+1]   // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdBias+3], Srd127_96              // Set bits 127_96 in post-loop SRD
s_cmp_eq_u64 s[sgprAddressBias:sgprAddressBias+1], 0 // s[AddressBias] == 0 ?
s_cbranch_scc0 label_BiasAddrValid                 // branch if s[AddressBias] != 0
s_mov_b32 s[sgprSrdBias+2], 0
s_branch label_BiasAddrValid_End
label_BiasAddrValid:
s_mov_b32 s[sgprSrdBias+2], s66
label_BiasAddrValid_End:

label_Load_Biasf32:
s_cmpk_lg_u32 s[sgprBiasType], 0                   // BiasType != 0
s_cbranch_scc1 label_Load_Biasf16                  // Branch if true

/******************************************/
/* Read Bias to LDS                       */
/******************************************/
s_mul_i32 s[sgprSrdBias+2], 0x4, s[sgprSrdBias+2]  // scaled by BPE
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_add_u32 v8, s66, v[vgprSerial]                   // coord 0 = wgp0 * MT0 + thread offset
s_mul_i32 s66, s[sgprBiasStride], s[sgprWorkGroup2] // Stride * WG
v_add_u32 v8, s66, v8                              // coord 0 = wgp0 * MT0 + thread offset + Stride * WG
v_lshlrev_b32 v8, 0x2, v8                          // Global bias address scaled by BPE
buffer_load_dword v4, v8, s[sgprSrdBias:sgprSrdBias+3], 0 offen offset:0 // load bias
v_lshlrev_b32 v8, 0x2, v[vgprSerial]               // Local bias address scaled by BPE
s_waitcnt vmcnt(0)                                 // wait for bias load
s_barrier                                          // Wait for all wavefronts
ds_write_b32 v8, v4 offset:0                       // store bias
s_branch label_Load_Bias_End                       // Branch to load bias end
label_Load_Biasf16:
s_cmpk_lg_u32 s[sgprBiasType], 4                   // BiasType != 4
s_cbranch_scc1 label_Load_Bias_End                 // Branch if true

/******************************************/
/* Read Bias to LDS                       */
/******************************************/
s_mul_i32 s[sgprSrdBias+2], 0x2, s[sgprSrdBias+2]  // scaled by BPE
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_add_u32 v8, s66, v[vgprSerial]                   // coord 0 = wgp0 * MT0 + thread offset
s_mul_i32 s66, s[sgprBiasStride], s[sgprWorkGroup2] // Stride * WG
v_add_u32 v8, s66, v8                              // coord 0 = wgp0 * MT0 + thread offset + Stride * WG
v_lshlrev_b32 v8, 0x1, v8                          // Global bias address scaled by BPE
buffer_load_short_d16 v4, v8, s[sgprSrdBias:sgprSrdBias+3], 0 offen offset:0 // load bias
v_lshlrev_b32 v8, 0x2, v[vgprSerial]               // Local bias address scaled by BPE
s_waitcnt vmcnt(0)                                 // wait for bias load
s_barrier                                          // Wait for all wavefronts
v_cvt_f32_f16 v4, v4                               // convert to FP32
ds_write_b32 v8, v4 offset:0                       // store bias
s_branch label_Load_Bias_End                       // Branch to load bias end
label_Load_Bias_End:
v_mov_b32 v4, s[sgprAlpha]
s_waitcnt lgkmcnt(0)                               // wait for scaleAB load
v_mul_f32 v4, v4, s51
v_mul_f32 v4, v4, s65
s_nop 0                                            // 1 wait states
v_readfirstlane_b32 s[sgprAlpha], v4               // Update Alpha
s_cmpk_eq_u32 s[sgprBeta], 0x0                     // Beta == 0
s_cbranch_scc1 label_NoBranch_6AB7M3RYAFDUQ6IV_0   // Only branch on scc0
s_getpc_b64 s[84:85]                               // addr of next instr
s_add_i32 s86, label_GW_Beta, 0x4                  // target branch offset
s_add_u32 s84, s84, s86                            // add target branch offset
s_addc_u32 s85, s85, 0                             // add high and carry
s_setpc_b64 s[84:85]                               // branch to label_GW_Beta
label_NoBranch_6AB7M3RYAFDUQ6IV_0:

s_and_b32 s84, 255, s[sgprSizeI]                   // s84 = s[sgprSizeI] % 256
s_add_u32 s85, -0x1, s[sgprNumWorkGroups0]
s_cmp_ge_u32 s[sgprWorkGroup0], s85                // wg0 >= nwg0-1 ?
s_cselect_b32 s84, s84, 0                          // set rMT0
s_cmpk_gt_u32 s84, 0x0                             // rMT0 > 0
s_cbranch_scc0 label_NoBranch_SCWBHSSTZZMPWU0B_0   // Only branch on scc1
// jump if edges required
s_getpc_b64 s[84:85]                               // addr of next instr
s_add_i32 s86, label_GW_B0_E1, 0x4                 // target branch offset
s_add_u32 s84, s84, s86                            // add target branch offset
s_addc_u32 s85, s85, 0                             // add high and carry
s_setpc_b64 s[84:85]                               // branch to label_GW_B0_E1
label_NoBranch_SCWBHSSTZZMPWU0B_0:
s_and_b32 s84, 15, s[sgprSizeJ]                    // s84 = s[sgprSizeJ] % 16
s_add_u32 s85, -0x1, s[sgprNumWorkGroups1]
s_cmp_ge_u32 s[sgprWorkGroup1], s85                // wg1 >= nwg1-1
s_cselect_b32 s84, s84, 0                          // set rMT1
s_cmpk_gt_u32 s84, 0x0                             // rMT1 > 0
s_cbranch_scc0 label_NoBranch_JYWMZIIYSLPDXYEH_0   // Only branch on scc1
// jump if edges required
s_getpc_b64 s[84:85]                               // addr of next instr
s_add_i32 s86, label_GW_B0_E1, 0x4                 // target branch offset
s_add_u32 s84, s84, s86                            // add target branch offset
s_addc_u32 s85, s85, 0                             // add high and carry
s_setpc_b64 s[84:85]                               // branch to label_GW_B0_E1
label_NoBranch_JYWMZIIYSLPDXYEH_0:
label_GW_B0_E0:

/* edge=0, allocate 2 sgpr. perBatchTmpS=2 perBatchMaskS=0 perElementMaskS=0 elementsPerBatch=18 */
s_cmpk_eq_u32 s[sgprActivationType], 0             // activationType == 0
s_cbranch_scc1 label_Activation_None               // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 1             // activationType == 1
s_cbranch_scc1 label_Activation_Abs                // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 2             // activationType == 2
s_cbranch_scc1 label_Activation_Clippedrelu        // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 3             // activationType == 3
s_cbranch_scc1 label_Activation_Gelu               // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 4             // activationType == 4
s_cbranch_scc1 label_Activation_Leakyrelu          // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 5             // activationType == 5
s_cbranch_scc1 label_Activation_Relu               // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 6             // activationType == 6
s_cbranch_scc1 label_Activation_Sigmoid            // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 7             // activationType == 7
s_cbranch_scc1 label_Activation_Tanh               // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 9             // activationType == 9
s_cbranch_scc1 label_Activation_Geluscaling        // Branch if true
label_Activation_None:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v9, v0, s66
v_lshlrev_b32 v9, 0x2, v9                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v9 offset:0                 // load bias
v_lshlrev_b32 v10, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v10, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_lshl_u32 v6, v3, v0, 0x2                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_add_lshl_u32 v7, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+24], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+25], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+26], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+27], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+28], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+29], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+30], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+31], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+32], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+33], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+34], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+35], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[20:23], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[28:31], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[32:35], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s84, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s84, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s84, s84, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s84, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s91, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s90, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s94, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s94, s94, 1                              // Free1
s_mul_hi_u32 s93, s94, s[sgprStrideC1J]            // Free1
s_mul_i32 s92, s94, s[sgprStrideC1J]               // Free1
s_add_u32 s90, s90, s92                            // Free1
s_addc_u32 s91, s91, s93                           // Free1
s_sub_u32 s94, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s94, s94, 1                              // Free2
s_mul_hi_u32 s93, s94, s[sgprStrideCK]             // Free2
s_mul_i32 s92, s94, s[sgprStrideCK]                // Free2
s_add_u32 s90, s90, s92                            // Free2
s_addc_u32 s91, s91, s93                           // Free2
s_lshl_b64 s[86:87], s[90:91], 2                   // scale by bpe

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_0
// check done end

// buffer load start
buffer_load_dwordx4 v[20:23], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_108 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_108 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_108 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_108 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_108 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_108 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_108 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_108:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_108 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_108 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_108 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_108 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_108 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_108 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[192:193]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_108 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_108     // Syncbranchhere

label_Synchronizer_read_add_end_7_108:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_108      // SyncAddbranch

label_Synchronizer_read_add_end_6_108:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_108      // SyncAddbranch

label_Synchronizer_read_add_end_5_108:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_108      // SyncAddbranch

label_Synchronizer_read_add_end_4_108:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_108      // SyncAddbranch

label_Synchronizer_read_add_end_3_108:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_108      // SyncAddbranch

label_Synchronizer_read_add_end_2_108:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_108      // SyncAddbranch

label_Synchronizer_read_add_end_1_108:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_108:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[24:27], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_109 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_109 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_109 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_109 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_109 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_109 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_109 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_109:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_109 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_109 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_109 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_109 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_109 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_109 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_109 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_109     // Syncbranchhere

label_Synchronizer_read_add_end_7_109:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_109      // SyncAddbranch

label_Synchronizer_read_add_end_6_109:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_109      // SyncAddbranch

label_Synchronizer_read_add_end_5_109:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_109      // SyncAddbranch

label_Synchronizer_read_add_end_4_109:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_109      // SyncAddbranch

label_Synchronizer_read_add_end_3_109:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_109      // SyncAddbranch

label_Synchronizer_read_add_end_2_109:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_109      // SyncAddbranch

label_Synchronizer_read_add_end_1_109:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_109:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[28:31], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_110 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_110 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_110 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_110 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_110 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_110 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_110 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_110:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_110 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_110 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_110 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_110 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_110 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_110 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[192:193]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_110 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_110     // Syncbranchhere

label_Synchronizer_read_add_end_7_110:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_110      // SyncAddbranch

label_Synchronizer_read_add_end_6_110:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_110      // SyncAddbranch

label_Synchronizer_read_add_end_5_110:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_110      // SyncAddbranch

label_Synchronizer_read_add_end_4_110:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_110      // SyncAddbranch

label_Synchronizer_read_add_end_3_110:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_110      // SyncAddbranch

label_Synchronizer_read_add_end_2_110:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_110      // SyncAddbranch

label_Synchronizer_read_add_end_1_110:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_110:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[32:35], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_111 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_111 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_111 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_111 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_111 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_111 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_111 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_111:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_111 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_111 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_111 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_111 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_111 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_111 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[192:193]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_111 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_111     // Syncbranchhere

label_Synchronizer_read_add_end_7_111:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_111      // SyncAddbranch

label_Synchronizer_read_add_end_6_111:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_111      // SyncAddbranch

label_Synchronizer_read_add_end_5_111:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_111      // SyncAddbranch

label_Synchronizer_read_add_end_4_111:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_111      // SyncAddbranch

label_Synchronizer_read_add_end_3_111:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_111      // SyncAddbranch

label_Synchronizer_read_add_end_2_111:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_111      // SyncAddbranch

label_Synchronizer_read_add_end_1_111:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_111:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha

/* apply mask, calc new C and issue writes */

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[12:13], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[14:15], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[12:13], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[14:15], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_0:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End
label_Activation_Abs:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v9, v0, s66
v_lshlrev_b32 v9, 0x2, v9                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v9 offset:0                 // load bias
v_lshlrev_b32 v10, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v10, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_lshl_u32 v6, v3, v0, 0x2                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_add_lshl_u32 v7, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+24], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+25], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+26], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+27], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+28], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+29], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+30], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+31], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+32], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+33], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+34], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+35], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[20:23], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[28:31], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[32:35], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s84, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s84, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s84, s84, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s84, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s91, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s90, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s94, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s94, s94, 1                              // Free1
s_mul_hi_u32 s93, s94, s[sgprStrideC1J]            // Free1
s_mul_i32 s92, s94, s[sgprStrideC1J]               // Free1
s_add_u32 s90, s90, s92                            // Free1
s_addc_u32 s91, s91, s93                           // Free1
s_sub_u32 s94, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s94, s94, 1                              // Free2
s_mul_hi_u32 s93, s94, s[sgprStrideCK]             // Free2
s_mul_i32 s92, s94, s[sgprStrideCK]                // Free2
s_add_u32 s90, s90, s92                            // Free2
s_addc_u32 s91, s91, s93                           // Free2
s_lshl_b64 s[86:87], s[90:91], 2                   // scale by bpe

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_1
// check done end

// buffer load start
buffer_load_dwordx4 v[20:23], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_112 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_112 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_112 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_112 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_112 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_112 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_112 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_112:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_112 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_112 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_112 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_112 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_112 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_112 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[192:193]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_112 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_112     // Syncbranchhere

label_Synchronizer_read_add_end_7_112:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_112      // SyncAddbranch

label_Synchronizer_read_add_end_6_112:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_112      // SyncAddbranch

label_Synchronizer_read_add_end_5_112:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_112      // SyncAddbranch

label_Synchronizer_read_add_end_4_112:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_112      // SyncAddbranch

label_Synchronizer_read_add_end_3_112:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_112      // SyncAddbranch

label_Synchronizer_read_add_end_2_112:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_112      // SyncAddbranch

label_Synchronizer_read_add_end_1_112:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_112:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[24:27], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_113 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_113 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_113 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_113 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_113 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_113 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_113 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_113:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_113 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_113 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_113 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_113 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_113 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_113 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_113 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_113     // Syncbranchhere

label_Synchronizer_read_add_end_7_113:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_113      // SyncAddbranch

label_Synchronizer_read_add_end_6_113:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_113      // SyncAddbranch

label_Synchronizer_read_add_end_5_113:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_113      // SyncAddbranch

label_Synchronizer_read_add_end_4_113:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_113      // SyncAddbranch

label_Synchronizer_read_add_end_3_113:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_113      // SyncAddbranch

label_Synchronizer_read_add_end_2_113:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_113      // SyncAddbranch

label_Synchronizer_read_add_end_1_113:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_113:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[28:31], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_114 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_114 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_114 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_114 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_114 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_114 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_114 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_114:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_114 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_114 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_114 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_114 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_114 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_114 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[192:193]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_114 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_114     // Syncbranchhere

label_Synchronizer_read_add_end_7_114:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_114      // SyncAddbranch

label_Synchronizer_read_add_end_6_114:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_114      // SyncAddbranch

label_Synchronizer_read_add_end_5_114:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_114      // SyncAddbranch

label_Synchronizer_read_add_end_4_114:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_114      // SyncAddbranch

label_Synchronizer_read_add_end_3_114:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_114      // SyncAddbranch

label_Synchronizer_read_add_end_2_114:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_114      // SyncAddbranch

label_Synchronizer_read_add_end_1_114:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_114:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[32:35], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_115 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_115 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_115 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_115 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_115 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_115 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_115 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_115:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_115 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_115 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_115 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_115 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_115 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_115 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[192:193]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_115 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_115     // Syncbranchhere

label_Synchronizer_read_add_end_7_115:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_115      // SyncAddbranch

label_Synchronizer_read_add_end_6_115:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_115      // SyncAddbranch

label_Synchronizer_read_add_end_5_115:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_115      // SyncAddbranch

label_Synchronizer_read_add_end_4_115:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_115      // SyncAddbranch

label_Synchronizer_read_add_end_3_115:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_115      // SyncAddbranch

label_Synchronizer_read_add_end_2_115:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_115      // SyncAddbranch

label_Synchronizer_read_add_end_1_115:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_115:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha

/* apply mask, calc new C and issue writes */

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_and_b32 v[vgprValuC+20], 0x7fffffff, v[vgprValuC+20] // Remove sign bit
v_and_b32 v[vgprValuC+21], 0x7fffffff, v[vgprValuC+21] // Remove sign bit
v_and_b32 v[vgprValuC+22], 0x7fffffff, v[vgprValuC+22] // Remove sign bit
v_and_b32 v[vgprValuC+23], 0x7fffffff, v[vgprValuC+23] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[12:13], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[14:15], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_and_b32 v[vgprValuC+24], 0x7fffffff, v[vgprValuC+24] // Remove sign bit
v_and_b32 v[vgprValuC+25], 0x7fffffff, v[vgprValuC+25] // Remove sign bit
v_and_b32 v[vgprValuC+26], 0x7fffffff, v[vgprValuC+26] // Remove sign bit
v_and_b32 v[vgprValuC+27], 0x7fffffff, v[vgprValuC+27] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_and_b32 v[vgprValuC+28], 0x7fffffff, v[vgprValuC+28] // Remove sign bit
v_and_b32 v[vgprValuC+29], 0x7fffffff, v[vgprValuC+29] // Remove sign bit
v_and_b32 v[vgprValuC+30], 0x7fffffff, v[vgprValuC+30] // Remove sign bit
v_and_b32 v[vgprValuC+31], 0x7fffffff, v[vgprValuC+31] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[12:13], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[14:15], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_and_b32 v[vgprValuC+32], 0x7fffffff, v[vgprValuC+32] // Remove sign bit
v_and_b32 v[vgprValuC+33], 0x7fffffff, v[vgprValuC+33] // Remove sign bit
v_and_b32 v[vgprValuC+34], 0x7fffffff, v[vgprValuC+34] // Remove sign bit
v_and_b32 v[vgprValuC+35], 0x7fffffff, v[vgprValuC+35] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_1:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End
label_Activation_Clippedrelu:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v9, v0, s66
v_lshlrev_b32 v9, 0x2, v9                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v9 offset:0                 // load bias
v_lshlrev_b32 v10, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v10, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_lshl_u32 v6, v3, v0, 0x2                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_add_lshl_u32 v7, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+24], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+25], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+26], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+27], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+28], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+29], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+30], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+31], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+32], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+33], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+34], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+35], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[20:23], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[28:31], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[32:35], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s84, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s84, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s84, s84, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s84, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s91, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s90, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s94, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s94, s94, 1                              // Free1
s_mul_hi_u32 s93, s94, s[sgprStrideC1J]            // Free1
s_mul_i32 s92, s94, s[sgprStrideC1J]               // Free1
s_add_u32 s90, s90, s92                            // Free1
s_addc_u32 s91, s91, s93                           // Free1
s_sub_u32 s94, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s94, s94, 1                              // Free2
s_mul_hi_u32 s93, s94, s[sgprStrideCK]             // Free2
s_mul_i32 s92, s94, s[sgprStrideCK]                // Free2
s_add_u32 s90, s90, s92                            // Free2
s_addc_u32 s91, s91, s93                           // Free2
s_lshl_b64 s[86:87], s[90:91], 2                   // scale by bpe

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_2
// check done end

// buffer load start
buffer_load_dwordx4 v[20:23], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_116 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_116 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_116 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_116 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_116 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_116 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_116 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_116:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_116 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_116 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_116 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_116 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_116 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_116 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[192:193]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_116 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_116     // Syncbranchhere

label_Synchronizer_read_add_end_7_116:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_116      // SyncAddbranch

label_Synchronizer_read_add_end_6_116:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_116      // SyncAddbranch

label_Synchronizer_read_add_end_5_116:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_116      // SyncAddbranch

label_Synchronizer_read_add_end_4_116:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_116      // SyncAddbranch

label_Synchronizer_read_add_end_3_116:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_116      // SyncAddbranch

label_Synchronizer_read_add_end_2_116:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_116      // SyncAddbranch

label_Synchronizer_read_add_end_1_116:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_116:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[24:27], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_117 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_117 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_117 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_117 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_117 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_117 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_117 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_117:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_117 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_117 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_117 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_117 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_117 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_117 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_117 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_117     // Syncbranchhere

label_Synchronizer_read_add_end_7_117:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_117      // SyncAddbranch

label_Synchronizer_read_add_end_6_117:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_117      // SyncAddbranch

label_Synchronizer_read_add_end_5_117:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_117      // SyncAddbranch

label_Synchronizer_read_add_end_4_117:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_117      // SyncAddbranch

label_Synchronizer_read_add_end_3_117:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_117      // SyncAddbranch

label_Synchronizer_read_add_end_2_117:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_117      // SyncAddbranch

label_Synchronizer_read_add_end_1_117:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_117:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[28:31], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_118 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_118 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_118 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_118 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_118 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_118 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_118 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_118:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_118 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_118 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_118 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_118 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_118 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_118 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[192:193]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_118 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_118     // Syncbranchhere

label_Synchronizer_read_add_end_7_118:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_118      // SyncAddbranch

label_Synchronizer_read_add_end_6_118:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_118      // SyncAddbranch

label_Synchronizer_read_add_end_5_118:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_118      // SyncAddbranch

label_Synchronizer_read_add_end_4_118:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_118      // SyncAddbranch

label_Synchronizer_read_add_end_3_118:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_118      // SyncAddbranch

label_Synchronizer_read_add_end_2_118:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_118      // SyncAddbranch

label_Synchronizer_read_add_end_1_118:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_118:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[32:35], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_119 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_119 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_119 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_119 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_119 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_119 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_119 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_119:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_119 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_119 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_119 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_119 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_119 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_119 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[192:193]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_119 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_119     // Syncbranchhere

label_Synchronizer_read_add_end_7_119:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_119      // SyncAddbranch

label_Synchronizer_read_add_end_6_119:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_119      // SyncAddbranch

label_Synchronizer_read_add_end_5_119:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_119      // SyncAddbranch

label_Synchronizer_read_add_end_4_119:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_119      // SyncAddbranch

label_Synchronizer_read_add_end_3_119:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_119      // SyncAddbranch

label_Synchronizer_read_add_end_2_119:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_119      // SyncAddbranch

label_Synchronizer_read_add_end_1_119:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_119:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha

/* apply mask, calc new C and issue writes */

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+20], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+20], s[sgpractivationBeta], v[vgprValuC+20] // min(x, beta)
v_cndmask_b32 v[vgprValuC+20], 0.0, v[vgprValuC+20], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+21], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+21], s[sgpractivationBeta], v[vgprValuC+21] // min(x, beta)
v_cndmask_b32 v[vgprValuC+21], 0.0, v[vgprValuC+21], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+22], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+22], s[sgpractivationBeta], v[vgprValuC+22] // min(x, beta)
v_cndmask_b32 v[vgprValuC+22], 0.0, v[vgprValuC+22], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+23], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+23], s[sgpractivationBeta], v[vgprValuC+23] // min(x, beta)
v_cndmask_b32 v[vgprValuC+23], 0.0, v[vgprValuC+23], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[12:13], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[14:15], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+24], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+24], s[sgpractivationBeta], v[vgprValuC+24] // min(x, beta)
v_cndmask_b32 v[vgprValuC+24], 0.0, v[vgprValuC+24], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+25], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+25], s[sgpractivationBeta], v[vgprValuC+25] // min(x, beta)
v_cndmask_b32 v[vgprValuC+25], 0.0, v[vgprValuC+25], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+26], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+26], s[sgpractivationBeta], v[vgprValuC+26] // min(x, beta)
v_cndmask_b32 v[vgprValuC+26], 0.0, v[vgprValuC+26], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+27], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+27], s[sgpractivationBeta], v[vgprValuC+27] // min(x, beta)
v_cndmask_b32 v[vgprValuC+27], 0.0, v[vgprValuC+27], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+28], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+28], s[sgpractivationBeta], v[vgprValuC+28] // min(x, beta)
v_cndmask_b32 v[vgprValuC+28], 0.0, v[vgprValuC+28], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+29], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+29], s[sgpractivationBeta], v[vgprValuC+29] // min(x, beta)
v_cndmask_b32 v[vgprValuC+29], 0.0, v[vgprValuC+29], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+30], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+30], s[sgpractivationBeta], v[vgprValuC+30] // min(x, beta)
v_cndmask_b32 v[vgprValuC+30], 0.0, v[vgprValuC+30], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+31], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+31], s[sgpractivationBeta], v[vgprValuC+31] // min(x, beta)
v_cndmask_b32 v[vgprValuC+31], 0.0, v[vgprValuC+31], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[12:13], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[14:15], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+32], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+32], s[sgpractivationBeta], v[vgprValuC+32] // min(x, beta)
v_cndmask_b32 v[vgprValuC+32], 0.0, v[vgprValuC+32], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+33], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+33], s[sgpractivationBeta], v[vgprValuC+33] // min(x, beta)
v_cndmask_b32 v[vgprValuC+33], 0.0, v[vgprValuC+33], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+34], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+34], s[sgpractivationBeta], v[vgprValuC+34] // min(x, beta)
v_cndmask_b32 v[vgprValuC+34], 0.0, v[vgprValuC+34], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+35], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+35], s[sgpractivationBeta], v[vgprValuC+35] // min(x, beta)
v_cndmask_b32 v[vgprValuC+35], 0.0, v[vgprValuC+35], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_2:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End
label_Activation_Gelu:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v9, v0, s66
v_lshlrev_b32 v9, 0x2, v9                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v9 offset:0                 // load bias
v_lshlrev_b32 v10, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v10, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_lshl_u32 v6, v3, v0, 0x2                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_add_lshl_u32 v7, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+24], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+25], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+26], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+27], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+28], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+29], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+30], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+31], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+32], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+33], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+34], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+35], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[20:23], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[28:31], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[32:35], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s84, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s84, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s84, s84, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s84, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s91, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s90, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s94, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s94, s94, 1                              // Free1
s_mul_hi_u32 s93, s94, s[sgprStrideC1J]            // Free1
s_mul_i32 s92, s94, s[sgprStrideC1J]               // Free1
s_add_u32 s90, s90, s92                            // Free1
s_addc_u32 s91, s91, s93                           // Free1
s_sub_u32 s94, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s94, s94, 1                              // Free2
s_mul_hi_u32 s93, s94, s[sgprStrideCK]             // Free2
s_mul_i32 s92, s94, s[sgprStrideCK]                // Free2
s_add_u32 s90, s90, s92                            // Free2
s_addc_u32 s91, s91, s93                           // Free2
s_lshl_b64 s[86:87], s[90:91], 2                   // scale by bpe

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_3
// check done end

// buffer load start
buffer_load_dwordx4 v[20:23], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_120 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_120 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_120 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_120 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_120 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_120 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_120 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_120:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_120 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_120 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_120 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_120 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_120 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_120 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[192:193]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_120 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_120     // Syncbranchhere

label_Synchronizer_read_add_end_7_120:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_120      // SyncAddbranch

label_Synchronizer_read_add_end_6_120:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_120      // SyncAddbranch

label_Synchronizer_read_add_end_5_120:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_120      // SyncAddbranch

label_Synchronizer_read_add_end_4_120:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_120      // SyncAddbranch

label_Synchronizer_read_add_end_3_120:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_120      // SyncAddbranch

label_Synchronizer_read_add_end_2_120:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_120      // SyncAddbranch

label_Synchronizer_read_add_end_1_120:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_120:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[24:27], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_121 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_121 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_121 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_121 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_121 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_121 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_121 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_121:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_121 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_121 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_121 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_121 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_121 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_121 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_121 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_121     // Syncbranchhere

label_Synchronizer_read_add_end_7_121:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_121      // SyncAddbranch

label_Synchronizer_read_add_end_6_121:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_121      // SyncAddbranch

label_Synchronizer_read_add_end_5_121:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_121      // SyncAddbranch

label_Synchronizer_read_add_end_4_121:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_121      // SyncAddbranch

label_Synchronizer_read_add_end_3_121:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_121      // SyncAddbranch

label_Synchronizer_read_add_end_2_121:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_121      // SyncAddbranch

label_Synchronizer_read_add_end_1_121:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_121:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[28:31], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_122 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_122 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_122 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_122 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_122 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_122 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_122 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_122:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_122 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_122 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_122 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_122 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_122 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_122 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[192:193]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_122 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_122     // Syncbranchhere

label_Synchronizer_read_add_end_7_122:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_122      // SyncAddbranch

label_Synchronizer_read_add_end_6_122:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_122      // SyncAddbranch

label_Synchronizer_read_add_end_5_122:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_122      // SyncAddbranch

label_Synchronizer_read_add_end_4_122:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_122      // SyncAddbranch

label_Synchronizer_read_add_end_3_122:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_122      // SyncAddbranch

label_Synchronizer_read_add_end_2_122:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_122      // SyncAddbranch

label_Synchronizer_read_add_end_1_122:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_122:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[32:35], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_123 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_123 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_123 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_123 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_123 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_123 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_123 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_123:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_123 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_123 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_123 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_123 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_123 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_123 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[192:193]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_123 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_123     // Syncbranchhere

label_Synchronizer_read_add_end_7_123:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_123      // SyncAddbranch

label_Synchronizer_read_add_end_6_123:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_123      // SyncAddbranch

label_Synchronizer_read_add_end_5_123:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_123      // SyncAddbranch

label_Synchronizer_read_add_end_4_123:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_123      // SyncAddbranch

label_Synchronizer_read_add_end_3_123:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_123      // SyncAddbranch

label_Synchronizer_read_add_end_2_123:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_123      // SyncAddbranch

label_Synchronizer_read_add_end_1_123:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_123:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha

/* apply mask, calc new C and issue writes */

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+20]          // k1 * x
v_fma_f32 v4, v[vgprValuC+20], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+20], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+20], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+20], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+21]          // k1 * x
v_fma_f32 v4, v[vgprValuC+21], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+21], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+21], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+21], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+22]          // k1 * x
v_fma_f32 v4, v[vgprValuC+22], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+22], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+22], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+22], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+23]          // k1 * x
v_fma_f32 v4, v[vgprValuC+23], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+23], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+23], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+23], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[12:13], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[14:15], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+24]          // k1 * x
v_fma_f32 v4, v[vgprValuC+24], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+24], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+24], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+24], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+25]          // k1 * x
v_fma_f32 v4, v[vgprValuC+25], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+25], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+25], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+25], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+26]          // k1 * x
v_fma_f32 v4, v[vgprValuC+26], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+26], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+26], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+26], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+27]          // k1 * x
v_fma_f32 v4, v[vgprValuC+27], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+27], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+27], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+27], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+28]          // k1 * x
v_fma_f32 v4, v[vgprValuC+28], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+28], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+28], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+28], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+29]          // k1 * x
v_fma_f32 v4, v[vgprValuC+29], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+29], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+29], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+29], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+30]          // k1 * x
v_fma_f32 v4, v[vgprValuC+30], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+30], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+30], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+30], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+31]          // k1 * x
v_fma_f32 v4, v[vgprValuC+31], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+31], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+31], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+31], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[12:13], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[14:15], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+32]          // k1 * x
v_fma_f32 v4, v[vgprValuC+32], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+32], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+32], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+32], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+33]          // k1 * x
v_fma_f32 v4, v[vgprValuC+33], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+33], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+33], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+33], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+34]          // k1 * x
v_fma_f32 v4, v[vgprValuC+34], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+34], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+34], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+34], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+35]          // k1 * x
v_fma_f32 v4, v[vgprValuC+35], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+35], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+35], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+35], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_3:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End
label_Activation_Leakyrelu:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v9, v0, s66
v_lshlrev_b32 v9, 0x2, v9                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v9 offset:0                 // load bias
v_lshlrev_b32 v10, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v10, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_lshl_u32 v6, v3, v0, 0x2                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_add_lshl_u32 v7, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+24], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+25], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+26], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+27], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+28], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+29], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+30], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+31], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+32], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+33], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+34], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+35], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[20:23], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[28:31], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[32:35], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s84, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s84, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s84, s84, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s84, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s91, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s90, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s94, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s94, s94, 1                              // Free1
s_mul_hi_u32 s93, s94, s[sgprStrideC1J]            // Free1
s_mul_i32 s92, s94, s[sgprStrideC1J]               // Free1
s_add_u32 s90, s90, s92                            // Free1
s_addc_u32 s91, s91, s93                           // Free1
s_sub_u32 s94, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s94, s94, 1                              // Free2
s_mul_hi_u32 s93, s94, s[sgprStrideCK]             // Free2
s_mul_i32 s92, s94, s[sgprStrideCK]                // Free2
s_add_u32 s90, s90, s92                            // Free2
s_addc_u32 s91, s91, s93                           // Free2
s_lshl_b64 s[86:87], s[90:91], 2                   // scale by bpe

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_4
// check done end

// buffer load start
buffer_load_dwordx4 v[20:23], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_124 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_124 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_124 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_124 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_124 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_124 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_124 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_124:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_124 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_124 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_124 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_124 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_124 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_124 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[192:193]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_124 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_124     // Syncbranchhere

label_Synchronizer_read_add_end_7_124:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_124      // SyncAddbranch

label_Synchronizer_read_add_end_6_124:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_124      // SyncAddbranch

label_Synchronizer_read_add_end_5_124:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_124      // SyncAddbranch

label_Synchronizer_read_add_end_4_124:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_124      // SyncAddbranch

label_Synchronizer_read_add_end_3_124:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_124      // SyncAddbranch

label_Synchronizer_read_add_end_2_124:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_124      // SyncAddbranch

label_Synchronizer_read_add_end_1_124:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_124:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[24:27], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_125 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_125 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_125 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_125 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_125 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_125 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_125 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_125:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_125 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_125 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_125 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_125 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_125 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_125 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_125 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_125     // Syncbranchhere

label_Synchronizer_read_add_end_7_125:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_125      // SyncAddbranch

label_Synchronizer_read_add_end_6_125:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_125      // SyncAddbranch

label_Synchronizer_read_add_end_5_125:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_125      // SyncAddbranch

label_Synchronizer_read_add_end_4_125:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_125      // SyncAddbranch

label_Synchronizer_read_add_end_3_125:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_125      // SyncAddbranch

label_Synchronizer_read_add_end_2_125:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_125      // SyncAddbranch

label_Synchronizer_read_add_end_1_125:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_125:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[28:31], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_126 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_126 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_126 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_126 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_126 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_126 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_126 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_126:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_126 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_126 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_126 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_126 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_126 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_126 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[192:193]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_126 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_126     // Syncbranchhere

label_Synchronizer_read_add_end_7_126:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_126      // SyncAddbranch

label_Synchronizer_read_add_end_6_126:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_126      // SyncAddbranch

label_Synchronizer_read_add_end_5_126:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_126      // SyncAddbranch

label_Synchronizer_read_add_end_4_126:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_126      // SyncAddbranch

label_Synchronizer_read_add_end_3_126:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_126      // SyncAddbranch

label_Synchronizer_read_add_end_2_126:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_126      // SyncAddbranch

label_Synchronizer_read_add_end_1_126:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_126:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[32:35], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_127 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_127 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_127 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_127 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_127 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_127 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_127 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_127:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_127 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_127 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_127 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_127 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_127 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_127 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[192:193]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_127 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_127     // Syncbranchhere

label_Synchronizer_read_add_end_7_127:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_127      // SyncAddbranch

label_Synchronizer_read_add_end_6_127:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_127      // SyncAddbranch

label_Synchronizer_read_add_end_5_127:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_127      // SyncAddbranch

label_Synchronizer_read_add_end_4_127:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_127      // SyncAddbranch

label_Synchronizer_read_add_end_3_127:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_127      // SyncAddbranch

label_Synchronizer_read_add_end_2_127:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_127      // SyncAddbranch

label_Synchronizer_read_add_end_1_127:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_127:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha

/* apply mask, calc new C and issue writes */

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+20] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+20], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+20], v4, v[vgprValuC+20], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+21] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+21], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+21], v4, v[vgprValuC+21], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+22] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+22], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+22], v4, v[vgprValuC+22], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+23] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+23], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+23], v4, v[vgprValuC+23], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[12:13], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[14:15], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+24] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+24], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+24], v4, v[vgprValuC+24], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+25] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+25], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+25], v4, v[vgprValuC+25], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+26] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+26], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+26], v4, v[vgprValuC+26], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+27] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+27], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+27], v4, v[vgprValuC+27], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+28] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+28], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+28], v4, v[vgprValuC+28], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+29] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+29], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+29], v4, v[vgprValuC+29], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+30] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+30], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+30], v4, v[vgprValuC+30], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+31] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+31], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+31], v4, v[vgprValuC+31], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[12:13], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[14:15], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+32] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+32], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+32], v4, v[vgprValuC+32], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+33] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+33], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+33], v4, v[vgprValuC+33], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+34] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+34], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+34], v4, v[vgprValuC+34], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+35] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+35], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+35], v4, v[vgprValuC+35], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_4:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End
label_Activation_Relu:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v9, v0, s66
v_lshlrev_b32 v9, 0x2, v9                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v9 offset:0                 // load bias
v_lshlrev_b32 v10, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v10, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_lshl_u32 v6, v3, v0, 0x2                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_add_lshl_u32 v7, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+24], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+25], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+26], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+27], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+28], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+29], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+30], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+31], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+32], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+33], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+34], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+35], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[20:23], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[28:31], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[32:35], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s84, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s84, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s84, s84, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s84, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s91, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s90, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s94, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s94, s94, 1                              // Free1
s_mul_hi_u32 s93, s94, s[sgprStrideC1J]            // Free1
s_mul_i32 s92, s94, s[sgprStrideC1J]               // Free1
s_add_u32 s90, s90, s92                            // Free1
s_addc_u32 s91, s91, s93                           // Free1
s_sub_u32 s94, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s94, s94, 1                              // Free2
s_mul_hi_u32 s93, s94, s[sgprStrideCK]             // Free2
s_mul_i32 s92, s94, s[sgprStrideCK]                // Free2
s_add_u32 s90, s90, s92                            // Free2
s_addc_u32 s91, s91, s93                           // Free2
s_lshl_b64 s[86:87], s[90:91], 2                   // scale by bpe

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_5
// check done end

// buffer load start
buffer_load_dwordx4 v[20:23], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_128 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_128 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_128 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_128 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_128 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_128 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_128 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_128:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_128 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_128 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_128 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_128 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_128 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_128 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[192:193]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_128 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_128     // Syncbranchhere

label_Synchronizer_read_add_end_7_128:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_128      // SyncAddbranch

label_Synchronizer_read_add_end_6_128:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_128      // SyncAddbranch

label_Synchronizer_read_add_end_5_128:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_128      // SyncAddbranch

label_Synchronizer_read_add_end_4_128:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_128      // SyncAddbranch

label_Synchronizer_read_add_end_3_128:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_128      // SyncAddbranch

label_Synchronizer_read_add_end_2_128:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_128      // SyncAddbranch

label_Synchronizer_read_add_end_1_128:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_128:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[24:27], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_129 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_129 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_129 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_129 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_129 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_129 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_129 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_129:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_129 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_129 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_129 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_129 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_129 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_129 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_129 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_129     // Syncbranchhere

label_Synchronizer_read_add_end_7_129:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_129      // SyncAddbranch

label_Synchronizer_read_add_end_6_129:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_129      // SyncAddbranch

label_Synchronizer_read_add_end_5_129:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_129      // SyncAddbranch

label_Synchronizer_read_add_end_4_129:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_129      // SyncAddbranch

label_Synchronizer_read_add_end_3_129:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_129      // SyncAddbranch

label_Synchronizer_read_add_end_2_129:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_129      // SyncAddbranch

label_Synchronizer_read_add_end_1_129:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_129:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[28:31], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_130 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_130 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_130 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_130 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_130 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_130 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_130 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_130:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_130 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_130 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_130 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_130 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_130 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_130 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[192:193]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_130 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_130     // Syncbranchhere

label_Synchronizer_read_add_end_7_130:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_130      // SyncAddbranch

label_Synchronizer_read_add_end_6_130:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_130      // SyncAddbranch

label_Synchronizer_read_add_end_5_130:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_130      // SyncAddbranch

label_Synchronizer_read_add_end_4_130:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_130      // SyncAddbranch

label_Synchronizer_read_add_end_3_130:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_130      // SyncAddbranch

label_Synchronizer_read_add_end_2_130:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_130      // SyncAddbranch

label_Synchronizer_read_add_end_1_130:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_130:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[32:35], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_131 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_131 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_131 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_131 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_131 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_131 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_131 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_131:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_131 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_131 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_131 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_131 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_131 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_131 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[192:193]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_131 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_131     // Syncbranchhere

label_Synchronizer_read_add_end_7_131:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_131      // SyncAddbranch

label_Synchronizer_read_add_end_6_131:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_131      // SyncAddbranch

label_Synchronizer_read_add_end_5_131:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_131      // SyncAddbranch

label_Synchronizer_read_add_end_4_131:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_131      // SyncAddbranch

label_Synchronizer_read_add_end_3_131:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_131      // SyncAddbranch

label_Synchronizer_read_add_end_2_131:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_131      // SyncAddbranch

label_Synchronizer_read_add_end_1_131:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_131:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha

/* apply mask, calc new C and issue writes */

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_max_f32 v[vgprValuC+20], v[vgprValuC+20], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+21], v[vgprValuC+21], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+22], v[vgprValuC+22], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+23], v[vgprValuC+23], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[12:13], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[14:15], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_max_f32 v[vgprValuC+24], v[vgprValuC+24], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+25], v[vgprValuC+25], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+26], v[vgprValuC+26], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+27], v[vgprValuC+27], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_max_f32 v[vgprValuC+28], v[vgprValuC+28], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+29], v[vgprValuC+29], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+30], v[vgprValuC+30], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+31], v[vgprValuC+31], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[12:13], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[14:15], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_max_f32 v[vgprValuC+32], v[vgprValuC+32], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+33], v[vgprValuC+33], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+34], v[vgprValuC+34], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+35], v[vgprValuC+35], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_5:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End
label_Activation_Sigmoid:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v9, v0, s66
v_lshlrev_b32 v9, 0x2, v9                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v9 offset:0                 // load bias
v_lshlrev_b32 v10, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v10, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_lshl_u32 v6, v3, v0, 0x2                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_add_lshl_u32 v7, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+24], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+25], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+26], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+27], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+28], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+29], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+30], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+31], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+32], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+33], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+34], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+35], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[20:23], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[28:31], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[32:35], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s84, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s84, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s84, s84, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s84, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s91, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s90, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s94, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s94, s94, 1                              // Free1
s_mul_hi_u32 s93, s94, s[sgprStrideC1J]            // Free1
s_mul_i32 s92, s94, s[sgprStrideC1J]               // Free1
s_add_u32 s90, s90, s92                            // Free1
s_addc_u32 s91, s91, s93                           // Free1
s_sub_u32 s94, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s94, s94, 1                              // Free2
s_mul_hi_u32 s93, s94, s[sgprStrideCK]             // Free2
s_mul_i32 s92, s94, s[sgprStrideCK]                // Free2
s_add_u32 s90, s90, s92                            // Free2
s_addc_u32 s91, s91, s93                           // Free2
s_lshl_b64 s[86:87], s[90:91], 2                   // scale by bpe

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_6
// check done end

// buffer load start
buffer_load_dwordx4 v[20:23], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_132 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_132 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_132 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_132 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_132 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_132 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_132 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_132:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_132 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_132 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_132 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_132 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_132 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_132 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[192:193]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_132 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_132     // Syncbranchhere

label_Synchronizer_read_add_end_7_132:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_132      // SyncAddbranch

label_Synchronizer_read_add_end_6_132:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_132      // SyncAddbranch

label_Synchronizer_read_add_end_5_132:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_132      // SyncAddbranch

label_Synchronizer_read_add_end_4_132:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_132      // SyncAddbranch

label_Synchronizer_read_add_end_3_132:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_132      // SyncAddbranch

label_Synchronizer_read_add_end_2_132:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_132      // SyncAddbranch

label_Synchronizer_read_add_end_1_132:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_132:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[24:27], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_133 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_133 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_133 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_133 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_133 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_133 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_133 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_133:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_133 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_133 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_133 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_133 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_133 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_133 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_133 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_133     // Syncbranchhere

label_Synchronizer_read_add_end_7_133:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_133      // SyncAddbranch

label_Synchronizer_read_add_end_6_133:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_133      // SyncAddbranch

label_Synchronizer_read_add_end_5_133:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_133      // SyncAddbranch

label_Synchronizer_read_add_end_4_133:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_133      // SyncAddbranch

label_Synchronizer_read_add_end_3_133:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_133      // SyncAddbranch

label_Synchronizer_read_add_end_2_133:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_133      // SyncAddbranch

label_Synchronizer_read_add_end_1_133:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_133:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[28:31], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_134 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_134 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_134 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_134 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_134 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_134 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_134 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_134:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_134 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_134 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_134 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_134 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_134 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_134 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[192:193]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_134 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_134     // Syncbranchhere

label_Synchronizer_read_add_end_7_134:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_134      // SyncAddbranch

label_Synchronizer_read_add_end_6_134:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_134      // SyncAddbranch

label_Synchronizer_read_add_end_5_134:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_134      // SyncAddbranch

label_Synchronizer_read_add_end_4_134:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_134      // SyncAddbranch

label_Synchronizer_read_add_end_3_134:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_134      // SyncAddbranch

label_Synchronizer_read_add_end_2_134:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_134      // SyncAddbranch

label_Synchronizer_read_add_end_1_134:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_134:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[32:35], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_135 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_135 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_135 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_135 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_135 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_135 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_135 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_135:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_135 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_135 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_135 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_135 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_135 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_135 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[192:193]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_135 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_135     // Syncbranchhere

label_Synchronizer_read_add_end_7_135:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_135      // SyncAddbranch

label_Synchronizer_read_add_end_6_135:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_135      // SyncAddbranch

label_Synchronizer_read_add_end_5_135:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_135      // SyncAddbranch

label_Synchronizer_read_add_end_4_135:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_135      // SyncAddbranch

label_Synchronizer_read_add_end_3_135:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_135      // SyncAddbranch

label_Synchronizer_read_add_end_2_135:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_135      // SyncAddbranch

label_Synchronizer_read_add_end_1_135:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_135:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha

/* apply mask, calc new C and issue writes */

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_mul_f32 v[vgprValuC+20], 0xbfb8aa3b, v[vgprValuC+20] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+20], v[vgprValuC+20]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+20], 1.0, v[vgprValuC+20]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+20], v[vgprValuC+20]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+21], 0xbfb8aa3b, v[vgprValuC+21] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+21], v[vgprValuC+21]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+21], 1.0, v[vgprValuC+21]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+21], v[vgprValuC+21]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+22], 0xbfb8aa3b, v[vgprValuC+22] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+22], v[vgprValuC+22]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+22], 1.0, v[vgprValuC+22]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+22], v[vgprValuC+22]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+23], 0xbfb8aa3b, v[vgprValuC+23] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+23], v[vgprValuC+23]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+23], 1.0, v[vgprValuC+23]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+23], v[vgprValuC+23]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[12:13], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[14:15], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v[vgprValuC+24], 0xbfb8aa3b, v[vgprValuC+24] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+24], v[vgprValuC+24]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+24], 1.0, v[vgprValuC+24]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+24], v[vgprValuC+24]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+25], 0xbfb8aa3b, v[vgprValuC+25] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+25], v[vgprValuC+25]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+25], 1.0, v[vgprValuC+25]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+25], v[vgprValuC+25]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+26], 0xbfb8aa3b, v[vgprValuC+26] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+26], v[vgprValuC+26]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+26], 1.0, v[vgprValuC+26]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+26], v[vgprValuC+26]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+27], 0xbfb8aa3b, v[vgprValuC+27] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+27], v[vgprValuC+27]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+27], 1.0, v[vgprValuC+27]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+27], v[vgprValuC+27]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_mul_f32 v[vgprValuC+28], 0xbfb8aa3b, v[vgprValuC+28] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+28], v[vgprValuC+28]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+28], 1.0, v[vgprValuC+28]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+28], v[vgprValuC+28]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+29], 0xbfb8aa3b, v[vgprValuC+29] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+29], v[vgprValuC+29]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+29], 1.0, v[vgprValuC+29]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+29], v[vgprValuC+29]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+30], 0xbfb8aa3b, v[vgprValuC+30] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+30], v[vgprValuC+30]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+30], 1.0, v[vgprValuC+30]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+30], v[vgprValuC+30]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+31], 0xbfb8aa3b, v[vgprValuC+31] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+31], v[vgprValuC+31]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+31], 1.0, v[vgprValuC+31]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+31], v[vgprValuC+31]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[12:13], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[14:15], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v[vgprValuC+32], 0xbfb8aa3b, v[vgprValuC+32] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+32], v[vgprValuC+32]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+32], 1.0, v[vgprValuC+32]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+32], v[vgprValuC+32]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+33], 0xbfb8aa3b, v[vgprValuC+33] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+33], v[vgprValuC+33]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+33], 1.0, v[vgprValuC+33]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+33], v[vgprValuC+33]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+34], 0xbfb8aa3b, v[vgprValuC+34] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+34], v[vgprValuC+34]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+34], 1.0, v[vgprValuC+34]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+34], v[vgprValuC+34]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+35], 0xbfb8aa3b, v[vgprValuC+35] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+35], v[vgprValuC+35]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+35], 1.0, v[vgprValuC+35]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+35], v[vgprValuC+35]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_6:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End
label_Activation_Tanh:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v9, v0, s66
v_lshlrev_b32 v9, 0x2, v9                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v9 offset:0                 // load bias
v_lshlrev_b32 v10, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v10, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_lshl_u32 v6, v3, v0, 0x2                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_add_lshl_u32 v7, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+24], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+25], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+26], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+27], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+28], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+29], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+30], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+31], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+32], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+33], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+34], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+35], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[20:23], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[28:31], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[32:35], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s84, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s84, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s84, s84, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s84, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s91, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s90, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s94, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s94, s94, 1                              // Free1
s_mul_hi_u32 s93, s94, s[sgprStrideC1J]            // Free1
s_mul_i32 s92, s94, s[sgprStrideC1J]               // Free1
s_add_u32 s90, s90, s92                            // Free1
s_addc_u32 s91, s91, s93                           // Free1
s_sub_u32 s94, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s94, s94, 1                              // Free2
s_mul_hi_u32 s93, s94, s[sgprStrideCK]             // Free2
s_mul_i32 s92, s94, s[sgprStrideCK]                // Free2
s_add_u32 s90, s90, s92                            // Free2
s_addc_u32 s91, s91, s93                           // Free2
s_lshl_b64 s[86:87], s[90:91], 2                   // scale by bpe

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_7
// check done end

// buffer load start
buffer_load_dwordx4 v[20:23], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_136 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_136 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_136 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_136 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_136 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_136 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_136 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_136:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_136 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_136 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_136 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_136 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_136 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_136 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[192:193]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_136 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_136     // Syncbranchhere

label_Synchronizer_read_add_end_7_136:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_136      // SyncAddbranch

label_Synchronizer_read_add_end_6_136:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_136      // SyncAddbranch

label_Synchronizer_read_add_end_5_136:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_136      // SyncAddbranch

label_Synchronizer_read_add_end_4_136:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_136      // SyncAddbranch

label_Synchronizer_read_add_end_3_136:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_136      // SyncAddbranch

label_Synchronizer_read_add_end_2_136:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_136      // SyncAddbranch

label_Synchronizer_read_add_end_1_136:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_136:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[24:27], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_137 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_137 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_137 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_137 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_137 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_137 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_137 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_137:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_137 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_137 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_137 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_137 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_137 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_137 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_137 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_137     // Syncbranchhere

label_Synchronizer_read_add_end_7_137:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_137      // SyncAddbranch

label_Synchronizer_read_add_end_6_137:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_137      // SyncAddbranch

label_Synchronizer_read_add_end_5_137:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_137      // SyncAddbranch

label_Synchronizer_read_add_end_4_137:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_137      // SyncAddbranch

label_Synchronizer_read_add_end_3_137:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_137      // SyncAddbranch

label_Synchronizer_read_add_end_2_137:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_137      // SyncAddbranch

label_Synchronizer_read_add_end_1_137:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_137:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[28:31], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_138 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_138 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_138 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_138 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_138 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_138 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_138 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_138:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_138 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_138 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_138 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_138 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_138 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_138 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[192:193]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_138 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_138     // Syncbranchhere

label_Synchronizer_read_add_end_7_138:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_138      // SyncAddbranch

label_Synchronizer_read_add_end_6_138:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_138      // SyncAddbranch

label_Synchronizer_read_add_end_5_138:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_138      // SyncAddbranch

label_Synchronizer_read_add_end_4_138:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_138      // SyncAddbranch

label_Synchronizer_read_add_end_3_138:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_138      // SyncAddbranch

label_Synchronizer_read_add_end_2_138:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_138      // SyncAddbranch

label_Synchronizer_read_add_end_1_138:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_138:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[32:35], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_139 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_139 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_139 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_139 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_139 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_139 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_139 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_139:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_139 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_139 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_139 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_139 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_139 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_139 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[192:193]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_139 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_139     // Syncbranchhere

label_Synchronizer_read_add_end_7_139:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_139      // SyncAddbranch

label_Synchronizer_read_add_end_6_139:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_139      // SyncAddbranch

label_Synchronizer_read_add_end_5_139:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_139      // SyncAddbranch

label_Synchronizer_read_add_end_4_139:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_139      // SyncAddbranch

label_Synchronizer_read_add_end_3_139:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_139      // SyncAddbranch

label_Synchronizer_read_add_end_2_139:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_139      // SyncAddbranch

label_Synchronizer_read_add_end_1_139:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_139:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha

/* apply mask, calc new C and issue writes */

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_mul_f32 v[vgprValuC+20], s[sgpractivationAlpha], v[vgprValuC+20] // x * alpha
v_mul_f32 v[vgprValuC+20], 0x4038aa3b, v[vgprValuC+20] //  (fused 2)
v_exp_f32 v[vgprValuC+20], v[vgprValuC+20]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+20], 1.0, v[vgprValuC+20]    // e^2x + 1
v_rcp_f32 v[vgprValuC+20], v[vgprValuC+20]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+20], -2.0, v[vgprValuC+20], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+20], s[sgpractivationBeta], v[vgprValuC+20] // beta * tanh(x)
v_mul_f32 v[vgprValuC+21], s[sgpractivationAlpha], v[vgprValuC+21] // x * alpha
v_mul_f32 v[vgprValuC+21], 0x4038aa3b, v[vgprValuC+21] //  (fused 2)
v_exp_f32 v[vgprValuC+21], v[vgprValuC+21]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+21], 1.0, v[vgprValuC+21]    // e^2x + 1
v_rcp_f32 v[vgprValuC+21], v[vgprValuC+21]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+21], -2.0, v[vgprValuC+21], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+21], s[sgpractivationBeta], v[vgprValuC+21] // beta * tanh(x)
v_mul_f32 v[vgprValuC+22], s[sgpractivationAlpha], v[vgprValuC+22] // x * alpha
v_mul_f32 v[vgprValuC+22], 0x4038aa3b, v[vgprValuC+22] //  (fused 2)
v_exp_f32 v[vgprValuC+22], v[vgprValuC+22]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+22], 1.0, v[vgprValuC+22]    // e^2x + 1
v_rcp_f32 v[vgprValuC+22], v[vgprValuC+22]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+22], -2.0, v[vgprValuC+22], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+22], s[sgpractivationBeta], v[vgprValuC+22] // beta * tanh(x)
v_mul_f32 v[vgprValuC+23], s[sgpractivationAlpha], v[vgprValuC+23] // x * alpha
v_mul_f32 v[vgprValuC+23], 0x4038aa3b, v[vgprValuC+23] //  (fused 2)
v_exp_f32 v[vgprValuC+23], v[vgprValuC+23]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+23], 1.0, v[vgprValuC+23]    // e^2x + 1
v_rcp_f32 v[vgprValuC+23], v[vgprValuC+23]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+23], -2.0, v[vgprValuC+23], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+23], s[sgpractivationBeta], v[vgprValuC+23] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[12:13], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[14:15], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v[vgprValuC+24], s[sgpractivationAlpha], v[vgprValuC+24] // x * alpha
v_mul_f32 v[vgprValuC+24], 0x4038aa3b, v[vgprValuC+24] //  (fused 2)
v_exp_f32 v[vgprValuC+24], v[vgprValuC+24]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+24], 1.0, v[vgprValuC+24]    // e^2x + 1
v_rcp_f32 v[vgprValuC+24], v[vgprValuC+24]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+24], -2.0, v[vgprValuC+24], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+24], s[sgpractivationBeta], v[vgprValuC+24] // beta * tanh(x)
v_mul_f32 v[vgprValuC+25], s[sgpractivationAlpha], v[vgprValuC+25] // x * alpha
v_mul_f32 v[vgprValuC+25], 0x4038aa3b, v[vgprValuC+25] //  (fused 2)
v_exp_f32 v[vgprValuC+25], v[vgprValuC+25]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+25], 1.0, v[vgprValuC+25]    // e^2x + 1
v_rcp_f32 v[vgprValuC+25], v[vgprValuC+25]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+25], -2.0, v[vgprValuC+25], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+25], s[sgpractivationBeta], v[vgprValuC+25] // beta * tanh(x)
v_mul_f32 v[vgprValuC+26], s[sgpractivationAlpha], v[vgprValuC+26] // x * alpha
v_mul_f32 v[vgprValuC+26], 0x4038aa3b, v[vgprValuC+26] //  (fused 2)
v_exp_f32 v[vgprValuC+26], v[vgprValuC+26]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+26], 1.0, v[vgprValuC+26]    // e^2x + 1
v_rcp_f32 v[vgprValuC+26], v[vgprValuC+26]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+26], -2.0, v[vgprValuC+26], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+26], s[sgpractivationBeta], v[vgprValuC+26] // beta * tanh(x)
v_mul_f32 v[vgprValuC+27], s[sgpractivationAlpha], v[vgprValuC+27] // x * alpha
v_mul_f32 v[vgprValuC+27], 0x4038aa3b, v[vgprValuC+27] //  (fused 2)
v_exp_f32 v[vgprValuC+27], v[vgprValuC+27]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+27], 1.0, v[vgprValuC+27]    // e^2x + 1
v_rcp_f32 v[vgprValuC+27], v[vgprValuC+27]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+27], -2.0, v[vgprValuC+27], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+27], s[sgpractivationBeta], v[vgprValuC+27] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_mul_f32 v[vgprValuC+28], s[sgpractivationAlpha], v[vgprValuC+28] // x * alpha
v_mul_f32 v[vgprValuC+28], 0x4038aa3b, v[vgprValuC+28] //  (fused 2)
v_exp_f32 v[vgprValuC+28], v[vgprValuC+28]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+28], 1.0, v[vgprValuC+28]    // e^2x + 1
v_rcp_f32 v[vgprValuC+28], v[vgprValuC+28]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+28], -2.0, v[vgprValuC+28], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+28], s[sgpractivationBeta], v[vgprValuC+28] // beta * tanh(x)
v_mul_f32 v[vgprValuC+29], s[sgpractivationAlpha], v[vgprValuC+29] // x * alpha
v_mul_f32 v[vgprValuC+29], 0x4038aa3b, v[vgprValuC+29] //  (fused 2)
v_exp_f32 v[vgprValuC+29], v[vgprValuC+29]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+29], 1.0, v[vgprValuC+29]    // e^2x + 1
v_rcp_f32 v[vgprValuC+29], v[vgprValuC+29]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+29], -2.0, v[vgprValuC+29], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+29], s[sgpractivationBeta], v[vgprValuC+29] // beta * tanh(x)
v_mul_f32 v[vgprValuC+30], s[sgpractivationAlpha], v[vgprValuC+30] // x * alpha
v_mul_f32 v[vgprValuC+30], 0x4038aa3b, v[vgprValuC+30] //  (fused 2)
v_exp_f32 v[vgprValuC+30], v[vgprValuC+30]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+30], 1.0, v[vgprValuC+30]    // e^2x + 1
v_rcp_f32 v[vgprValuC+30], v[vgprValuC+30]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+30], -2.0, v[vgprValuC+30], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+30], s[sgpractivationBeta], v[vgprValuC+30] // beta * tanh(x)
v_mul_f32 v[vgprValuC+31], s[sgpractivationAlpha], v[vgprValuC+31] // x * alpha
v_mul_f32 v[vgprValuC+31], 0x4038aa3b, v[vgprValuC+31] //  (fused 2)
v_exp_f32 v[vgprValuC+31], v[vgprValuC+31]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+31], 1.0, v[vgprValuC+31]    // e^2x + 1
v_rcp_f32 v[vgprValuC+31], v[vgprValuC+31]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+31], -2.0, v[vgprValuC+31], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+31], s[sgpractivationBeta], v[vgprValuC+31] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[12:13], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[14:15], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v[vgprValuC+32], s[sgpractivationAlpha], v[vgprValuC+32] // x * alpha
v_mul_f32 v[vgprValuC+32], 0x4038aa3b, v[vgprValuC+32] //  (fused 2)
v_exp_f32 v[vgprValuC+32], v[vgprValuC+32]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+32], 1.0, v[vgprValuC+32]    // e^2x + 1
v_rcp_f32 v[vgprValuC+32], v[vgprValuC+32]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+32], -2.0, v[vgprValuC+32], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+32], s[sgpractivationBeta], v[vgprValuC+32] // beta * tanh(x)
v_mul_f32 v[vgprValuC+33], s[sgpractivationAlpha], v[vgprValuC+33] // x * alpha
v_mul_f32 v[vgprValuC+33], 0x4038aa3b, v[vgprValuC+33] //  (fused 2)
v_exp_f32 v[vgprValuC+33], v[vgprValuC+33]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+33], 1.0, v[vgprValuC+33]    // e^2x + 1
v_rcp_f32 v[vgprValuC+33], v[vgprValuC+33]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+33], -2.0, v[vgprValuC+33], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+33], s[sgpractivationBeta], v[vgprValuC+33] // beta * tanh(x)
v_mul_f32 v[vgprValuC+34], s[sgpractivationAlpha], v[vgprValuC+34] // x * alpha
v_mul_f32 v[vgprValuC+34], 0x4038aa3b, v[vgprValuC+34] //  (fused 2)
v_exp_f32 v[vgprValuC+34], v[vgprValuC+34]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+34], 1.0, v[vgprValuC+34]    // e^2x + 1
v_rcp_f32 v[vgprValuC+34], v[vgprValuC+34]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+34], -2.0, v[vgprValuC+34], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+34], s[sgpractivationBeta], v[vgprValuC+34] // beta * tanh(x)
v_mul_f32 v[vgprValuC+35], s[sgpractivationAlpha], v[vgprValuC+35] // x * alpha
v_mul_f32 v[vgprValuC+35], 0x4038aa3b, v[vgprValuC+35] //  (fused 2)
v_exp_f32 v[vgprValuC+35], v[vgprValuC+35]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+35], 1.0, v[vgprValuC+35]    // e^2x + 1
v_rcp_f32 v[vgprValuC+35], v[vgprValuC+35]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+35], -2.0, v[vgprValuC+35], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+35], s[sgpractivationBeta], v[vgprValuC+35] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_7:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End
label_Activation_Geluscaling:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v9, v0, s66
v_lshlrev_b32 v9, 0x2, v9                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v9 offset:0                 // load bias
v_lshlrev_b32 v10, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v10, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_lshl_u32 v6, v3, v0, 0x2                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_add_lshl_u32 v7, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+24], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+25], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+26], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+27], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+28], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+29], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+30], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+31], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+32], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+33], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+34], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+35], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[20:23], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[28:31], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[32:35], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s84, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s84, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s84, s84, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s84, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s91, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s90, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s94, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s94, s94, 1                              // Free1
s_mul_hi_u32 s93, s94, s[sgprStrideC1J]            // Free1
s_mul_i32 s92, s94, s[sgprStrideC1J]               // Free1
s_add_u32 s90, s90, s92                            // Free1
s_addc_u32 s91, s91, s93                           // Free1
s_sub_u32 s94, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s94, s94, 1                              // Free2
s_mul_hi_u32 s93, s94, s[sgprStrideCK]             // Free2
s_mul_i32 s92, s94, s[sgprStrideCK]                // Free2
s_add_u32 s90, s90, s92                            // Free2
s_addc_u32 s91, s91, s93                           // Free2
s_lshl_b64 s[86:87], s[90:91], 2                   // scale by bpe

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_8
// check done end

// buffer load start
buffer_load_dwordx4 v[20:23], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_140 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_140 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_140 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_140 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_140 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_140 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_140 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_140:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_140 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_140 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_140 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_140 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_140 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_140 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[192:193]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_140 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_140     // Syncbranchhere

label_Synchronizer_read_add_end_7_140:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_140      // SyncAddbranch

label_Synchronizer_read_add_end_6_140:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_140      // SyncAddbranch

label_Synchronizer_read_add_end_5_140:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_140      // SyncAddbranch

label_Synchronizer_read_add_end_4_140:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_140      // SyncAddbranch

label_Synchronizer_read_add_end_3_140:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_140      // SyncAddbranch

label_Synchronizer_read_add_end_2_140:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_140      // SyncAddbranch

label_Synchronizer_read_add_end_1_140:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_140:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[24:27], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_141 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_141 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_141 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_141 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_141 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_141 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_141 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_141:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_141 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_141 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_141 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_141 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_141 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_141 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_141 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_141     // Syncbranchhere

label_Synchronizer_read_add_end_7_141:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_141      // SyncAddbranch

label_Synchronizer_read_add_end_6_141:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_141      // SyncAddbranch

label_Synchronizer_read_add_end_5_141:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_141      // SyncAddbranch

label_Synchronizer_read_add_end_4_141:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_141      // SyncAddbranch

label_Synchronizer_read_add_end_3_141:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_141      // SyncAddbranch

label_Synchronizer_read_add_end_2_141:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_141      // SyncAddbranch

label_Synchronizer_read_add_end_1_141:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_141:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[28:31], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_142 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_142 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_142 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_142 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_142 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_142 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_142 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_142:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_142 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_142 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_142 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_142 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_142 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_142 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[192:193]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_142 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_142     // Syncbranchhere

label_Synchronizer_read_add_end_7_142:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_142      // SyncAddbranch

label_Synchronizer_read_add_end_6_142:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_142      // SyncAddbranch

label_Synchronizer_read_add_end_5_142:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_142      // SyncAddbranch

label_Synchronizer_read_add_end_4_142:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_142      // SyncAddbranch

label_Synchronizer_read_add_end_3_142:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_142      // SyncAddbranch

label_Synchronizer_read_add_end_2_142:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_142      // SyncAddbranch

label_Synchronizer_read_add_end_1_142:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_142:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[32:35], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_143 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_143 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_143 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_143 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_143 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_143 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_143 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_143:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_143 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_143 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_143 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_143 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_143 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_143 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[192:193]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_143 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v36, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v36, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_143     // Syncbranchhere

label_Synchronizer_read_add_end_7_143:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_143      // SyncAddbranch

label_Synchronizer_read_add_end_6_143:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_143      // SyncAddbranch

label_Synchronizer_read_add_end_5_143:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_143      // SyncAddbranch

label_Synchronizer_read_add_end_4_143:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_143      // SyncAddbranch

label_Synchronizer_read_add_end_3_143:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_143      // SyncAddbranch

label_Synchronizer_read_add_end_2_143:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_143      // SyncAddbranch

label_Synchronizer_read_add_end_1_143:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_143:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha

/* apply mask, calc new C and issue writes */

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+20]          // k1 * x
v_fma_f32 v4, v[vgprValuC+20], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+20], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+20], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+20], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+21]          // k1 * x
v_fma_f32 v4, v[vgprValuC+21], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+21], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+21], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+21], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+22]          // k1 * x
v_fma_f32 v4, v[vgprValuC+22], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+22], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+22], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+22], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+23]          // k1 * x
v_fma_f32 v4, v[vgprValuC+23], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+23], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+23], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+23], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[12:13], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[14:15], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+24]          // k1 * x
v_fma_f32 v4, v[vgprValuC+24], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+24], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+24], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+24], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+25]          // k1 * x
v_fma_f32 v4, v[vgprValuC+25], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+25], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+25], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+25], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+26]          // k1 * x
v_fma_f32 v4, v[vgprValuC+26], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+26], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+26], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+26], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+27]          // k1 * x
v_fma_f32 v4, v[vgprValuC+27], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+27], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+27], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+27], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+28]          // k1 * x
v_fma_f32 v4, v[vgprValuC+28], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+28], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+28], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+28], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+29]          // k1 * x
v_fma_f32 v4, v[vgprValuC+29], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+29], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+29], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+29], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+30]          // k1 * x
v_fma_f32 v4, v[vgprValuC+30], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+30], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+30], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+30], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+31]          // k1 * x
v_fma_f32 v4, v[vgprValuC+31], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+31], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+31], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+31], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[12:13], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[14:15], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+32]          // k1 * x
v_fma_f32 v4, v[vgprValuC+32], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+32], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+32], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+32], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+33]          // k1 * x
v_fma_f32 v4, v[vgprValuC+33], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+33], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+33], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+33], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+34]          // k1 * x
v_fma_f32 v4, v[vgprValuC+34], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+34], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+34], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+34], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+35]          // k1 * x
v_fma_f32 v4, v[vgprValuC+35], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+35], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+35], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+35], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_8:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
label_Activation_End:
// jump to end
s_getpc_b64 s[84:85]                               // addr of next instr
s_add_i32 s86, label_GW_End, 0x4                   // target branch offset
s_add_u32 s84, s84, s86                            // add target branch offset
s_addc_u32 s85, s85, 0                             // add high and carry
s_setpc_b64 s[84:85]                               // branch to label_GW_End
label_GW_B0_E1:

/* edge=1, allocate 6 sgpr. perBatchTmpS=4 perBatchMaskS=2 perElementMaskS=0 elementsPerBatch=14 */
s_cmpk_eq_u32 s[sgprActivationType], 0             // activationType == 0
s_cbranch_scc1 label_Activation_None_Edge          // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 1             // activationType == 1
s_cbranch_scc1 label_Activation_Abs_Edge           // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 2             // activationType == 2
s_cbranch_scc1 label_Activation_Clippedrelu_Edge   // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 3             // activationType == 3
s_cbranch_scc1 label_Activation_Gelu_Edge          // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 4             // activationType == 4
s_cbranch_scc1 label_Activation_Leakyrelu_Edge     // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 5             // activationType == 5
s_cbranch_scc1 label_Activation_Relu_Edge          // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 6             // activationType == 6
s_cbranch_scc1 label_Activation_Sigmoid_Edge       // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 7             // activationType == 7
s_cbranch_scc1 label_Activation_Tanh_Edge          // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 9             // activationType == 9
s_cbranch_scc1 label_Activation_Geluscaling_Edge   // Branch if true
label_Activation_None_Edge:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v42, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s84
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
v_cndmask_b32 v8, v42, v8, s[88:89]                // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v8 offset:0                 // load bias
v_lshlrev_b32 v9, 0x2, v0                          // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v9, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v42, v6, s[88:89]                // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v42, v7, s[88:89]                // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v24, v0, s84
v_lshlrev_b32 v24, 0x2, v24                        // Bias address scaled by BPE
v_cndmask_b32 v24, v42, v24, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v25, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v42, v10, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v11, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v42, v11, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v0, s84
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v42, v32, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v33, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v42, v26, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v27, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v42, v27, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s84
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_cndmask_b32 v40, v42, v40, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v41, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v34, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v42, v34, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v35, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v35, v42, v35, s[88:89]              // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[20:23], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

buffer_store_dwordx4 v[28:31], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

buffer_store_dwordx4 v[36:39], v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

buffer_store_dwordx4 v[44:47], v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s66, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s66, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s66, s66, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s66, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s95, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s94, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s98, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s98, s98, 1                              // Free1
s_mul_hi_u32 s97, s98, s[sgprStrideC1J]            // Free1
s_mul_i32 s96, s98, s[sgprStrideC1J]               // Free1
s_add_u32 s94, s94, s96                            // Free1
s_addc_u32 s95, s95, s97                           // Free1
s_sub_u32 s98, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s98, s98, 1                              // Free2
s_mul_hi_u32 s97, s98, s[sgprStrideCK]             // Free2
s_mul_i32 s96, s98, s[sgprStrideCK]                // Free2
s_add_u32 s94, s94, s96                            // Free2
s_addc_u32 s95, s95, s97                           // Free2
s_lshl_b64 s[90:91], s[94:95], 2                   // scale by bpe

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Edge_0
// check done end

// buffer load start
buffer_load_dwordx4 v[20:23], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_72 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_72 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_72 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_72 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_72 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_72 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_72 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_72:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_72 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_72 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_72 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_72 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_72 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_72 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[192:193]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_72 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_72      // Syncbranchhere

label_Synchronizer_read_add_end_7_72:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_72       // SyncAddbranch

label_Synchronizer_read_add_end_6_72:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_72       // SyncAddbranch

label_Synchronizer_read_add_end_5_72:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_72       // SyncAddbranch

label_Synchronizer_read_add_end_4_72:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_72       // SyncAddbranch

label_Synchronizer_read_add_end_3_72:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_72       // SyncAddbranch

label_Synchronizer_read_add_end_2_72:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_72       // SyncAddbranch

label_Synchronizer_read_add_end_1_72:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_72:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[28:31], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_73 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_73 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_73 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_73 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_73 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_73 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_73 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_73:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_73 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_73 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_73 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_73 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_73 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_73 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[192:193]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_73 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_73      // Syncbranchhere

label_Synchronizer_read_add_end_7_73:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_73       // SyncAddbranch

label_Synchronizer_read_add_end_6_73:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_73       // SyncAddbranch

label_Synchronizer_read_add_end_5_73:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_73       // SyncAddbranch

label_Synchronizer_read_add_end_4_73:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_73       // SyncAddbranch

label_Synchronizer_read_add_end_3_73:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_73       // SyncAddbranch

label_Synchronizer_read_add_end_2_73:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_73       // SyncAddbranch

label_Synchronizer_read_add_end_1_73:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_73:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[36:39], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_74 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_74 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_74 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_74 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_74 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_74 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_74 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_74:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_74 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_74 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_74 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_74 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_74 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_74 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[192:193]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_74 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_74      // Syncbranchhere

label_Synchronizer_read_add_end_7_74:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_74       // SyncAddbranch

label_Synchronizer_read_add_end_6_74:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_74       // SyncAddbranch

label_Synchronizer_read_add_end_5_74:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_74       // SyncAddbranch

label_Synchronizer_read_add_end_4_74:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_74       // SyncAddbranch

label_Synchronizer_read_add_end_3_74:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_74       // SyncAddbranch

label_Synchronizer_read_add_end_2_74:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_74       // SyncAddbranch

label_Synchronizer_read_add_end_1_74:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_74:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[44:47], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_75 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_75 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_75 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_75 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_75 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_75 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_75 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_75:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_75 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_75 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_75 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_75 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_75 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[176:177]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_75 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[192:193]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_75 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_75      // Syncbranchhere

label_Synchronizer_read_add_end_7_75:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[176:177]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_75       // SyncAddbranch

label_Synchronizer_read_add_end_6_75:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_75       // SyncAddbranch

label_Synchronizer_read_add_end_5_75:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_75       // SyncAddbranch

label_Synchronizer_read_add_end_4_75:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_75       // SyncAddbranch

label_Synchronizer_read_add_end_3_75:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_75       // SyncAddbranch

label_Synchronizer_read_add_end_2_75:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_75       // SyncAddbranch

label_Synchronizer_read_add_end_1_75:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_75:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
buffer_store_dwordx2 v[28:29], v11, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[12:13], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[14:15], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v27, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[12:13], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[14:15], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v35, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Edge_0:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Edge
label_Activation_Abs_Edge:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v42, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s84
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
v_cndmask_b32 v8, v42, v8, s[88:89]                // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v8 offset:0                 // load bias
v_lshlrev_b32 v9, 0x2, v0                          // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v9, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v42, v6, s[88:89]                // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v42, v7, s[88:89]                // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v24, v0, s84
v_lshlrev_b32 v24, 0x2, v24                        // Bias address scaled by BPE
v_cndmask_b32 v24, v42, v24, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v25, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v42, v10, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v11, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v42, v11, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v0, s84
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v42, v32, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v33, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v42, v26, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v27, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v42, v27, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s84
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_cndmask_b32 v40, v42, v40, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v41, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v34, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v42, v34, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v35, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v35, v42, v35, s[88:89]              // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[20:23], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

buffer_store_dwordx4 v[28:31], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

buffer_store_dwordx4 v[36:39], v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

buffer_store_dwordx4 v[44:47], v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s66, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s66, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s66, s66, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s66, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s95, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s94, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s98, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s98, s98, 1                              // Free1
s_mul_hi_u32 s97, s98, s[sgprStrideC1J]            // Free1
s_mul_i32 s96, s98, s[sgprStrideC1J]               // Free1
s_add_u32 s94, s94, s96                            // Free1
s_addc_u32 s95, s95, s97                           // Free1
s_sub_u32 s98, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s98, s98, 1                              // Free2
s_mul_hi_u32 s97, s98, s[sgprStrideCK]             // Free2
s_mul_i32 s96, s98, s[sgprStrideCK]                // Free2
s_add_u32 s94, s94, s96                            // Free2
s_addc_u32 s95, s95, s97                           // Free2
s_lshl_b64 s[90:91], s[94:95], 2                   // scale by bpe

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Edge_1
// check done end

// buffer load start
buffer_load_dwordx4 v[20:23], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_76 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_76 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_76 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_76 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_76 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_76 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_76 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_76:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_76 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_76 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_76 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_76 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_76 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_76 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[192:193]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_76 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_76      // Syncbranchhere

label_Synchronizer_read_add_end_7_76:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_76       // SyncAddbranch

label_Synchronizer_read_add_end_6_76:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_76       // SyncAddbranch

label_Synchronizer_read_add_end_5_76:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_76       // SyncAddbranch

label_Synchronizer_read_add_end_4_76:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_76       // SyncAddbranch

label_Synchronizer_read_add_end_3_76:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_76       // SyncAddbranch

label_Synchronizer_read_add_end_2_76:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_76       // SyncAddbranch

label_Synchronizer_read_add_end_1_76:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_76:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[28:31], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_77 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_77 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_77 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_77 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_77 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_77 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_77 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_77:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_77 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_77 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_77 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_77 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_77 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_77 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[192:193]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_77 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_77      // Syncbranchhere

label_Synchronizer_read_add_end_7_77:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_77       // SyncAddbranch

label_Synchronizer_read_add_end_6_77:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_77       // SyncAddbranch

label_Synchronizer_read_add_end_5_77:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_77       // SyncAddbranch

label_Synchronizer_read_add_end_4_77:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_77       // SyncAddbranch

label_Synchronizer_read_add_end_3_77:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_77       // SyncAddbranch

label_Synchronizer_read_add_end_2_77:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_77       // SyncAddbranch

label_Synchronizer_read_add_end_1_77:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_77:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[36:39], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_78 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_78 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_78 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_78 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_78 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_78 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_78 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_78:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_78 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_78 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_78 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_78 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_78 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_78 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[192:193]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_78 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_78      // Syncbranchhere

label_Synchronizer_read_add_end_7_78:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_78       // SyncAddbranch

label_Synchronizer_read_add_end_6_78:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_78       // SyncAddbranch

label_Synchronizer_read_add_end_5_78:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_78       // SyncAddbranch

label_Synchronizer_read_add_end_4_78:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_78       // SyncAddbranch

label_Synchronizer_read_add_end_3_78:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_78       // SyncAddbranch

label_Synchronizer_read_add_end_2_78:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_78       // SyncAddbranch

label_Synchronizer_read_add_end_1_78:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_78:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[44:47], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_79 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_79 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_79 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_79 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_79 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_79 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_79 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_79:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_79 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_79 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_79 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_79 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_79 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[176:177]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_79 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[192:193]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_79 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_79      // Syncbranchhere

label_Synchronizer_read_add_end_7_79:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[176:177]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_79       // SyncAddbranch

label_Synchronizer_read_add_end_6_79:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_79       // SyncAddbranch

label_Synchronizer_read_add_end_5_79:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_79       // SyncAddbranch

label_Synchronizer_read_add_end_4_79:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_79       // SyncAddbranch

label_Synchronizer_read_add_end_3_79:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_79       // SyncAddbranch

label_Synchronizer_read_add_end_2_79:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_79       // SyncAddbranch

label_Synchronizer_read_add_end_1_79:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_79:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_and_b32 v[vgprValuC+20], 0x7fffffff, v[vgprValuC+20] // Remove sign bit
v_and_b32 v[vgprValuC+21], 0x7fffffff, v[vgprValuC+21] // Remove sign bit
v_and_b32 v[vgprValuC+22], 0x7fffffff, v[vgprValuC+22] // Remove sign bit
v_and_b32 v[vgprValuC+23], 0x7fffffff, v[vgprValuC+23] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_and_b32 v[vgprValuC+28], 0x7fffffff, v[vgprValuC+28] // Remove sign bit
v_and_b32 v[vgprValuC+29], 0x7fffffff, v[vgprValuC+29] // Remove sign bit
v_and_b32 v[vgprValuC+30], 0x7fffffff, v[vgprValuC+30] // Remove sign bit
v_and_b32 v[vgprValuC+31], 0x7fffffff, v[vgprValuC+31] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
buffer_store_dwordx2 v[28:29], v11, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[12:13], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[14:15], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_and_b32 v[vgprValuC+36], 0x7fffffff, v[vgprValuC+36] // Remove sign bit
v_and_b32 v[vgprValuC+37], 0x7fffffff, v[vgprValuC+37] // Remove sign bit
v_and_b32 v[vgprValuC+38], 0x7fffffff, v[vgprValuC+38] // Remove sign bit
v_and_b32 v[vgprValuC+39], 0x7fffffff, v[vgprValuC+39] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v27, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[12:13], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[14:15], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_and_b32 v[vgprValuC+44], 0x7fffffff, v[vgprValuC+44] // Remove sign bit
v_and_b32 v[vgprValuC+45], 0x7fffffff, v[vgprValuC+45] // Remove sign bit
v_and_b32 v[vgprValuC+46], 0x7fffffff, v[vgprValuC+46] // Remove sign bit
v_and_b32 v[vgprValuC+47], 0x7fffffff, v[vgprValuC+47] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v35, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Edge_1:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Edge
label_Activation_Clippedrelu_Edge:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v42, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s84
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
v_cndmask_b32 v8, v42, v8, s[88:89]                // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v8 offset:0                 // load bias
v_lshlrev_b32 v9, 0x2, v0                          // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v9, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v42, v6, s[88:89]                // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v42, v7, s[88:89]                // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v24, v0, s84
v_lshlrev_b32 v24, 0x2, v24                        // Bias address scaled by BPE
v_cndmask_b32 v24, v42, v24, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v25, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v42, v10, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v11, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v42, v11, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v0, s84
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v42, v32, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v33, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v42, v26, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v27, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v42, v27, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s84
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_cndmask_b32 v40, v42, v40, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v41, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v34, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v42, v34, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v35, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v35, v42, v35, s[88:89]              // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[20:23], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

buffer_store_dwordx4 v[28:31], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

buffer_store_dwordx4 v[36:39], v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

buffer_store_dwordx4 v[44:47], v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s66, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s66, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s66, s66, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s66, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s95, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s94, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s98, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s98, s98, 1                              // Free1
s_mul_hi_u32 s97, s98, s[sgprStrideC1J]            // Free1
s_mul_i32 s96, s98, s[sgprStrideC1J]               // Free1
s_add_u32 s94, s94, s96                            // Free1
s_addc_u32 s95, s95, s97                           // Free1
s_sub_u32 s98, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s98, s98, 1                              // Free2
s_mul_hi_u32 s97, s98, s[sgprStrideCK]             // Free2
s_mul_i32 s96, s98, s[sgprStrideCK]                // Free2
s_add_u32 s94, s94, s96                            // Free2
s_addc_u32 s95, s95, s97                           // Free2
s_lshl_b64 s[90:91], s[94:95], 2                   // scale by bpe

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Edge_2
// check done end

// buffer load start
buffer_load_dwordx4 v[20:23], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_80 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_80 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_80 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_80 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_80 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_80 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_80 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_80:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_80 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_80 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_80 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_80 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_80 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_80 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[192:193]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_80 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_80      // Syncbranchhere

label_Synchronizer_read_add_end_7_80:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_80       // SyncAddbranch

label_Synchronizer_read_add_end_6_80:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_80       // SyncAddbranch

label_Synchronizer_read_add_end_5_80:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_80       // SyncAddbranch

label_Synchronizer_read_add_end_4_80:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_80       // SyncAddbranch

label_Synchronizer_read_add_end_3_80:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_80       // SyncAddbranch

label_Synchronizer_read_add_end_2_80:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_80       // SyncAddbranch

label_Synchronizer_read_add_end_1_80:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_80:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[28:31], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_81 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_81 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_81 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_81 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_81 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_81 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_81 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_81:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_81 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_81 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_81 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_81 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_81 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_81 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[192:193]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_81 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_81      // Syncbranchhere

label_Synchronizer_read_add_end_7_81:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_81       // SyncAddbranch

label_Synchronizer_read_add_end_6_81:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_81       // SyncAddbranch

label_Synchronizer_read_add_end_5_81:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_81       // SyncAddbranch

label_Synchronizer_read_add_end_4_81:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_81       // SyncAddbranch

label_Synchronizer_read_add_end_3_81:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_81       // SyncAddbranch

label_Synchronizer_read_add_end_2_81:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_81       // SyncAddbranch

label_Synchronizer_read_add_end_1_81:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_81:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[36:39], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_82 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_82 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_82 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_82 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_82 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_82 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_82 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_82:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_82 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_82 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_82 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_82 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_82 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_82 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[192:193]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_82 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_82      // Syncbranchhere

label_Synchronizer_read_add_end_7_82:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_82       // SyncAddbranch

label_Synchronizer_read_add_end_6_82:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_82       // SyncAddbranch

label_Synchronizer_read_add_end_5_82:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_82       // SyncAddbranch

label_Synchronizer_read_add_end_4_82:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_82       // SyncAddbranch

label_Synchronizer_read_add_end_3_82:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_82       // SyncAddbranch

label_Synchronizer_read_add_end_2_82:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_82       // SyncAddbranch

label_Synchronizer_read_add_end_1_82:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_82:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[44:47], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_83 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_83 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_83 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_83 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_83 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_83 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_83 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_83:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_83 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_83 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_83 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_83 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_83 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[176:177]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_83 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[192:193]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_83 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_83      // Syncbranchhere

label_Synchronizer_read_add_end_7_83:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[176:177]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_83       // SyncAddbranch

label_Synchronizer_read_add_end_6_83:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_83       // SyncAddbranch

label_Synchronizer_read_add_end_5_83:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_83       // SyncAddbranch

label_Synchronizer_read_add_end_4_83:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_83       // SyncAddbranch

label_Synchronizer_read_add_end_3_83:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_83       // SyncAddbranch

label_Synchronizer_read_add_end_2_83:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_83       // SyncAddbranch

label_Synchronizer_read_add_end_1_83:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_83:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+20], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+20], s[sgpractivationBeta], v[vgprValuC+20] // min(x, beta)
v_cndmask_b32 v[vgprValuC+20], 0.0, v[vgprValuC+20], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+21], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+21], s[sgpractivationBeta], v[vgprValuC+21] // min(x, beta)
v_cndmask_b32 v[vgprValuC+21], 0.0, v[vgprValuC+21], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+22], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+22], s[sgpractivationBeta], v[vgprValuC+22] // min(x, beta)
v_cndmask_b32 v[vgprValuC+22], 0.0, v[vgprValuC+22], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+23], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+23], s[sgpractivationBeta], v[vgprValuC+23] // min(x, beta)
v_cndmask_b32 v[vgprValuC+23], 0.0, v[vgprValuC+23], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+28], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+28], s[sgpractivationBeta], v[vgprValuC+28] // min(x, beta)
v_cndmask_b32 v[vgprValuC+28], 0.0, v[vgprValuC+28], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+29], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+29], s[sgpractivationBeta], v[vgprValuC+29] // min(x, beta)
v_cndmask_b32 v[vgprValuC+29], 0.0, v[vgprValuC+29], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+30], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+30], s[sgpractivationBeta], v[vgprValuC+30] // min(x, beta)
v_cndmask_b32 v[vgprValuC+30], 0.0, v[vgprValuC+30], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+31], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+31], s[sgpractivationBeta], v[vgprValuC+31] // min(x, beta)
v_cndmask_b32 v[vgprValuC+31], 0.0, v[vgprValuC+31], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
buffer_store_dwordx2 v[28:29], v11, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[12:13], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[14:15], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+36], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+36], s[sgpractivationBeta], v[vgprValuC+36] // min(x, beta)
v_cndmask_b32 v[vgprValuC+36], 0.0, v[vgprValuC+36], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+37], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+37], s[sgpractivationBeta], v[vgprValuC+37] // min(x, beta)
v_cndmask_b32 v[vgprValuC+37], 0.0, v[vgprValuC+37], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+38], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+38], s[sgpractivationBeta], v[vgprValuC+38] // min(x, beta)
v_cndmask_b32 v[vgprValuC+38], 0.0, v[vgprValuC+38], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+39], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+39], s[sgpractivationBeta], v[vgprValuC+39] // min(x, beta)
v_cndmask_b32 v[vgprValuC+39], 0.0, v[vgprValuC+39], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v27, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[12:13], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[14:15], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+44], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+44], s[sgpractivationBeta], v[vgprValuC+44] // min(x, beta)
v_cndmask_b32 v[vgprValuC+44], 0.0, v[vgprValuC+44], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+45], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+45], s[sgpractivationBeta], v[vgprValuC+45] // min(x, beta)
v_cndmask_b32 v[vgprValuC+45], 0.0, v[vgprValuC+45], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+46], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+46], s[sgpractivationBeta], v[vgprValuC+46] // min(x, beta)
v_cndmask_b32 v[vgprValuC+46], 0.0, v[vgprValuC+46], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+47], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+47], s[sgpractivationBeta], v[vgprValuC+47] // min(x, beta)
v_cndmask_b32 v[vgprValuC+47], 0.0, v[vgprValuC+47], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v35, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Edge_2:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Edge
label_Activation_Gelu_Edge:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v42, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s84
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
v_cndmask_b32 v8, v42, v8, s[88:89]                // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v8 offset:0                 // load bias
v_lshlrev_b32 v9, 0x2, v0                          // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v9, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v42, v6, s[88:89]                // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v42, v7, s[88:89]                // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v24, v0, s84
v_lshlrev_b32 v24, 0x2, v24                        // Bias address scaled by BPE
v_cndmask_b32 v24, v42, v24, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v25, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v42, v10, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v11, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v42, v11, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v0, s84
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v42, v32, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v33, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v42, v26, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v27, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v42, v27, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s84
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_cndmask_b32 v40, v42, v40, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v41, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v34, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v42, v34, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v35, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v35, v42, v35, s[88:89]              // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[20:23], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

buffer_store_dwordx4 v[28:31], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

buffer_store_dwordx4 v[36:39], v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

buffer_store_dwordx4 v[44:47], v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s66, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s66, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s66, s66, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s66, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s95, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s94, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s98, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s98, s98, 1                              // Free1
s_mul_hi_u32 s97, s98, s[sgprStrideC1J]            // Free1
s_mul_i32 s96, s98, s[sgprStrideC1J]               // Free1
s_add_u32 s94, s94, s96                            // Free1
s_addc_u32 s95, s95, s97                           // Free1
s_sub_u32 s98, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s98, s98, 1                              // Free2
s_mul_hi_u32 s97, s98, s[sgprStrideCK]             // Free2
s_mul_i32 s96, s98, s[sgprStrideCK]                // Free2
s_add_u32 s94, s94, s96                            // Free2
s_addc_u32 s95, s95, s97                           // Free2
s_lshl_b64 s[90:91], s[94:95], 2                   // scale by bpe

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Edge_3
// check done end

// buffer load start
buffer_load_dwordx4 v[20:23], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_84 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_84 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_84 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_84 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_84 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_84 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_84 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_84:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_84 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_84 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_84 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_84 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_84 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_84 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[192:193]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_84 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_84      // Syncbranchhere

label_Synchronizer_read_add_end_7_84:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_84       // SyncAddbranch

label_Synchronizer_read_add_end_6_84:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_84       // SyncAddbranch

label_Synchronizer_read_add_end_5_84:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_84       // SyncAddbranch

label_Synchronizer_read_add_end_4_84:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_84       // SyncAddbranch

label_Synchronizer_read_add_end_3_84:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_84       // SyncAddbranch

label_Synchronizer_read_add_end_2_84:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_84       // SyncAddbranch

label_Synchronizer_read_add_end_1_84:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_84:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[28:31], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_85 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_85 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_85 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_85 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_85 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_85 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_85 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_85:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_85 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_85 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_85 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_85 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_85 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_85 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[192:193]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_85 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_85      // Syncbranchhere

label_Synchronizer_read_add_end_7_85:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_85       // SyncAddbranch

label_Synchronizer_read_add_end_6_85:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_85       // SyncAddbranch

label_Synchronizer_read_add_end_5_85:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_85       // SyncAddbranch

label_Synchronizer_read_add_end_4_85:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_85       // SyncAddbranch

label_Synchronizer_read_add_end_3_85:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_85       // SyncAddbranch

label_Synchronizer_read_add_end_2_85:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_85       // SyncAddbranch

label_Synchronizer_read_add_end_1_85:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_85:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[36:39], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_86 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_86 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_86 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_86 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_86 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_86 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_86 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_86:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_86 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_86 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_86 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_86 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_86 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_86 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[192:193]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_86 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_86      // Syncbranchhere

label_Synchronizer_read_add_end_7_86:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_86       // SyncAddbranch

label_Synchronizer_read_add_end_6_86:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_86       // SyncAddbranch

label_Synchronizer_read_add_end_5_86:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_86       // SyncAddbranch

label_Synchronizer_read_add_end_4_86:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_86       // SyncAddbranch

label_Synchronizer_read_add_end_3_86:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_86       // SyncAddbranch

label_Synchronizer_read_add_end_2_86:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_86       // SyncAddbranch

label_Synchronizer_read_add_end_1_86:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_86:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[44:47], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_87 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_87 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_87 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_87 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_87 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_87 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_87 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_87:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_87 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_87 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_87 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_87 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_87 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[176:177]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_87 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[192:193]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_87 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_87      // Syncbranchhere

label_Synchronizer_read_add_end_7_87:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[176:177]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_87       // SyncAddbranch

label_Synchronizer_read_add_end_6_87:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_87       // SyncAddbranch

label_Synchronizer_read_add_end_5_87:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_87       // SyncAddbranch

label_Synchronizer_read_add_end_4_87:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_87       // SyncAddbranch

label_Synchronizer_read_add_end_3_87:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_87       // SyncAddbranch

label_Synchronizer_read_add_end_2_87:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_87       // SyncAddbranch

label_Synchronizer_read_add_end_1_87:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_87:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+20]          // k1 * x
v_fma_f32 v4, v[vgprValuC+20], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+20], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+20], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+20], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+21]          // k1 * x
v_fma_f32 v4, v[vgprValuC+21], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+21], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+21], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+21], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+22]          // k1 * x
v_fma_f32 v4, v[vgprValuC+22], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+22], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+22], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+22], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+23]          // k1 * x
v_fma_f32 v4, v[vgprValuC+23], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+23], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+23], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+23], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+28]          // k1 * x
v_fma_f32 v4, v[vgprValuC+28], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+28], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+28], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+28], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+29]          // k1 * x
v_fma_f32 v4, v[vgprValuC+29], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+29], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+29], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+29], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+30]          // k1 * x
v_fma_f32 v4, v[vgprValuC+30], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+30], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+30], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+30], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+31]          // k1 * x
v_fma_f32 v4, v[vgprValuC+31], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+31], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+31], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+31], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
buffer_store_dwordx2 v[28:29], v11, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[12:13], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[14:15], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+36]          // k1 * x
v_fma_f32 v4, v[vgprValuC+36], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+36], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+36], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+36], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+37]          // k1 * x
v_fma_f32 v4, v[vgprValuC+37], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+37], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+37], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+37], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+38]          // k1 * x
v_fma_f32 v4, v[vgprValuC+38], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+38], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+38], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+38], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+39]          // k1 * x
v_fma_f32 v4, v[vgprValuC+39], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+39], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+39], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+39], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v27, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[12:13], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[14:15], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+44]          // k1 * x
v_fma_f32 v4, v[vgprValuC+44], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+44], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+44], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+44], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+45]          // k1 * x
v_fma_f32 v4, v[vgprValuC+45], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+45], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+45], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+45], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+46]          // k1 * x
v_fma_f32 v4, v[vgprValuC+46], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+46], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+46], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+46], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+47]          // k1 * x
v_fma_f32 v4, v[vgprValuC+47], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+47], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+47], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+47], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v35, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Edge_3:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Edge
label_Activation_Leakyrelu_Edge:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v42, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s84
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
v_cndmask_b32 v8, v42, v8, s[88:89]                // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v8 offset:0                 // load bias
v_lshlrev_b32 v9, 0x2, v0                          // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v9, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v42, v6, s[88:89]                // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v42, v7, s[88:89]                // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v24, v0, s84
v_lshlrev_b32 v24, 0x2, v24                        // Bias address scaled by BPE
v_cndmask_b32 v24, v42, v24, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v25, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v42, v10, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v11, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v42, v11, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v0, s84
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v42, v32, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v33, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v42, v26, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v27, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v42, v27, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s84
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_cndmask_b32 v40, v42, v40, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v41, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v34, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v42, v34, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v35, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v35, v42, v35, s[88:89]              // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[20:23], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

buffer_store_dwordx4 v[28:31], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

buffer_store_dwordx4 v[36:39], v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

buffer_store_dwordx4 v[44:47], v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s66, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s66, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s66, s66, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s66, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s95, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s94, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s98, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s98, s98, 1                              // Free1
s_mul_hi_u32 s97, s98, s[sgprStrideC1J]            // Free1
s_mul_i32 s96, s98, s[sgprStrideC1J]               // Free1
s_add_u32 s94, s94, s96                            // Free1
s_addc_u32 s95, s95, s97                           // Free1
s_sub_u32 s98, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s98, s98, 1                              // Free2
s_mul_hi_u32 s97, s98, s[sgprStrideCK]             // Free2
s_mul_i32 s96, s98, s[sgprStrideCK]                // Free2
s_add_u32 s94, s94, s96                            // Free2
s_addc_u32 s95, s95, s97                           // Free2
s_lshl_b64 s[90:91], s[94:95], 2                   // scale by bpe

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Edge_4
// check done end

// buffer load start
buffer_load_dwordx4 v[20:23], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_88 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_88 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_88 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_88 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_88 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_88 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_88 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_88:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_88 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_88 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_88 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_88 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_88 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_88 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[192:193]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_88 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_88      // Syncbranchhere

label_Synchronizer_read_add_end_7_88:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_88       // SyncAddbranch

label_Synchronizer_read_add_end_6_88:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_88       // SyncAddbranch

label_Synchronizer_read_add_end_5_88:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_88       // SyncAddbranch

label_Synchronizer_read_add_end_4_88:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_88       // SyncAddbranch

label_Synchronizer_read_add_end_3_88:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_88       // SyncAddbranch

label_Synchronizer_read_add_end_2_88:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_88       // SyncAddbranch

label_Synchronizer_read_add_end_1_88:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_88:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[28:31], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_89 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_89 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_89 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_89 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_89 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_89 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_89 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_89:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_89 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_89 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_89 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_89 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_89 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_89 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[192:193]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_89 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_89      // Syncbranchhere

label_Synchronizer_read_add_end_7_89:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_89       // SyncAddbranch

label_Synchronizer_read_add_end_6_89:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_89       // SyncAddbranch

label_Synchronizer_read_add_end_5_89:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_89       // SyncAddbranch

label_Synchronizer_read_add_end_4_89:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_89       // SyncAddbranch

label_Synchronizer_read_add_end_3_89:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_89       // SyncAddbranch

label_Synchronizer_read_add_end_2_89:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_89       // SyncAddbranch

label_Synchronizer_read_add_end_1_89:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_89:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[36:39], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_90 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_90 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_90 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_90 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_90 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_90 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_90 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_90:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_90 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_90 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_90 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_90 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_90 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_90 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[192:193]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_90 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_90      // Syncbranchhere

label_Synchronizer_read_add_end_7_90:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_90       // SyncAddbranch

label_Synchronizer_read_add_end_6_90:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_90       // SyncAddbranch

label_Synchronizer_read_add_end_5_90:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_90       // SyncAddbranch

label_Synchronizer_read_add_end_4_90:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_90       // SyncAddbranch

label_Synchronizer_read_add_end_3_90:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_90       // SyncAddbranch

label_Synchronizer_read_add_end_2_90:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_90       // SyncAddbranch

label_Synchronizer_read_add_end_1_90:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_90:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[44:47], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_91 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_91 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_91 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_91 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_91 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_91 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_91 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_91:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_91 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_91 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_91 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_91 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_91 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[176:177]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_91 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[192:193]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_91 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_91      // Syncbranchhere

label_Synchronizer_read_add_end_7_91:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[176:177]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_91       // SyncAddbranch

label_Synchronizer_read_add_end_6_91:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_91       // SyncAddbranch

label_Synchronizer_read_add_end_5_91:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_91       // SyncAddbranch

label_Synchronizer_read_add_end_4_91:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_91       // SyncAddbranch

label_Synchronizer_read_add_end_3_91:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_91       // SyncAddbranch

label_Synchronizer_read_add_end_2_91:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_91       // SyncAddbranch

label_Synchronizer_read_add_end_1_91:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_91:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+20] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+20], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+20], v4, v[vgprValuC+20], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+21] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+21], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+21], v4, v[vgprValuC+21], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+22] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+22], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+22], v4, v[vgprValuC+22], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+23] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+23], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+23], v4, v[vgprValuC+23], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+28] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+28], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+28], v4, v[vgprValuC+28], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+29] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+29], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+29], v4, v[vgprValuC+29], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+30] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+30], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+30], v4, v[vgprValuC+30], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+31] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+31], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+31], v4, v[vgprValuC+31], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
buffer_store_dwordx2 v[28:29], v11, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[12:13], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[14:15], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+36] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+36], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+36], v4, v[vgprValuC+36], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+37] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+37], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+37], v4, v[vgprValuC+37], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+38] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+38], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+38], v4, v[vgprValuC+38], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+39] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+39], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+39], v4, v[vgprValuC+39], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v27, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[12:13], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[14:15], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+44] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+44], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+44], v4, v[vgprValuC+44], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+45] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+45], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+45], v4, v[vgprValuC+45], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+46] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+46], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+46], v4, v[vgprValuC+46], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+47] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+47], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+47], v4, v[vgprValuC+47], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v35, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Edge_4:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Edge
label_Activation_Relu_Edge:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v42, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s84
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
v_cndmask_b32 v8, v42, v8, s[88:89]                // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v8 offset:0                 // load bias
v_lshlrev_b32 v9, 0x2, v0                          // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v9, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v42, v6, s[88:89]                // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v42, v7, s[88:89]                // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v24, v0, s84
v_lshlrev_b32 v24, 0x2, v24                        // Bias address scaled by BPE
v_cndmask_b32 v24, v42, v24, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v25, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v42, v10, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v11, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v42, v11, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v0, s84
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v42, v32, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v33, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v42, v26, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v27, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v42, v27, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s84
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_cndmask_b32 v40, v42, v40, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v41, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v34, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v42, v34, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v35, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v35, v42, v35, s[88:89]              // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[20:23], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

buffer_store_dwordx4 v[28:31], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

buffer_store_dwordx4 v[36:39], v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

buffer_store_dwordx4 v[44:47], v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s66, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s66, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s66, s66, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s66, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s95, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s94, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s98, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s98, s98, 1                              // Free1
s_mul_hi_u32 s97, s98, s[sgprStrideC1J]            // Free1
s_mul_i32 s96, s98, s[sgprStrideC1J]               // Free1
s_add_u32 s94, s94, s96                            // Free1
s_addc_u32 s95, s95, s97                           // Free1
s_sub_u32 s98, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s98, s98, 1                              // Free2
s_mul_hi_u32 s97, s98, s[sgprStrideCK]             // Free2
s_mul_i32 s96, s98, s[sgprStrideCK]                // Free2
s_add_u32 s94, s94, s96                            // Free2
s_addc_u32 s95, s95, s97                           // Free2
s_lshl_b64 s[90:91], s[94:95], 2                   // scale by bpe

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Edge_5
// check done end

// buffer load start
buffer_load_dwordx4 v[20:23], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_92 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_92 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_92 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_92 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_92 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_92 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_92 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_92:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_92 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_92 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_92 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_92 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_92 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_92 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[192:193]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_92 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_92      // Syncbranchhere

label_Synchronizer_read_add_end_7_92:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_92       // SyncAddbranch

label_Synchronizer_read_add_end_6_92:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_92       // SyncAddbranch

label_Synchronizer_read_add_end_5_92:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_92       // SyncAddbranch

label_Synchronizer_read_add_end_4_92:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_92       // SyncAddbranch

label_Synchronizer_read_add_end_3_92:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_92       // SyncAddbranch

label_Synchronizer_read_add_end_2_92:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_92       // SyncAddbranch

label_Synchronizer_read_add_end_1_92:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_92:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[28:31], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_93 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_93 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_93 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_93 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_93 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_93 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_93 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_93:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_93 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_93 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_93 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_93 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_93 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_93 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[192:193]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_93 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_93      // Syncbranchhere

label_Synchronizer_read_add_end_7_93:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_93       // SyncAddbranch

label_Synchronizer_read_add_end_6_93:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_93       // SyncAddbranch

label_Synchronizer_read_add_end_5_93:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_93       // SyncAddbranch

label_Synchronizer_read_add_end_4_93:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_93       // SyncAddbranch

label_Synchronizer_read_add_end_3_93:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_93       // SyncAddbranch

label_Synchronizer_read_add_end_2_93:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_93       // SyncAddbranch

label_Synchronizer_read_add_end_1_93:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_93:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[36:39], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_94 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_94 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_94 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_94 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_94 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_94 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_94 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_94:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_94 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_94 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_94 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_94 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_94 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_94 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[192:193]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_94 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_94      // Syncbranchhere

label_Synchronizer_read_add_end_7_94:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_94       // SyncAddbranch

label_Synchronizer_read_add_end_6_94:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_94       // SyncAddbranch

label_Synchronizer_read_add_end_5_94:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_94       // SyncAddbranch

label_Synchronizer_read_add_end_4_94:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_94       // SyncAddbranch

label_Synchronizer_read_add_end_3_94:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_94       // SyncAddbranch

label_Synchronizer_read_add_end_2_94:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_94       // SyncAddbranch

label_Synchronizer_read_add_end_1_94:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_94:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[44:47], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_95 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_95 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_95 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_95 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_95 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_95 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_95 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_95:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_95 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_95 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_95 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_95 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_95 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[176:177]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_95 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[192:193]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_95 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_95      // Syncbranchhere

label_Synchronizer_read_add_end_7_95:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[176:177]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_95       // SyncAddbranch

label_Synchronizer_read_add_end_6_95:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_95       // SyncAddbranch

label_Synchronizer_read_add_end_5_95:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_95       // SyncAddbranch

label_Synchronizer_read_add_end_4_95:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_95       // SyncAddbranch

label_Synchronizer_read_add_end_3_95:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_95       // SyncAddbranch

label_Synchronizer_read_add_end_2_95:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_95       // SyncAddbranch

label_Synchronizer_read_add_end_1_95:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_95:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_max_f32 v[vgprValuC+20], v[vgprValuC+20], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+21], v[vgprValuC+21], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+22], v[vgprValuC+22], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+23], v[vgprValuC+23], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_max_f32 v[vgprValuC+28], v[vgprValuC+28], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+29], v[vgprValuC+29], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+30], v[vgprValuC+30], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+31], v[vgprValuC+31], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
buffer_store_dwordx2 v[28:29], v11, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[12:13], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[14:15], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_max_f32 v[vgprValuC+36], v[vgprValuC+36], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+37], v[vgprValuC+37], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+38], v[vgprValuC+38], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+39], v[vgprValuC+39], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v27, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[12:13], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[14:15], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_max_f32 v[vgprValuC+44], v[vgprValuC+44], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+45], v[vgprValuC+45], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+46], v[vgprValuC+46], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+47], v[vgprValuC+47], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v35, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Edge_5:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Edge
label_Activation_Sigmoid_Edge:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v42, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s84
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
v_cndmask_b32 v8, v42, v8, s[88:89]                // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v8 offset:0                 // load bias
v_lshlrev_b32 v9, 0x2, v0                          // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v9, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v42, v6, s[88:89]                // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v42, v7, s[88:89]                // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v24, v0, s84
v_lshlrev_b32 v24, 0x2, v24                        // Bias address scaled by BPE
v_cndmask_b32 v24, v42, v24, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v25, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v42, v10, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v11, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v42, v11, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v0, s84
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v42, v32, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v33, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v42, v26, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v27, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v42, v27, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s84
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_cndmask_b32 v40, v42, v40, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v41, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v34, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v42, v34, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v35, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v35, v42, v35, s[88:89]              // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[20:23], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

buffer_store_dwordx4 v[28:31], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

buffer_store_dwordx4 v[36:39], v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

buffer_store_dwordx4 v[44:47], v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s66, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s66, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s66, s66, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s66, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s95, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s94, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s98, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s98, s98, 1                              // Free1
s_mul_hi_u32 s97, s98, s[sgprStrideC1J]            // Free1
s_mul_i32 s96, s98, s[sgprStrideC1J]               // Free1
s_add_u32 s94, s94, s96                            // Free1
s_addc_u32 s95, s95, s97                           // Free1
s_sub_u32 s98, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s98, s98, 1                              // Free2
s_mul_hi_u32 s97, s98, s[sgprStrideCK]             // Free2
s_mul_i32 s96, s98, s[sgprStrideCK]                // Free2
s_add_u32 s94, s94, s96                            // Free2
s_addc_u32 s95, s95, s97                           // Free2
s_lshl_b64 s[90:91], s[94:95], 2                   // scale by bpe

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Edge_6
// check done end

// buffer load start
buffer_load_dwordx4 v[20:23], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_96 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_96 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_96 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_96 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_96 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_96 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_96 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_96:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_96 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_96 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_96 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_96 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_96 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_96 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[192:193]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_96 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_96      // Syncbranchhere

label_Synchronizer_read_add_end_7_96:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_96       // SyncAddbranch

label_Synchronizer_read_add_end_6_96:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_96       // SyncAddbranch

label_Synchronizer_read_add_end_5_96:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_96       // SyncAddbranch

label_Synchronizer_read_add_end_4_96:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_96       // SyncAddbranch

label_Synchronizer_read_add_end_3_96:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_96       // SyncAddbranch

label_Synchronizer_read_add_end_2_96:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_96       // SyncAddbranch

label_Synchronizer_read_add_end_1_96:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_96:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[28:31], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_97 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_97 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_97 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_97 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_97 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_97 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_97 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_97:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_97 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_97 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_97 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_97 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_97 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_97 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[192:193]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_97 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_97      // Syncbranchhere

label_Synchronizer_read_add_end_7_97:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_97       // SyncAddbranch

label_Synchronizer_read_add_end_6_97:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_97       // SyncAddbranch

label_Synchronizer_read_add_end_5_97:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_97       // SyncAddbranch

label_Synchronizer_read_add_end_4_97:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_97       // SyncAddbranch

label_Synchronizer_read_add_end_3_97:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_97       // SyncAddbranch

label_Synchronizer_read_add_end_2_97:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_97       // SyncAddbranch

label_Synchronizer_read_add_end_1_97:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_97:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[36:39], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_98 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_98 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_98 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_98 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_98 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_98 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_98 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_98:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_98 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_98 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_98 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_98 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_98 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_98 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[192:193]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_98 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_98      // Syncbranchhere

label_Synchronizer_read_add_end_7_98:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_98       // SyncAddbranch

label_Synchronizer_read_add_end_6_98:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_98       // SyncAddbranch

label_Synchronizer_read_add_end_5_98:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_98       // SyncAddbranch

label_Synchronizer_read_add_end_4_98:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_98       // SyncAddbranch

label_Synchronizer_read_add_end_3_98:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_98       // SyncAddbranch

label_Synchronizer_read_add_end_2_98:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_98       // SyncAddbranch

label_Synchronizer_read_add_end_1_98:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_98:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[44:47], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_99 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_99 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_99 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_99 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_99 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_99 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_99 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_99:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_99 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_99 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_99 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_99 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_99 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[176:177]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_99 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[192:193]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_99 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_99      // Syncbranchhere

label_Synchronizer_read_add_end_7_99:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[176:177]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_99       // SyncAddbranch

label_Synchronizer_read_add_end_6_99:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_99       // SyncAddbranch

label_Synchronizer_read_add_end_5_99:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_99       // SyncAddbranch

label_Synchronizer_read_add_end_4_99:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_99       // SyncAddbranch

label_Synchronizer_read_add_end_3_99:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_99       // SyncAddbranch

label_Synchronizer_read_add_end_2_99:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_99       // SyncAddbranch

label_Synchronizer_read_add_end_1_99:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_99:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_mul_f32 v[vgprValuC+20], 0xbfb8aa3b, v[vgprValuC+20] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+20], v[vgprValuC+20]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+20], 1.0, v[vgprValuC+20]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+20], v[vgprValuC+20]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+21], 0xbfb8aa3b, v[vgprValuC+21] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+21], v[vgprValuC+21]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+21], 1.0, v[vgprValuC+21]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+21], v[vgprValuC+21]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+22], 0xbfb8aa3b, v[vgprValuC+22] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+22], v[vgprValuC+22]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+22], 1.0, v[vgprValuC+22]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+22], v[vgprValuC+22]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+23], 0xbfb8aa3b, v[vgprValuC+23] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+23], v[vgprValuC+23]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+23], 1.0, v[vgprValuC+23]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+23], v[vgprValuC+23]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_mul_f32 v[vgprValuC+28], 0xbfb8aa3b, v[vgprValuC+28] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+28], v[vgprValuC+28]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+28], 1.0, v[vgprValuC+28]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+28], v[vgprValuC+28]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+29], 0xbfb8aa3b, v[vgprValuC+29] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+29], v[vgprValuC+29]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+29], 1.0, v[vgprValuC+29]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+29], v[vgprValuC+29]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+30], 0xbfb8aa3b, v[vgprValuC+30] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+30], v[vgprValuC+30]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+30], 1.0, v[vgprValuC+30]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+30], v[vgprValuC+30]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+31], 0xbfb8aa3b, v[vgprValuC+31] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+31], v[vgprValuC+31]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+31], 1.0, v[vgprValuC+31]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+31], v[vgprValuC+31]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
buffer_store_dwordx2 v[28:29], v11, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[12:13], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[14:15], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v[vgprValuC+36], 0xbfb8aa3b, v[vgprValuC+36] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+36], v[vgprValuC+36]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+36], 1.0, v[vgprValuC+36]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+36], v[vgprValuC+36]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+37], 0xbfb8aa3b, v[vgprValuC+37] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+37], v[vgprValuC+37]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+37], 1.0, v[vgprValuC+37]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+37], v[vgprValuC+37]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+38], 0xbfb8aa3b, v[vgprValuC+38] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+38], v[vgprValuC+38]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+38], 1.0, v[vgprValuC+38]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+38], v[vgprValuC+38]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+39], 0xbfb8aa3b, v[vgprValuC+39] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+39], v[vgprValuC+39]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+39], 1.0, v[vgprValuC+39]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+39], v[vgprValuC+39]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v27, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[12:13], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[14:15], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_mul_f32 v[vgprValuC+44], 0xbfb8aa3b, v[vgprValuC+44] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+44], v[vgprValuC+44]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+44], 1.0, v[vgprValuC+44]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+44], v[vgprValuC+44]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+45], 0xbfb8aa3b, v[vgprValuC+45] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+45], v[vgprValuC+45]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+45], 1.0, v[vgprValuC+45]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+45], v[vgprValuC+45]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+46], 0xbfb8aa3b, v[vgprValuC+46] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+46], v[vgprValuC+46]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+46], 1.0, v[vgprValuC+46]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+46], v[vgprValuC+46]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+47], 0xbfb8aa3b, v[vgprValuC+47] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+47], v[vgprValuC+47]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+47], 1.0, v[vgprValuC+47]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+47], v[vgprValuC+47]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v35, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Edge_6:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Edge
label_Activation_Tanh_Edge:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v42, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s84
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
v_cndmask_b32 v8, v42, v8, s[88:89]                // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v8 offset:0                 // load bias
v_lshlrev_b32 v9, 0x2, v0                          // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v9, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v42, v6, s[88:89]                // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v42, v7, s[88:89]                // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v24, v0, s84
v_lshlrev_b32 v24, 0x2, v24                        // Bias address scaled by BPE
v_cndmask_b32 v24, v42, v24, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v25, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v42, v10, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v11, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v42, v11, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v0, s84
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v42, v32, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v33, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v42, v26, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v27, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v42, v27, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s84
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_cndmask_b32 v40, v42, v40, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v41, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v34, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v42, v34, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v35, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v35, v42, v35, s[88:89]              // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[20:23], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

buffer_store_dwordx4 v[28:31], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

buffer_store_dwordx4 v[36:39], v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

buffer_store_dwordx4 v[44:47], v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s66, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s66, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s66, s66, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s66, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s95, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s94, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s98, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s98, s98, 1                              // Free1
s_mul_hi_u32 s97, s98, s[sgprStrideC1J]            // Free1
s_mul_i32 s96, s98, s[sgprStrideC1J]               // Free1
s_add_u32 s94, s94, s96                            // Free1
s_addc_u32 s95, s95, s97                           // Free1
s_sub_u32 s98, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s98, s98, 1                              // Free2
s_mul_hi_u32 s97, s98, s[sgprStrideCK]             // Free2
s_mul_i32 s96, s98, s[sgprStrideCK]                // Free2
s_add_u32 s94, s94, s96                            // Free2
s_addc_u32 s95, s95, s97                           // Free2
s_lshl_b64 s[90:91], s[94:95], 2                   // scale by bpe

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Edge_7
// check done end

// buffer load start
buffer_load_dwordx4 v[20:23], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_100 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_100 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_100 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_100 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_100 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_100 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_100 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_100:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_100 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_100 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_100 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_100 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_100 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_100 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[192:193]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_100 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_100     // Syncbranchhere

label_Synchronizer_read_add_end_7_100:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_100      // SyncAddbranch

label_Synchronizer_read_add_end_6_100:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_100      // SyncAddbranch

label_Synchronizer_read_add_end_5_100:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_100      // SyncAddbranch

label_Synchronizer_read_add_end_4_100:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_100      // SyncAddbranch

label_Synchronizer_read_add_end_3_100:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_100      // SyncAddbranch

label_Synchronizer_read_add_end_2_100:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_100      // SyncAddbranch

label_Synchronizer_read_add_end_1_100:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_100:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[28:31], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_101 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_101 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_101 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_101 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_101 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_101 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_101 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_101:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_101 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_101 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_101 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_101 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_101 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_101 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[192:193]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_101 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_101     // Syncbranchhere

label_Synchronizer_read_add_end_7_101:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_101      // SyncAddbranch

label_Synchronizer_read_add_end_6_101:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_101      // SyncAddbranch

label_Synchronizer_read_add_end_5_101:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_101      // SyncAddbranch

label_Synchronizer_read_add_end_4_101:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_101      // SyncAddbranch

label_Synchronizer_read_add_end_3_101:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_101      // SyncAddbranch

label_Synchronizer_read_add_end_2_101:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_101      // SyncAddbranch

label_Synchronizer_read_add_end_1_101:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_101:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[36:39], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_102 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_102 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_102 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_102 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_102 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_102 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_102 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_102:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_102 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_102 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_102 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_102 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_102 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_102 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[192:193]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_102 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_102     // Syncbranchhere

label_Synchronizer_read_add_end_7_102:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_102      // SyncAddbranch

label_Synchronizer_read_add_end_6_102:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_102      // SyncAddbranch

label_Synchronizer_read_add_end_5_102:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_102      // SyncAddbranch

label_Synchronizer_read_add_end_4_102:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_102      // SyncAddbranch

label_Synchronizer_read_add_end_3_102:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_102      // SyncAddbranch

label_Synchronizer_read_add_end_2_102:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_102      // SyncAddbranch

label_Synchronizer_read_add_end_1_102:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_102:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[44:47], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_103 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_103 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_103 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_103 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_103 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_103 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_103 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_103:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_103 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_103 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_103 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_103 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_103 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[176:177]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_103 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[192:193]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_103 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_103     // Syncbranchhere

label_Synchronizer_read_add_end_7_103:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[176:177]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_103      // SyncAddbranch

label_Synchronizer_read_add_end_6_103:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_103      // SyncAddbranch

label_Synchronizer_read_add_end_5_103:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_103      // SyncAddbranch

label_Synchronizer_read_add_end_4_103:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_103      // SyncAddbranch

label_Synchronizer_read_add_end_3_103:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_103      // SyncAddbranch

label_Synchronizer_read_add_end_2_103:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_103      // SyncAddbranch

label_Synchronizer_read_add_end_1_103:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_103:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_mul_f32 v[vgprValuC+20], s[sgpractivationAlpha], v[vgprValuC+20] // x * alpha
v_mul_f32 v[vgprValuC+20], 0x4038aa3b, v[vgprValuC+20] //  (fused 2)
v_exp_f32 v[vgprValuC+20], v[vgprValuC+20]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+20], 1.0, v[vgprValuC+20]    // e^2x + 1
v_rcp_f32 v[vgprValuC+20], v[vgprValuC+20]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+20], -2.0, v[vgprValuC+20], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+20], s[sgpractivationBeta], v[vgprValuC+20] // beta * tanh(x)
v_mul_f32 v[vgprValuC+21], s[sgpractivationAlpha], v[vgprValuC+21] // x * alpha
v_mul_f32 v[vgprValuC+21], 0x4038aa3b, v[vgprValuC+21] //  (fused 2)
v_exp_f32 v[vgprValuC+21], v[vgprValuC+21]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+21], 1.0, v[vgprValuC+21]    // e^2x + 1
v_rcp_f32 v[vgprValuC+21], v[vgprValuC+21]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+21], -2.0, v[vgprValuC+21], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+21], s[sgpractivationBeta], v[vgprValuC+21] // beta * tanh(x)
v_mul_f32 v[vgprValuC+22], s[sgpractivationAlpha], v[vgprValuC+22] // x * alpha
v_mul_f32 v[vgprValuC+22], 0x4038aa3b, v[vgprValuC+22] //  (fused 2)
v_exp_f32 v[vgprValuC+22], v[vgprValuC+22]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+22], 1.0, v[vgprValuC+22]    // e^2x + 1
v_rcp_f32 v[vgprValuC+22], v[vgprValuC+22]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+22], -2.0, v[vgprValuC+22], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+22], s[sgpractivationBeta], v[vgprValuC+22] // beta * tanh(x)
v_mul_f32 v[vgprValuC+23], s[sgpractivationAlpha], v[vgprValuC+23] // x * alpha
v_mul_f32 v[vgprValuC+23], 0x4038aa3b, v[vgprValuC+23] //  (fused 2)
v_exp_f32 v[vgprValuC+23], v[vgprValuC+23]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+23], 1.0, v[vgprValuC+23]    // e^2x + 1
v_rcp_f32 v[vgprValuC+23], v[vgprValuC+23]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+23], -2.0, v[vgprValuC+23], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+23], s[sgpractivationBeta], v[vgprValuC+23] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_mul_f32 v[vgprValuC+28], s[sgpractivationAlpha], v[vgprValuC+28] // x * alpha
v_mul_f32 v[vgprValuC+28], 0x4038aa3b, v[vgprValuC+28] //  (fused 2)
v_exp_f32 v[vgprValuC+28], v[vgprValuC+28]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+28], 1.0, v[vgprValuC+28]    // e^2x + 1
v_rcp_f32 v[vgprValuC+28], v[vgprValuC+28]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+28], -2.0, v[vgprValuC+28], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+28], s[sgpractivationBeta], v[vgprValuC+28] // beta * tanh(x)
v_mul_f32 v[vgprValuC+29], s[sgpractivationAlpha], v[vgprValuC+29] // x * alpha
v_mul_f32 v[vgprValuC+29], 0x4038aa3b, v[vgprValuC+29] //  (fused 2)
v_exp_f32 v[vgprValuC+29], v[vgprValuC+29]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+29], 1.0, v[vgprValuC+29]    // e^2x + 1
v_rcp_f32 v[vgprValuC+29], v[vgprValuC+29]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+29], -2.0, v[vgprValuC+29], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+29], s[sgpractivationBeta], v[vgprValuC+29] // beta * tanh(x)
v_mul_f32 v[vgprValuC+30], s[sgpractivationAlpha], v[vgprValuC+30] // x * alpha
v_mul_f32 v[vgprValuC+30], 0x4038aa3b, v[vgprValuC+30] //  (fused 2)
v_exp_f32 v[vgprValuC+30], v[vgprValuC+30]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+30], 1.0, v[vgprValuC+30]    // e^2x + 1
v_rcp_f32 v[vgprValuC+30], v[vgprValuC+30]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+30], -2.0, v[vgprValuC+30], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+30], s[sgpractivationBeta], v[vgprValuC+30] // beta * tanh(x)
v_mul_f32 v[vgprValuC+31], s[sgpractivationAlpha], v[vgprValuC+31] // x * alpha
v_mul_f32 v[vgprValuC+31], 0x4038aa3b, v[vgprValuC+31] //  (fused 2)
v_exp_f32 v[vgprValuC+31], v[vgprValuC+31]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+31], 1.0, v[vgprValuC+31]    // e^2x + 1
v_rcp_f32 v[vgprValuC+31], v[vgprValuC+31]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+31], -2.0, v[vgprValuC+31], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+31], s[sgpractivationBeta], v[vgprValuC+31] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
buffer_store_dwordx2 v[28:29], v11, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[12:13], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[14:15], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v[vgprValuC+36], s[sgpractivationAlpha], v[vgprValuC+36] // x * alpha
v_mul_f32 v[vgprValuC+36], 0x4038aa3b, v[vgprValuC+36] //  (fused 2)
v_exp_f32 v[vgprValuC+36], v[vgprValuC+36]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+36], 1.0, v[vgprValuC+36]    // e^2x + 1
v_rcp_f32 v[vgprValuC+36], v[vgprValuC+36]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+36], -2.0, v[vgprValuC+36], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+36], s[sgpractivationBeta], v[vgprValuC+36] // beta * tanh(x)
v_mul_f32 v[vgprValuC+37], s[sgpractivationAlpha], v[vgprValuC+37] // x * alpha
v_mul_f32 v[vgprValuC+37], 0x4038aa3b, v[vgprValuC+37] //  (fused 2)
v_exp_f32 v[vgprValuC+37], v[vgprValuC+37]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+37], 1.0, v[vgprValuC+37]    // e^2x + 1
v_rcp_f32 v[vgprValuC+37], v[vgprValuC+37]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+37], -2.0, v[vgprValuC+37], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+37], s[sgpractivationBeta], v[vgprValuC+37] // beta * tanh(x)
v_mul_f32 v[vgprValuC+38], s[sgpractivationAlpha], v[vgprValuC+38] // x * alpha
v_mul_f32 v[vgprValuC+38], 0x4038aa3b, v[vgprValuC+38] //  (fused 2)
v_exp_f32 v[vgprValuC+38], v[vgprValuC+38]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+38], 1.0, v[vgprValuC+38]    // e^2x + 1
v_rcp_f32 v[vgprValuC+38], v[vgprValuC+38]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+38], -2.0, v[vgprValuC+38], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+38], s[sgpractivationBeta], v[vgprValuC+38] // beta * tanh(x)
v_mul_f32 v[vgprValuC+39], s[sgpractivationAlpha], v[vgprValuC+39] // x * alpha
v_mul_f32 v[vgprValuC+39], 0x4038aa3b, v[vgprValuC+39] //  (fused 2)
v_exp_f32 v[vgprValuC+39], v[vgprValuC+39]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+39], 1.0, v[vgprValuC+39]    // e^2x + 1
v_rcp_f32 v[vgprValuC+39], v[vgprValuC+39]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+39], -2.0, v[vgprValuC+39], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+39], s[sgpractivationBeta], v[vgprValuC+39] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v27, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[12:13], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[14:15], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_mul_f32 v[vgprValuC+44], s[sgpractivationAlpha], v[vgprValuC+44] // x * alpha
v_mul_f32 v[vgprValuC+44], 0x4038aa3b, v[vgprValuC+44] //  (fused 2)
v_exp_f32 v[vgprValuC+44], v[vgprValuC+44]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+44], 1.0, v[vgprValuC+44]    // e^2x + 1
v_rcp_f32 v[vgprValuC+44], v[vgprValuC+44]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+44], -2.0, v[vgprValuC+44], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+44], s[sgpractivationBeta], v[vgprValuC+44] // beta * tanh(x)
v_mul_f32 v[vgprValuC+45], s[sgpractivationAlpha], v[vgprValuC+45] // x * alpha
v_mul_f32 v[vgprValuC+45], 0x4038aa3b, v[vgprValuC+45] //  (fused 2)
v_exp_f32 v[vgprValuC+45], v[vgprValuC+45]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+45], 1.0, v[vgprValuC+45]    // e^2x + 1
v_rcp_f32 v[vgprValuC+45], v[vgprValuC+45]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+45], -2.0, v[vgprValuC+45], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+45], s[sgpractivationBeta], v[vgprValuC+45] // beta * tanh(x)
v_mul_f32 v[vgprValuC+46], s[sgpractivationAlpha], v[vgprValuC+46] // x * alpha
v_mul_f32 v[vgprValuC+46], 0x4038aa3b, v[vgprValuC+46] //  (fused 2)
v_exp_f32 v[vgprValuC+46], v[vgprValuC+46]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+46], 1.0, v[vgprValuC+46]    // e^2x + 1
v_rcp_f32 v[vgprValuC+46], v[vgprValuC+46]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+46], -2.0, v[vgprValuC+46], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+46], s[sgpractivationBeta], v[vgprValuC+46] // beta * tanh(x)
v_mul_f32 v[vgprValuC+47], s[sgpractivationAlpha], v[vgprValuC+47] // x * alpha
v_mul_f32 v[vgprValuC+47], 0x4038aa3b, v[vgprValuC+47] //  (fused 2)
v_exp_f32 v[vgprValuC+47], v[vgprValuC+47]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+47], 1.0, v[vgprValuC+47]    // e^2x + 1
v_rcp_f32 v[vgprValuC+47], v[vgprValuC+47]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+47], -2.0, v[vgprValuC+47], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+47], s[sgpractivationBeta], v[vgprValuC+47] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v35, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Edge_7:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Edge
label_Activation_Geluscaling_Edge:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v42, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s84
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
v_cndmask_b32 v8, v42, v8, s[88:89]                // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v8 offset:0                 // load bias
v_lshlrev_b32 v9, 0x2, v0                          // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v9, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v42, v6, s[88:89]                // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v42, v7, s[88:89]                // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v24, v0, s84
v_lshlrev_b32 v24, 0x2, v24                        // Bias address scaled by BPE
v_cndmask_b32 v24, v42, v24, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v25, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v42, v10, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v11, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v42, v11, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v0, s84
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v42, v32, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v33, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v42, v26, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v27, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v42, v27, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s84
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_cndmask_b32 v40, v42, v40, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v41, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v34, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v42, v34, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v35, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v35, v42, v35, s[88:89]              // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[20:23], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

buffer_store_dwordx4 v[28:31], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

buffer_store_dwordx4 v[36:39], v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

buffer_store_dwordx4 v[44:47], v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s66, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s66, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s66, s66, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s66, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s95, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s94, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s98, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s98, s98, 1                              // Free1
s_mul_hi_u32 s97, s98, s[sgprStrideC1J]            // Free1
s_mul_i32 s96, s98, s[sgprStrideC1J]               // Free1
s_add_u32 s94, s94, s96                            // Free1
s_addc_u32 s95, s95, s97                           // Free1
s_sub_u32 s98, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s98, s98, 1                              // Free2
s_mul_hi_u32 s97, s98, s[sgprStrideCK]             // Free2
s_mul_i32 s96, s98, s[sgprStrideCK]                // Free2
s_add_u32 s94, s94, s96                            // Free2
s_addc_u32 s95, s95, s97                           // Free2
s_lshl_b64 s[90:91], s[94:95], 2                   // scale by bpe

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Edge_8
// check done end

// buffer load start
buffer_load_dwordx4 v[20:23], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_104 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_104 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_104 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_104 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_104 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_104 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_104 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_104:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_104 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_104 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_104 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_104 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_104 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_104 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[192:193]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_104 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v6, v42, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_104     // Syncbranchhere

label_Synchronizer_read_add_end_7_104:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[176:177]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_104      // SyncAddbranch

label_Synchronizer_read_add_end_6_104:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[160:161]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_104      // SyncAddbranch

label_Synchronizer_read_add_end_5_104:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[144:145]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_104      // SyncAddbranch

label_Synchronizer_read_add_end_4_104:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[128:129]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_104      // SyncAddbranch

label_Synchronizer_read_add_end_3_104:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[112:113]        // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_104      // SyncAddbranch

label_Synchronizer_read_add_end_2_104:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[20:21], v[20:21], v[96:97]          // buffer pk
v_pk_add_f32 v[22:23], v[22:23], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_104      // SyncAddbranch

label_Synchronizer_read_add_end_1_104:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_104:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[28:31], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_105 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_105 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_105 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_105 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_105 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_105 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_105 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_105:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_105 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_105 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_105 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_105 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_105 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_105 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[192:193]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_105 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v10, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_105     // Syncbranchhere

label_Synchronizer_read_add_end_7_105:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[176:177]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_105      // SyncAddbranch

label_Synchronizer_read_add_end_6_105:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[160:161]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_105      // SyncAddbranch

label_Synchronizer_read_add_end_5_105:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[144:145]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_105      // SyncAddbranch

label_Synchronizer_read_add_end_4_105:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[128:129]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_105      // SyncAddbranch

label_Synchronizer_read_add_end_3_105:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[112:113]        // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_105      // SyncAddbranch

label_Synchronizer_read_add_end_2_105:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[28:29], v[28:29], v[96:97]          // buffer pk
v_pk_add_f32 v[30:31], v[30:31], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_105      // SyncAddbranch

label_Synchronizer_read_add_end_1_105:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_105:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[36:39], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_106 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_106 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_106 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_106 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_106 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_106 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_106 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v26, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_106:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_106 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_106 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_106 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_106 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_106 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_106 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[192:193]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_106 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v26, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_106     // Syncbranchhere

label_Synchronizer_read_add_end_7_106:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_106      // SyncAddbranch

label_Synchronizer_read_add_end_6_106:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_106      // SyncAddbranch

label_Synchronizer_read_add_end_5_106:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_106      // SyncAddbranch

label_Synchronizer_read_add_end_4_106:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_106      // SyncAddbranch

label_Synchronizer_read_add_end_3_106:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_106      // SyncAddbranch

label_Synchronizer_read_add_end_2_106:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_106      // SyncAddbranch

label_Synchronizer_read_add_end_1_106:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_106:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v42, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[44:47], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_107 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_107 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_107 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_107 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_107 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_107 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_107 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v34, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_107:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_107 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_107 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_107 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_107 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_107 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[176:177]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_107 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[192:193]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_107 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v43, v34, v42, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v43, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_107     // Syncbranchhere

label_Synchronizer_read_add_end_7_107:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[176:177]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_107      // SyncAddbranch

label_Synchronizer_read_add_end_6_107:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[160:161]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_107      // SyncAddbranch

label_Synchronizer_read_add_end_5_107:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[144:145]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_107      // SyncAddbranch

label_Synchronizer_read_add_end_4_107:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[128:129]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_107      // SyncAddbranch

label_Synchronizer_read_add_end_3_107:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[112:113]        // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_107      // SyncAddbranch

label_Synchronizer_read_add_end_2_107:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[44:45], v[44:45], v[96:97]          // buffer pk
v_pk_add_f32 v[46:47], v[46:47], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_107      // SyncAddbranch

label_Synchronizer_read_add_end_1_107:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_107:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+20]          // k1 * x
v_fma_f32 v4, v[vgprValuC+20], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+20], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+20], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+20], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+21]          // k1 * x
v_fma_f32 v4, v[vgprValuC+21], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+21], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+21], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+21], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+22]          // k1 * x
v_fma_f32 v4, v[vgprValuC+22], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+22], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+22], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+22], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+23]          // k1 * x
v_fma_f32 v4, v[vgprValuC+23], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+23], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+23], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+23], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+28]          // k1 * x
v_fma_f32 v4, v[vgprValuC+28], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+28], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+28], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+28], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+29]          // k1 * x
v_fma_f32 v4, v[vgprValuC+29], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+29], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+29], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+29], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+30]          // k1 * x
v_fma_f32 v4, v[vgprValuC+30], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+30], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+30], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+30], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+31]          // k1 * x
v_fma_f32 v4, v[vgprValuC+31], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+31], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+31], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+31], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
buffer_store_dwordx2 v[28:29], v11, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[12:13], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[14:15], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+36]          // k1 * x
v_fma_f32 v4, v[vgprValuC+36], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+36], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+36], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+36], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+37]          // k1 * x
v_fma_f32 v4, v[vgprValuC+37], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+37], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+37], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+37], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+38]          // k1 * x
v_fma_f32 v4, v[vgprValuC+38], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+38], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+38], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+38], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+39]          // k1 * x
v_fma_f32 v4, v[vgprValuC+39], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+39], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+39], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+39], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v27, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[12:13], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[14:15], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+44]          // k1 * x
v_fma_f32 v4, v[vgprValuC+44], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+44], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+44], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+44], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+45]          // k1 * x
v_fma_f32 v4, v[vgprValuC+45], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+45], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+45], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+45], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+46]          // k1 * x
v_fma_f32 v4, v[vgprValuC+46], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+46], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+46], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+46], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+47]          // k1 * x
v_fma_f32 v4, v[vgprValuC+47], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+47], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+47], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+47], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v35, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Edge_8:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
label_Activation_End_Edge:
// jump to end
s_getpc_b64 s[84:85]                               // addr of next instr
s_add_i32 s86, label_GW_End, 0x4                   // target branch offset
s_add_u32 s84, s84, s86                            // add target branch offset
s_addc_u32 s85, s85, 0                             // add high and carry
s_setpc_b64 s[84:85]                               // branch to label_GW_End
label_GW_Beta:
s_and_b32 s84, 255, s[sgprSizeI]                   // s84 = s[sgprSizeI] % 256
s_add_u32 s85, -0x1, s[sgprNumWorkGroups0]
s_cmp_ge_u32 s[sgprWorkGroup0], s85                // wg0 >= nwg0-1 ?
s_cselect_b32 s84, s84, 0                          // set rMT0
s_cmpk_gt_u32 s84, 0x0                             // rMT0 > 0
s_cbranch_scc0 label_NoBranch_O0ST4IVCWLX9JVLJ_0   // Only branch on scc1
// jump if edges required
s_getpc_b64 s[84:85]                               // addr of next instr
s_add_i32 s86, label_GW_B1_E1, 0x4                 // target branch offset
s_add_u32 s84, s84, s86                            // add target branch offset
s_addc_u32 s85, s85, 0                             // add high and carry
s_setpc_b64 s[84:85]                               // branch to label_GW_B1_E1
label_NoBranch_O0ST4IVCWLX9JVLJ_0:
s_and_b32 s84, 15, s[sgprSizeJ]                    // s84 = s[sgprSizeJ] % 16
s_add_u32 s85, -0x1, s[sgprNumWorkGroups1]
s_cmp_ge_u32 s[sgprWorkGroup1], s85                // wg1 >= nwg1-1
s_cselect_b32 s84, s84, 0                          // set rMT1
s_cmpk_gt_u32 s84, 0x0                             // rMT1 > 0
s_cbranch_scc0 label_NoBranch_NLZZV71G7M0ZLLOF_0   // Only branch on scc1
// jump if edges required
s_getpc_b64 s[84:85]                               // addr of next instr
s_add_i32 s86, label_GW_B1_E1, 0x4                 // target branch offset
s_add_u32 s84, s84, s86                            // add target branch offset
s_addc_u32 s85, s85, 0                             // add high and carry
s_setpc_b64 s[84:85]                               // branch to label_GW_B1_E1
label_NoBranch_NLZZV71G7M0ZLLOF_0:
label_GW_B1_E0:

/* edge=0, allocate 2 sgpr. perBatchTmpS=2 perBatchMaskS=0 perElementMaskS=0 elementsPerBatch=14 */
s_cmpk_eq_u32 s[sgprActivationType], 0             // activationType == 0
s_cbranch_scc1 label_Activation_None_Beta          // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 1             // activationType == 1
s_cbranch_scc1 label_Activation_Abs_Beta           // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 2             // activationType == 2
s_cbranch_scc1 label_Activation_Clippedrelu_Beta   // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 3             // activationType == 3
s_cbranch_scc1 label_Activation_Gelu_Beta          // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 4             // activationType == 4
s_cbranch_scc1 label_Activation_Leakyrelu_Beta     // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 5             // activationType == 5
s_cbranch_scc1 label_Activation_Relu_Beta          // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 6             // activationType == 6
s_cbranch_scc1 label_Activation_Sigmoid_Beta       // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 7             // activationType == 7
s_cbranch_scc1 label_Activation_Tanh_Beta          // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 9             // activationType == 9
s_cbranch_scc1 label_Activation_Geluscaling_Beta   // Branch if true
label_Activation_None_Beta:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Beta Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_add_lshl_u32 v8, v2, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
buffer_load_dwordx2 v[12:13], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v9, v0, s66
v_lshlrev_b32 v9, 0x2, v9                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v9 offset:0                 // load bias
v_lshlrev_b32 v10, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v10, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[28:29], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[36:37], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[44:45], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
v_add_lshl_u32 v6, v3, v0, 0x2                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_add_lshl_u32 v7, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+40], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+41], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+42], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+43], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+48], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+49], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+50], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+51], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[32:35], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[40:43], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[48:51], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s84, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s84, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s84, s84, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s84, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s91, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s90, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s94, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s94, s94, 1                              // Free1
s_mul_hi_u32 s93, s94, s[sgprStrideC1J]            // Free1
s_mul_i32 s92, s94, s[sgprStrideC1J]               // Free1
s_add_u32 s90, s90, s92                            // Free1
s_addc_u32 s91, s91, s93                           // Free1
s_sub_u32 s94, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s94, s94, 1                              // Free2
s_mul_hi_u32 s93, s94, s[sgprStrideCK]             // Free2
s_mul_i32 s92, s94, s[sgprStrideCK]                // Free2
s_add_u32 s90, s90, s92                            // Free2
s_addc_u32 s91, s91, s93                           // Free2
s_lshl_b64 s[86:87], s[90:91], 2                   // scale by bpe

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Beta_0
// check done end

// buffer load start
buffer_load_dwordx4 v[24:27], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_36 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_36 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_36 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_36 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_36 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_36 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_36 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_36:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_36 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_36 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_36 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_36 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_36 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_36 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_36 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_36      // Syncbranchhere

label_Synchronizer_read_add_end_7_36:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_36       // SyncAddbranch

label_Synchronizer_read_add_end_6_36:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_36       // SyncAddbranch

label_Synchronizer_read_add_end_5_36:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_36       // SyncAddbranch

label_Synchronizer_read_add_end_4_36:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_36       // SyncAddbranch

label_Synchronizer_read_add_end_3_36:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_36       // SyncAddbranch

label_Synchronizer_read_add_end_2_36:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_36       // SyncAddbranch

label_Synchronizer_read_add_end_1_36:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_36:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[32:35], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_37 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_37 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_37 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_37 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_37 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_37 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_37 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_37:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_37 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_37 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_37 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_37 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_37 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_37 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[192:193]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_37 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_37      // Syncbranchhere

label_Synchronizer_read_add_end_7_37:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_37       // SyncAddbranch

label_Synchronizer_read_add_end_6_37:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_37       // SyncAddbranch

label_Synchronizer_read_add_end_5_37:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_37       // SyncAddbranch

label_Synchronizer_read_add_end_4_37:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_37       // SyncAddbranch

label_Synchronizer_read_add_end_3_37:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_37       // SyncAddbranch

label_Synchronizer_read_add_end_2_37:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_37       // SyncAddbranch

label_Synchronizer_read_add_end_1_37:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_37:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[40:43], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_38 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_38 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_38 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_38 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_38 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_38 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_38 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_38:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_38 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_38 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_38 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_38 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_38 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[176:177]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_38 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[192:193]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_38 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_38      // Syncbranchhere

label_Synchronizer_read_add_end_7_38:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[176:177]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_38       // SyncAddbranch

label_Synchronizer_read_add_end_6_38:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_38       // SyncAddbranch

label_Synchronizer_read_add_end_5_38:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_38       // SyncAddbranch

label_Synchronizer_read_add_end_4_38:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_38       // SyncAddbranch

label_Synchronizer_read_add_end_3_38:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_38       // SyncAddbranch

label_Synchronizer_read_add_end_2_38:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_38       // SyncAddbranch

label_Synchronizer_read_add_end_1_38:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_38:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[48:51], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_39 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_39 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_39 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_39 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_39 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_39 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_39 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_39:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_39 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_39 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_39 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_39 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_39 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_39 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[192:193]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_39 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_39      // Syncbranchhere

label_Synchronizer_read_add_end_7_39:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_39       // SyncAddbranch

label_Synchronizer_read_add_end_6_39:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_39       // SyncAddbranch

label_Synchronizer_read_add_end_5_39:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_39       // SyncAddbranch

label_Synchronizer_read_add_end_4_39:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_39       // SyncAddbranch

label_Synchronizer_read_add_end_3_39:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_39       // SyncAddbranch

label_Synchronizer_read_add_end_2_39:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_39       // SyncAddbranch

label_Synchronizer_read_add_end_1_39:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_39:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha

/* apply mask, calc new C and issue writes */

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v28, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v28, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v29, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v29, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v36, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+41], s[sgprBeta], v36, v[vgprValuC+41] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+42], s[sgprBeta], v37, v[vgprValuC+42] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+43], s[sgprBeta], v37, v[vgprValuC+43] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[40:41], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v44, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v44, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v45, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v45, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[48:49], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Beta_0:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta
label_Activation_Abs_Beta:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Beta Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_add_lshl_u32 v8, v2, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
buffer_load_dwordx2 v[12:13], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v9, v0, s66
v_lshlrev_b32 v9, 0x2, v9                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v9 offset:0                 // load bias
v_lshlrev_b32 v10, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v10, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[28:29], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[36:37], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[44:45], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
v_add_lshl_u32 v6, v3, v0, 0x2                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_add_lshl_u32 v7, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+40], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+41], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+42], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+43], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+48], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+49], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+50], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+51], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[32:35], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[40:43], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[48:51], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s84, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s84, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s84, s84, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s84, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s91, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s90, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s94, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s94, s94, 1                              // Free1
s_mul_hi_u32 s93, s94, s[sgprStrideC1J]            // Free1
s_mul_i32 s92, s94, s[sgprStrideC1J]               // Free1
s_add_u32 s90, s90, s92                            // Free1
s_addc_u32 s91, s91, s93                           // Free1
s_sub_u32 s94, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s94, s94, 1                              // Free2
s_mul_hi_u32 s93, s94, s[sgprStrideCK]             // Free2
s_mul_i32 s92, s94, s[sgprStrideCK]                // Free2
s_add_u32 s90, s90, s92                            // Free2
s_addc_u32 s91, s91, s93                           // Free2
s_lshl_b64 s[86:87], s[90:91], 2                   // scale by bpe

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Beta_1
// check done end

// buffer load start
buffer_load_dwordx4 v[24:27], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_40 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_40 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_40 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_40 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_40 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_40 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_40 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_40:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_40 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_40 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_40 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_40 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_40 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_40 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_40 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_40      // Syncbranchhere

label_Synchronizer_read_add_end_7_40:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_40       // SyncAddbranch

label_Synchronizer_read_add_end_6_40:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_40       // SyncAddbranch

label_Synchronizer_read_add_end_5_40:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_40       // SyncAddbranch

label_Synchronizer_read_add_end_4_40:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_40       // SyncAddbranch

label_Synchronizer_read_add_end_3_40:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_40       // SyncAddbranch

label_Synchronizer_read_add_end_2_40:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_40       // SyncAddbranch

label_Synchronizer_read_add_end_1_40:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_40:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[32:35], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_41 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_41 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_41 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_41 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_41 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_41 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_41 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_41:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_41 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_41 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_41 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_41 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_41 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_41 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[192:193]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_41 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_41      // Syncbranchhere

label_Synchronizer_read_add_end_7_41:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_41       // SyncAddbranch

label_Synchronizer_read_add_end_6_41:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_41       // SyncAddbranch

label_Synchronizer_read_add_end_5_41:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_41       // SyncAddbranch

label_Synchronizer_read_add_end_4_41:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_41       // SyncAddbranch

label_Synchronizer_read_add_end_3_41:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_41       // SyncAddbranch

label_Synchronizer_read_add_end_2_41:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_41       // SyncAddbranch

label_Synchronizer_read_add_end_1_41:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_41:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[40:43], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_42 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_42 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_42 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_42 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_42 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_42 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_42 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_42:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_42 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_42 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_42 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_42 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_42 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[176:177]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_42 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[192:193]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_42 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_42      // Syncbranchhere

label_Synchronizer_read_add_end_7_42:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[176:177]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_42       // SyncAddbranch

label_Synchronizer_read_add_end_6_42:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_42       // SyncAddbranch

label_Synchronizer_read_add_end_5_42:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_42       // SyncAddbranch

label_Synchronizer_read_add_end_4_42:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_42       // SyncAddbranch

label_Synchronizer_read_add_end_3_42:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_42       // SyncAddbranch

label_Synchronizer_read_add_end_2_42:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_42       // SyncAddbranch

label_Synchronizer_read_add_end_1_42:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_42:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[48:51], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_43 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_43 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_43 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_43 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_43 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_43 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_43 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_43:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_43 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_43 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_43 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_43 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_43 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_43 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[192:193]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_43 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_43      // Syncbranchhere

label_Synchronizer_read_add_end_7_43:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_43       // SyncAddbranch

label_Synchronizer_read_add_end_6_43:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_43       // SyncAddbranch

label_Synchronizer_read_add_end_5_43:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_43       // SyncAddbranch

label_Synchronizer_read_add_end_4_43:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_43       // SyncAddbranch

label_Synchronizer_read_add_end_3_43:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_43       // SyncAddbranch

label_Synchronizer_read_add_end_2_43:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_43       // SyncAddbranch

label_Synchronizer_read_add_end_1_43:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_43:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha

/* apply mask, calc new C and issue writes */

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_and_b32 v[vgprValuC+24], 0x7fffffff, v[vgprValuC+24] // Remove sign bit
v_and_b32 v[vgprValuC+25], 0x7fffffff, v[vgprValuC+25] // Remove sign bit
v_and_b32 v[vgprValuC+26], 0x7fffffff, v[vgprValuC+26] // Remove sign bit
v_and_b32 v[vgprValuC+27], 0x7fffffff, v[vgprValuC+27] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v28, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v28, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v29, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v29, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_and_b32 v[vgprValuC+32], 0x7fffffff, v[vgprValuC+32] // Remove sign bit
v_and_b32 v[vgprValuC+33], 0x7fffffff, v[vgprValuC+33] // Remove sign bit
v_and_b32 v[vgprValuC+34], 0x7fffffff, v[vgprValuC+34] // Remove sign bit
v_and_b32 v[vgprValuC+35], 0x7fffffff, v[vgprValuC+35] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v36, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+41], s[sgprBeta], v36, v[vgprValuC+41] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+42], s[sgprBeta], v37, v[vgprValuC+42] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+43], s[sgprBeta], v37, v[vgprValuC+43] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_and_b32 v[vgprValuC+40], 0x7fffffff, v[vgprValuC+40] // Remove sign bit
v_and_b32 v[vgprValuC+41], 0x7fffffff, v[vgprValuC+41] // Remove sign bit
v_and_b32 v[vgprValuC+42], 0x7fffffff, v[vgprValuC+42] // Remove sign bit
v_and_b32 v[vgprValuC+43], 0x7fffffff, v[vgprValuC+43] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[40:41], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v44, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v44, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v45, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v45, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_and_b32 v[vgprValuC+48], 0x7fffffff, v[vgprValuC+48] // Remove sign bit
v_and_b32 v[vgprValuC+49], 0x7fffffff, v[vgprValuC+49] // Remove sign bit
v_and_b32 v[vgprValuC+50], 0x7fffffff, v[vgprValuC+50] // Remove sign bit
v_and_b32 v[vgprValuC+51], 0x7fffffff, v[vgprValuC+51] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[48:49], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Beta_1:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta
label_Activation_Clippedrelu_Beta:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Beta Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_add_lshl_u32 v8, v2, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
buffer_load_dwordx2 v[12:13], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v9, v0, s66
v_lshlrev_b32 v9, 0x2, v9                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v9 offset:0                 // load bias
v_lshlrev_b32 v10, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v10, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[28:29], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[36:37], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[44:45], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
v_add_lshl_u32 v6, v3, v0, 0x2                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_add_lshl_u32 v7, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+40], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+41], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+42], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+43], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+48], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+49], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+50], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+51], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[32:35], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[40:43], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[48:51], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s84, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s84, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s84, s84, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s84, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s91, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s90, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s94, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s94, s94, 1                              // Free1
s_mul_hi_u32 s93, s94, s[sgprStrideC1J]            // Free1
s_mul_i32 s92, s94, s[sgprStrideC1J]               // Free1
s_add_u32 s90, s90, s92                            // Free1
s_addc_u32 s91, s91, s93                           // Free1
s_sub_u32 s94, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s94, s94, 1                              // Free2
s_mul_hi_u32 s93, s94, s[sgprStrideCK]             // Free2
s_mul_i32 s92, s94, s[sgprStrideCK]                // Free2
s_add_u32 s90, s90, s92                            // Free2
s_addc_u32 s91, s91, s93                           // Free2
s_lshl_b64 s[86:87], s[90:91], 2                   // scale by bpe

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Beta_2
// check done end

// buffer load start
buffer_load_dwordx4 v[24:27], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_44 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_44 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_44 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_44 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_44 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_44 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_44 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_44:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_44 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_44 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_44 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_44 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_44 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_44 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_44 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_44      // Syncbranchhere

label_Synchronizer_read_add_end_7_44:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_44       // SyncAddbranch

label_Synchronizer_read_add_end_6_44:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_44       // SyncAddbranch

label_Synchronizer_read_add_end_5_44:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_44       // SyncAddbranch

label_Synchronizer_read_add_end_4_44:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_44       // SyncAddbranch

label_Synchronizer_read_add_end_3_44:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_44       // SyncAddbranch

label_Synchronizer_read_add_end_2_44:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_44       // SyncAddbranch

label_Synchronizer_read_add_end_1_44:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_44:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[32:35], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_45 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_45 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_45 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_45 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_45 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_45 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_45 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_45:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_45 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_45 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_45 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_45 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_45 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_45 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[192:193]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_45 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_45      // Syncbranchhere

label_Synchronizer_read_add_end_7_45:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_45       // SyncAddbranch

label_Synchronizer_read_add_end_6_45:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_45       // SyncAddbranch

label_Synchronizer_read_add_end_5_45:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_45       // SyncAddbranch

label_Synchronizer_read_add_end_4_45:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_45       // SyncAddbranch

label_Synchronizer_read_add_end_3_45:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_45       // SyncAddbranch

label_Synchronizer_read_add_end_2_45:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_45       // SyncAddbranch

label_Synchronizer_read_add_end_1_45:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_45:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[40:43], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_46 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_46 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_46 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_46 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_46 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_46 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_46 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_46:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_46 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_46 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_46 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_46 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_46 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[176:177]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_46 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[192:193]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_46 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_46      // Syncbranchhere

label_Synchronizer_read_add_end_7_46:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[176:177]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_46       // SyncAddbranch

label_Synchronizer_read_add_end_6_46:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_46       // SyncAddbranch

label_Synchronizer_read_add_end_5_46:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_46       // SyncAddbranch

label_Synchronizer_read_add_end_4_46:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_46       // SyncAddbranch

label_Synchronizer_read_add_end_3_46:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_46       // SyncAddbranch

label_Synchronizer_read_add_end_2_46:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_46       // SyncAddbranch

label_Synchronizer_read_add_end_1_46:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_46:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[48:51], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_47 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_47 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_47 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_47 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_47 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_47 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_47 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_47:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_47 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_47 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_47 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_47 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_47 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_47 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[192:193]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_47 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_47      // Syncbranchhere

label_Synchronizer_read_add_end_7_47:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_47       // SyncAddbranch

label_Synchronizer_read_add_end_6_47:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_47       // SyncAddbranch

label_Synchronizer_read_add_end_5_47:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_47       // SyncAddbranch

label_Synchronizer_read_add_end_4_47:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_47       // SyncAddbranch

label_Synchronizer_read_add_end_3_47:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_47       // SyncAddbranch

label_Synchronizer_read_add_end_2_47:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_47       // SyncAddbranch

label_Synchronizer_read_add_end_1_47:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_47:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha

/* apply mask, calc new C and issue writes */

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+24], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+24], s[sgpractivationBeta], v[vgprValuC+24] // min(x, beta)
v_cndmask_b32 v[vgprValuC+24], 0.0, v[vgprValuC+24], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+25], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+25], s[sgpractivationBeta], v[vgprValuC+25] // min(x, beta)
v_cndmask_b32 v[vgprValuC+25], 0.0, v[vgprValuC+25], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+26], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+26], s[sgpractivationBeta], v[vgprValuC+26] // min(x, beta)
v_cndmask_b32 v[vgprValuC+26], 0.0, v[vgprValuC+26], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+27], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+27], s[sgpractivationBeta], v[vgprValuC+27] // min(x, beta)
v_cndmask_b32 v[vgprValuC+27], 0.0, v[vgprValuC+27], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v28, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v28, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v29, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v29, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+32], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+32], s[sgpractivationBeta], v[vgprValuC+32] // min(x, beta)
v_cndmask_b32 v[vgprValuC+32], 0.0, v[vgprValuC+32], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+33], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+33], s[sgpractivationBeta], v[vgprValuC+33] // min(x, beta)
v_cndmask_b32 v[vgprValuC+33], 0.0, v[vgprValuC+33], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+34], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+34], s[sgpractivationBeta], v[vgprValuC+34] // min(x, beta)
v_cndmask_b32 v[vgprValuC+34], 0.0, v[vgprValuC+34], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+35], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+35], s[sgpractivationBeta], v[vgprValuC+35] // min(x, beta)
v_cndmask_b32 v[vgprValuC+35], 0.0, v[vgprValuC+35], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v36, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+41], s[sgprBeta], v36, v[vgprValuC+41] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+42], s[sgprBeta], v37, v[vgprValuC+42] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+43], s[sgprBeta], v37, v[vgprValuC+43] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+40], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+40], s[sgpractivationBeta], v[vgprValuC+40] // min(x, beta)
v_cndmask_b32 v[vgprValuC+40], 0.0, v[vgprValuC+40], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+41], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+41], s[sgpractivationBeta], v[vgprValuC+41] // min(x, beta)
v_cndmask_b32 v[vgprValuC+41], 0.0, v[vgprValuC+41], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+42], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+42], s[sgpractivationBeta], v[vgprValuC+42] // min(x, beta)
v_cndmask_b32 v[vgprValuC+42], 0.0, v[vgprValuC+42], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+43], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+43], s[sgpractivationBeta], v[vgprValuC+43] // min(x, beta)
v_cndmask_b32 v[vgprValuC+43], 0.0, v[vgprValuC+43], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[40:41], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v44, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v44, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v45, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v45, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+48], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+48], s[sgpractivationBeta], v[vgprValuC+48] // min(x, beta)
v_cndmask_b32 v[vgprValuC+48], 0.0, v[vgprValuC+48], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+49], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+49], s[sgpractivationBeta], v[vgprValuC+49] // min(x, beta)
v_cndmask_b32 v[vgprValuC+49], 0.0, v[vgprValuC+49], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+50], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+50], s[sgpractivationBeta], v[vgprValuC+50] // min(x, beta)
v_cndmask_b32 v[vgprValuC+50], 0.0, v[vgprValuC+50], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+51], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+51], s[sgpractivationBeta], v[vgprValuC+51] // min(x, beta)
v_cndmask_b32 v[vgprValuC+51], 0.0, v[vgprValuC+51], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[48:49], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Beta_2:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta
label_Activation_Gelu_Beta:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Beta Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_add_lshl_u32 v8, v2, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
buffer_load_dwordx2 v[12:13], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v9, v0, s66
v_lshlrev_b32 v9, 0x2, v9                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v9 offset:0                 // load bias
v_lshlrev_b32 v10, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v10, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[28:29], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[36:37], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[44:45], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
v_add_lshl_u32 v6, v3, v0, 0x2                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_add_lshl_u32 v7, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+40], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+41], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+42], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+43], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+48], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+49], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+50], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+51], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[32:35], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[40:43], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[48:51], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s84, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s84, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s84, s84, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s84, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s91, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s90, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s94, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s94, s94, 1                              // Free1
s_mul_hi_u32 s93, s94, s[sgprStrideC1J]            // Free1
s_mul_i32 s92, s94, s[sgprStrideC1J]               // Free1
s_add_u32 s90, s90, s92                            // Free1
s_addc_u32 s91, s91, s93                           // Free1
s_sub_u32 s94, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s94, s94, 1                              // Free2
s_mul_hi_u32 s93, s94, s[sgprStrideCK]             // Free2
s_mul_i32 s92, s94, s[sgprStrideCK]                // Free2
s_add_u32 s90, s90, s92                            // Free2
s_addc_u32 s91, s91, s93                           // Free2
s_lshl_b64 s[86:87], s[90:91], 2                   // scale by bpe

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Beta_3
// check done end

// buffer load start
buffer_load_dwordx4 v[24:27], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_48 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_48 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_48 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_48 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_48 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_48 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_48 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_48:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_48 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_48 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_48 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_48 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_48 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_48 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_48 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_48      // Syncbranchhere

label_Synchronizer_read_add_end_7_48:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_48       // SyncAddbranch

label_Synchronizer_read_add_end_6_48:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_48       // SyncAddbranch

label_Synchronizer_read_add_end_5_48:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_48       // SyncAddbranch

label_Synchronizer_read_add_end_4_48:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_48       // SyncAddbranch

label_Synchronizer_read_add_end_3_48:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_48       // SyncAddbranch

label_Synchronizer_read_add_end_2_48:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_48       // SyncAddbranch

label_Synchronizer_read_add_end_1_48:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_48:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[32:35], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_49 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_49 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_49 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_49 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_49 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_49 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_49 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_49:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_49 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_49 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_49 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_49 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_49 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_49 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[192:193]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_49 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_49      // Syncbranchhere

label_Synchronizer_read_add_end_7_49:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_49       // SyncAddbranch

label_Synchronizer_read_add_end_6_49:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_49       // SyncAddbranch

label_Synchronizer_read_add_end_5_49:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_49       // SyncAddbranch

label_Synchronizer_read_add_end_4_49:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_49       // SyncAddbranch

label_Synchronizer_read_add_end_3_49:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_49       // SyncAddbranch

label_Synchronizer_read_add_end_2_49:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_49       // SyncAddbranch

label_Synchronizer_read_add_end_1_49:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_49:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[40:43], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_50 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_50 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_50 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_50 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_50 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_50 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_50 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_50:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_50 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_50 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_50 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_50 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_50 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[176:177]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_50 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[192:193]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_50 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_50      // Syncbranchhere

label_Synchronizer_read_add_end_7_50:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[176:177]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_50       // SyncAddbranch

label_Synchronizer_read_add_end_6_50:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_50       // SyncAddbranch

label_Synchronizer_read_add_end_5_50:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_50       // SyncAddbranch

label_Synchronizer_read_add_end_4_50:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_50       // SyncAddbranch

label_Synchronizer_read_add_end_3_50:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_50       // SyncAddbranch

label_Synchronizer_read_add_end_2_50:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_50       // SyncAddbranch

label_Synchronizer_read_add_end_1_50:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_50:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[48:51], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_51 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_51 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_51 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_51 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_51 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_51 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_51 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_51:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_51 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_51 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_51 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_51 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_51 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_51 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[192:193]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_51 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_51      // Syncbranchhere

label_Synchronizer_read_add_end_7_51:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_51       // SyncAddbranch

label_Synchronizer_read_add_end_6_51:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_51       // SyncAddbranch

label_Synchronizer_read_add_end_5_51:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_51       // SyncAddbranch

label_Synchronizer_read_add_end_4_51:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_51       // SyncAddbranch

label_Synchronizer_read_add_end_3_51:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_51       // SyncAddbranch

label_Synchronizer_read_add_end_2_51:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_51       // SyncAddbranch

label_Synchronizer_read_add_end_1_51:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_51:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha

/* apply mask, calc new C and issue writes */

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+24]          // k1 * x
v_fma_f32 v4, v[vgprValuC+24], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+24], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+24], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+24], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+25]          // k1 * x
v_fma_f32 v4, v[vgprValuC+25], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+25], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+25], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+25], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+26]          // k1 * x
v_fma_f32 v4, v[vgprValuC+26], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+26], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+26], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+26], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+27]          // k1 * x
v_fma_f32 v4, v[vgprValuC+27], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+27], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+27], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+27], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v28, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v28, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v29, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v29, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+32]          // k1 * x
v_fma_f32 v4, v[vgprValuC+32], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+32], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+32], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+32], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+33]          // k1 * x
v_fma_f32 v4, v[vgprValuC+33], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+33], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+33], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+33], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+34]          // k1 * x
v_fma_f32 v4, v[vgprValuC+34], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+34], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+34], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+34], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+35]          // k1 * x
v_fma_f32 v4, v[vgprValuC+35], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+35], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+35], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+35], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v36, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+41], s[sgprBeta], v36, v[vgprValuC+41] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+42], s[sgprBeta], v37, v[vgprValuC+42] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+43], s[sgprBeta], v37, v[vgprValuC+43] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+40]          // k1 * x
v_fma_f32 v4, v[vgprValuC+40], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+40], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+40], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+40], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+41]          // k1 * x
v_fma_f32 v4, v[vgprValuC+41], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+41], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+41], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+41], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+42]          // k1 * x
v_fma_f32 v4, v[vgprValuC+42], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+42], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+42], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+42], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+43]          // k1 * x
v_fma_f32 v4, v[vgprValuC+43], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+43], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+43], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+43], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[40:41], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v44, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v44, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v45, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v45, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+48]          // k1 * x
v_fma_f32 v4, v[vgprValuC+48], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+48], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+48], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+48], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+49]          // k1 * x
v_fma_f32 v4, v[vgprValuC+49], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+49], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+49], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+49], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+50]          // k1 * x
v_fma_f32 v4, v[vgprValuC+50], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+50], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+50], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+50], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+51]          // k1 * x
v_fma_f32 v4, v[vgprValuC+51], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+51], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+51], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+51], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[48:49], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Beta_3:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta
label_Activation_Leakyrelu_Beta:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Beta Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_add_lshl_u32 v8, v2, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
buffer_load_dwordx2 v[12:13], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v9, v0, s66
v_lshlrev_b32 v9, 0x2, v9                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v9 offset:0                 // load bias
v_lshlrev_b32 v10, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v10, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[28:29], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[36:37], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[44:45], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
v_add_lshl_u32 v6, v3, v0, 0x2                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_add_lshl_u32 v7, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+40], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+41], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+42], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+43], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+48], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+49], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+50], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+51], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[32:35], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[40:43], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[48:51], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s84, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s84, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s84, s84, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s84, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s91, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s90, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s94, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s94, s94, 1                              // Free1
s_mul_hi_u32 s93, s94, s[sgprStrideC1J]            // Free1
s_mul_i32 s92, s94, s[sgprStrideC1J]               // Free1
s_add_u32 s90, s90, s92                            // Free1
s_addc_u32 s91, s91, s93                           // Free1
s_sub_u32 s94, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s94, s94, 1                              // Free2
s_mul_hi_u32 s93, s94, s[sgprStrideCK]             // Free2
s_mul_i32 s92, s94, s[sgprStrideCK]                // Free2
s_add_u32 s90, s90, s92                            // Free2
s_addc_u32 s91, s91, s93                           // Free2
s_lshl_b64 s[86:87], s[90:91], 2                   // scale by bpe

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Beta_4
// check done end

// buffer load start
buffer_load_dwordx4 v[24:27], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_52 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_52 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_52 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_52 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_52 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_52 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_52 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_52:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_52 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_52 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_52 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_52 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_52 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_52 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_52 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_52      // Syncbranchhere

label_Synchronizer_read_add_end_7_52:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_52       // SyncAddbranch

label_Synchronizer_read_add_end_6_52:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_52       // SyncAddbranch

label_Synchronizer_read_add_end_5_52:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_52       // SyncAddbranch

label_Synchronizer_read_add_end_4_52:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_52       // SyncAddbranch

label_Synchronizer_read_add_end_3_52:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_52       // SyncAddbranch

label_Synchronizer_read_add_end_2_52:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_52       // SyncAddbranch

label_Synchronizer_read_add_end_1_52:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_52:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[32:35], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_53 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_53 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_53 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_53 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_53 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_53 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_53 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_53:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_53 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_53 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_53 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_53 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_53 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_53 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[192:193]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_53 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_53      // Syncbranchhere

label_Synchronizer_read_add_end_7_53:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_53       // SyncAddbranch

label_Synchronizer_read_add_end_6_53:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_53       // SyncAddbranch

label_Synchronizer_read_add_end_5_53:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_53       // SyncAddbranch

label_Synchronizer_read_add_end_4_53:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_53       // SyncAddbranch

label_Synchronizer_read_add_end_3_53:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_53       // SyncAddbranch

label_Synchronizer_read_add_end_2_53:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_53       // SyncAddbranch

label_Synchronizer_read_add_end_1_53:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_53:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[40:43], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_54 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_54 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_54 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_54 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_54 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_54 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_54 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_54:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_54 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_54 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_54 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_54 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_54 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[176:177]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_54 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[192:193]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_54 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_54      // Syncbranchhere

label_Synchronizer_read_add_end_7_54:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[176:177]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_54       // SyncAddbranch

label_Synchronizer_read_add_end_6_54:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_54       // SyncAddbranch

label_Synchronizer_read_add_end_5_54:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_54       // SyncAddbranch

label_Synchronizer_read_add_end_4_54:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_54       // SyncAddbranch

label_Synchronizer_read_add_end_3_54:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_54       // SyncAddbranch

label_Synchronizer_read_add_end_2_54:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_54       // SyncAddbranch

label_Synchronizer_read_add_end_1_54:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_54:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[48:51], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_55 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_55 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_55 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_55 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_55 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_55 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_55 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_55:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_55 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_55 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_55 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_55 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_55 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_55 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[192:193]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_55 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_55      // Syncbranchhere

label_Synchronizer_read_add_end_7_55:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_55       // SyncAddbranch

label_Synchronizer_read_add_end_6_55:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_55       // SyncAddbranch

label_Synchronizer_read_add_end_5_55:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_55       // SyncAddbranch

label_Synchronizer_read_add_end_4_55:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_55       // SyncAddbranch

label_Synchronizer_read_add_end_3_55:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_55       // SyncAddbranch

label_Synchronizer_read_add_end_2_55:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_55       // SyncAddbranch

label_Synchronizer_read_add_end_1_55:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_55:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha

/* apply mask, calc new C and issue writes */

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+24] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+24], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+24], v4, v[vgprValuC+24], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+25] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+25], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+25], v4, v[vgprValuC+25], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+26] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+26], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+26], v4, v[vgprValuC+26], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+27] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+27], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+27], v4, v[vgprValuC+27], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v28, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v28, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v29, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v29, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+32] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+32], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+32], v4, v[vgprValuC+32], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+33] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+33], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+33], v4, v[vgprValuC+33], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+34] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+34], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+34], v4, v[vgprValuC+34], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+35] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+35], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+35], v4, v[vgprValuC+35], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v36, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+41], s[sgprBeta], v36, v[vgprValuC+41] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+42], s[sgprBeta], v37, v[vgprValuC+42] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+43], s[sgprBeta], v37, v[vgprValuC+43] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+40] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+40], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+40], v4, v[vgprValuC+40], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+41] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+41], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+41], v4, v[vgprValuC+41], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+42] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+42], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+42], v4, v[vgprValuC+42], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+43] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+43], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+43], v4, v[vgprValuC+43], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[40:41], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v44, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v44, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v45, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v45, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+48] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+48], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+48], v4, v[vgprValuC+48], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+49] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+49], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+49], v4, v[vgprValuC+49], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+50] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+50], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+50], v4, v[vgprValuC+50], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+51] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+51], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+51], v4, v[vgprValuC+51], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[48:49], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Beta_4:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta
label_Activation_Relu_Beta:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Beta Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_add_lshl_u32 v8, v2, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
buffer_load_dwordx2 v[12:13], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v9, v0, s66
v_lshlrev_b32 v9, 0x2, v9                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v9 offset:0                 // load bias
v_lshlrev_b32 v10, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v10, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[28:29], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[36:37], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[44:45], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
v_add_lshl_u32 v6, v3, v0, 0x2                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_add_lshl_u32 v7, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+40], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+41], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+42], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+43], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+48], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+49], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+50], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+51], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[32:35], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[40:43], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[48:51], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s84, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s84, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s84, s84, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s84, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s91, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s90, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s94, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s94, s94, 1                              // Free1
s_mul_hi_u32 s93, s94, s[sgprStrideC1J]            // Free1
s_mul_i32 s92, s94, s[sgprStrideC1J]               // Free1
s_add_u32 s90, s90, s92                            // Free1
s_addc_u32 s91, s91, s93                           // Free1
s_sub_u32 s94, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s94, s94, 1                              // Free2
s_mul_hi_u32 s93, s94, s[sgprStrideCK]             // Free2
s_mul_i32 s92, s94, s[sgprStrideCK]                // Free2
s_add_u32 s90, s90, s92                            // Free2
s_addc_u32 s91, s91, s93                           // Free2
s_lshl_b64 s[86:87], s[90:91], 2                   // scale by bpe

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Beta_5
// check done end

// buffer load start
buffer_load_dwordx4 v[24:27], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_56 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_56 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_56 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_56 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_56 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_56 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_56 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_56:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_56 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_56 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_56 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_56 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_56 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_56 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_56 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_56      // Syncbranchhere

label_Synchronizer_read_add_end_7_56:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_56       // SyncAddbranch

label_Synchronizer_read_add_end_6_56:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_56       // SyncAddbranch

label_Synchronizer_read_add_end_5_56:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_56       // SyncAddbranch

label_Synchronizer_read_add_end_4_56:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_56       // SyncAddbranch

label_Synchronizer_read_add_end_3_56:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_56       // SyncAddbranch

label_Synchronizer_read_add_end_2_56:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_56       // SyncAddbranch

label_Synchronizer_read_add_end_1_56:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_56:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[32:35], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_57 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_57 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_57 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_57 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_57 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_57 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_57 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_57:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_57 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_57 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_57 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_57 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_57 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_57 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[192:193]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_57 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_57      // Syncbranchhere

label_Synchronizer_read_add_end_7_57:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_57       // SyncAddbranch

label_Synchronizer_read_add_end_6_57:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_57       // SyncAddbranch

label_Synchronizer_read_add_end_5_57:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_57       // SyncAddbranch

label_Synchronizer_read_add_end_4_57:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_57       // SyncAddbranch

label_Synchronizer_read_add_end_3_57:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_57       // SyncAddbranch

label_Synchronizer_read_add_end_2_57:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_57       // SyncAddbranch

label_Synchronizer_read_add_end_1_57:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_57:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[40:43], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_58 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_58 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_58 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_58 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_58 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_58 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_58 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_58:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_58 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_58 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_58 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_58 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_58 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[176:177]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_58 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[192:193]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_58 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_58      // Syncbranchhere

label_Synchronizer_read_add_end_7_58:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[176:177]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_58       // SyncAddbranch

label_Synchronizer_read_add_end_6_58:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_58       // SyncAddbranch

label_Synchronizer_read_add_end_5_58:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_58       // SyncAddbranch

label_Synchronizer_read_add_end_4_58:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_58       // SyncAddbranch

label_Synchronizer_read_add_end_3_58:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_58       // SyncAddbranch

label_Synchronizer_read_add_end_2_58:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_58       // SyncAddbranch

label_Synchronizer_read_add_end_1_58:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_58:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[48:51], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_59 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_59 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_59 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_59 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_59 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_59 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_59 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_59:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_59 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_59 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_59 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_59 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_59 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_59 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[192:193]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_59 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_59      // Syncbranchhere

label_Synchronizer_read_add_end_7_59:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_59       // SyncAddbranch

label_Synchronizer_read_add_end_6_59:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_59       // SyncAddbranch

label_Synchronizer_read_add_end_5_59:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_59       // SyncAddbranch

label_Synchronizer_read_add_end_4_59:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_59       // SyncAddbranch

label_Synchronizer_read_add_end_3_59:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_59       // SyncAddbranch

label_Synchronizer_read_add_end_2_59:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_59       // SyncAddbranch

label_Synchronizer_read_add_end_1_59:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_59:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha

/* apply mask, calc new C and issue writes */

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_max_f32 v[vgprValuC+24], v[vgprValuC+24], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+25], v[vgprValuC+25], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+26], v[vgprValuC+26], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+27], v[vgprValuC+27], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v28, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v28, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v29, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v29, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_max_f32 v[vgprValuC+32], v[vgprValuC+32], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+33], v[vgprValuC+33], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+34], v[vgprValuC+34], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+35], v[vgprValuC+35], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v36, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+41], s[sgprBeta], v36, v[vgprValuC+41] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+42], s[sgprBeta], v37, v[vgprValuC+42] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+43], s[sgprBeta], v37, v[vgprValuC+43] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_max_f32 v[vgprValuC+40], v[vgprValuC+40], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+41], v[vgprValuC+41], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+42], v[vgprValuC+42], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+43], v[vgprValuC+43], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[40:41], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v44, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v44, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v45, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v45, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_max_f32 v[vgprValuC+48], v[vgprValuC+48], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+49], v[vgprValuC+49], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+50], v[vgprValuC+50], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+51], v[vgprValuC+51], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[48:49], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Beta_5:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta
label_Activation_Sigmoid_Beta:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Beta Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_add_lshl_u32 v8, v2, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
buffer_load_dwordx2 v[12:13], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v9, v0, s66
v_lshlrev_b32 v9, 0x2, v9                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v9 offset:0                 // load bias
v_lshlrev_b32 v10, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v10, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[28:29], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[36:37], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[44:45], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
v_add_lshl_u32 v6, v3, v0, 0x2                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_add_lshl_u32 v7, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+40], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+41], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+42], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+43], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+48], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+49], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+50], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+51], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[32:35], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[40:43], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[48:51], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s84, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s84, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s84, s84, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s84, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s91, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s90, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s94, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s94, s94, 1                              // Free1
s_mul_hi_u32 s93, s94, s[sgprStrideC1J]            // Free1
s_mul_i32 s92, s94, s[sgprStrideC1J]               // Free1
s_add_u32 s90, s90, s92                            // Free1
s_addc_u32 s91, s91, s93                           // Free1
s_sub_u32 s94, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s94, s94, 1                              // Free2
s_mul_hi_u32 s93, s94, s[sgprStrideCK]             // Free2
s_mul_i32 s92, s94, s[sgprStrideCK]                // Free2
s_add_u32 s90, s90, s92                            // Free2
s_addc_u32 s91, s91, s93                           // Free2
s_lshl_b64 s[86:87], s[90:91], 2                   // scale by bpe

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Beta_6
// check done end

// buffer load start
buffer_load_dwordx4 v[24:27], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_60 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_60 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_60 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_60 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_60 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_60 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_60 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_60:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_60 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_60 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_60 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_60 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_60 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_60 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_60 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_60      // Syncbranchhere

label_Synchronizer_read_add_end_7_60:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_60       // SyncAddbranch

label_Synchronizer_read_add_end_6_60:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_60       // SyncAddbranch

label_Synchronizer_read_add_end_5_60:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_60       // SyncAddbranch

label_Synchronizer_read_add_end_4_60:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_60       // SyncAddbranch

label_Synchronizer_read_add_end_3_60:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_60       // SyncAddbranch

label_Synchronizer_read_add_end_2_60:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_60       // SyncAddbranch

label_Synchronizer_read_add_end_1_60:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_60:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[32:35], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_61 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_61 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_61 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_61 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_61 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_61 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_61 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_61:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_61 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_61 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_61 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_61 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_61 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_61 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[192:193]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_61 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_61      // Syncbranchhere

label_Synchronizer_read_add_end_7_61:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_61       // SyncAddbranch

label_Synchronizer_read_add_end_6_61:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_61       // SyncAddbranch

label_Synchronizer_read_add_end_5_61:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_61       // SyncAddbranch

label_Synchronizer_read_add_end_4_61:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_61       // SyncAddbranch

label_Synchronizer_read_add_end_3_61:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_61       // SyncAddbranch

label_Synchronizer_read_add_end_2_61:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_61       // SyncAddbranch

label_Synchronizer_read_add_end_1_61:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_61:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[40:43], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_62 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_62 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_62 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_62 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_62 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_62 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_62 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_62:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_62 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_62 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_62 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_62 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_62 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[176:177]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_62 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[192:193]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_62 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_62      // Syncbranchhere

label_Synchronizer_read_add_end_7_62:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[176:177]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_62       // SyncAddbranch

label_Synchronizer_read_add_end_6_62:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_62       // SyncAddbranch

label_Synchronizer_read_add_end_5_62:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_62       // SyncAddbranch

label_Synchronizer_read_add_end_4_62:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_62       // SyncAddbranch

label_Synchronizer_read_add_end_3_62:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_62       // SyncAddbranch

label_Synchronizer_read_add_end_2_62:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_62       // SyncAddbranch

label_Synchronizer_read_add_end_1_62:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_62:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[48:51], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_63 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_63 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_63 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_63 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_63 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_63 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_63 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_63:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_63 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_63 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_63 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_63 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_63 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_63 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[192:193]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_63 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_63      // Syncbranchhere

label_Synchronizer_read_add_end_7_63:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_63       // SyncAddbranch

label_Synchronizer_read_add_end_6_63:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_63       // SyncAddbranch

label_Synchronizer_read_add_end_5_63:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_63       // SyncAddbranch

label_Synchronizer_read_add_end_4_63:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_63       // SyncAddbranch

label_Synchronizer_read_add_end_3_63:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_63       // SyncAddbranch

label_Synchronizer_read_add_end_2_63:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_63       // SyncAddbranch

label_Synchronizer_read_add_end_1_63:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_63:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha

/* apply mask, calc new C and issue writes */

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v[vgprValuC+24], 0xbfb8aa3b, v[vgprValuC+24] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+24], v[vgprValuC+24]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+24], 1.0, v[vgprValuC+24]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+24], v[vgprValuC+24]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+25], 0xbfb8aa3b, v[vgprValuC+25] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+25], v[vgprValuC+25]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+25], 1.0, v[vgprValuC+25]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+25], v[vgprValuC+25]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+26], 0xbfb8aa3b, v[vgprValuC+26] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+26], v[vgprValuC+26]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+26], 1.0, v[vgprValuC+26]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+26], v[vgprValuC+26]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+27], 0xbfb8aa3b, v[vgprValuC+27] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+27], v[vgprValuC+27]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+27], 1.0, v[vgprValuC+27]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+27], v[vgprValuC+27]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v28, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v28, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v29, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v29, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v[vgprValuC+32], 0xbfb8aa3b, v[vgprValuC+32] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+32], v[vgprValuC+32]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+32], 1.0, v[vgprValuC+32]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+32], v[vgprValuC+32]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+33], 0xbfb8aa3b, v[vgprValuC+33] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+33], v[vgprValuC+33]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+33], 1.0, v[vgprValuC+33]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+33], v[vgprValuC+33]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+34], 0xbfb8aa3b, v[vgprValuC+34] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+34], v[vgprValuC+34]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+34], 1.0, v[vgprValuC+34]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+34], v[vgprValuC+34]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+35], 0xbfb8aa3b, v[vgprValuC+35] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+35], v[vgprValuC+35]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+35], 1.0, v[vgprValuC+35]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+35], v[vgprValuC+35]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v36, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+41], s[sgprBeta], v36, v[vgprValuC+41] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+42], s[sgprBeta], v37, v[vgprValuC+42] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+43], s[sgprBeta], v37, v[vgprValuC+43] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_mul_f32 v[vgprValuC+40], 0xbfb8aa3b, v[vgprValuC+40] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+40], v[vgprValuC+40]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+40], 1.0, v[vgprValuC+40]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+40], v[vgprValuC+40]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+41], 0xbfb8aa3b, v[vgprValuC+41] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+41], v[vgprValuC+41]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+41], 1.0, v[vgprValuC+41]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+41], v[vgprValuC+41]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+42], 0xbfb8aa3b, v[vgprValuC+42] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+42], v[vgprValuC+42]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+42], 1.0, v[vgprValuC+42]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+42], v[vgprValuC+42]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+43], 0xbfb8aa3b, v[vgprValuC+43] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+43], v[vgprValuC+43]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+43], 1.0, v[vgprValuC+43]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+43], v[vgprValuC+43]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[40:41], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v44, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v44, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v45, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v45, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_mul_f32 v[vgprValuC+48], 0xbfb8aa3b, v[vgprValuC+48] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+48], v[vgprValuC+48]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+48], 1.0, v[vgprValuC+48]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+48], v[vgprValuC+48]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+49], 0xbfb8aa3b, v[vgprValuC+49] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+49], v[vgprValuC+49]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+49], 1.0, v[vgprValuC+49]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+49], v[vgprValuC+49]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+50], 0xbfb8aa3b, v[vgprValuC+50] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+50], v[vgprValuC+50]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+50], 1.0, v[vgprValuC+50]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+50], v[vgprValuC+50]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+51], 0xbfb8aa3b, v[vgprValuC+51] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+51], v[vgprValuC+51]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+51], 1.0, v[vgprValuC+51]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+51], v[vgprValuC+51]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[48:49], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Beta_6:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta
label_Activation_Tanh_Beta:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Beta Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_add_lshl_u32 v8, v2, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
buffer_load_dwordx2 v[12:13], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v9, v0, s66
v_lshlrev_b32 v9, 0x2, v9                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v9 offset:0                 // load bias
v_lshlrev_b32 v10, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v10, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[28:29], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[36:37], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[44:45], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
v_add_lshl_u32 v6, v3, v0, 0x2                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_add_lshl_u32 v7, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+40], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+41], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+42], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+43], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+48], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+49], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+50], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+51], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[32:35], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[40:43], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[48:51], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s84, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s84, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s84, s84, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s84, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s91, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s90, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s94, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s94, s94, 1                              // Free1
s_mul_hi_u32 s93, s94, s[sgprStrideC1J]            // Free1
s_mul_i32 s92, s94, s[sgprStrideC1J]               // Free1
s_add_u32 s90, s90, s92                            // Free1
s_addc_u32 s91, s91, s93                           // Free1
s_sub_u32 s94, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s94, s94, 1                              // Free2
s_mul_hi_u32 s93, s94, s[sgprStrideCK]             // Free2
s_mul_i32 s92, s94, s[sgprStrideCK]                // Free2
s_add_u32 s90, s90, s92                            // Free2
s_addc_u32 s91, s91, s93                           // Free2
s_lshl_b64 s[86:87], s[90:91], 2                   // scale by bpe

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Beta_7
// check done end

// buffer load start
buffer_load_dwordx4 v[24:27], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_64 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_64 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_64 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_64 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_64 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_64 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_64 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_64:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_64 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_64 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_64 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_64 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_64 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_64 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_64 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_64      // Syncbranchhere

label_Synchronizer_read_add_end_7_64:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_64       // SyncAddbranch

label_Synchronizer_read_add_end_6_64:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_64       // SyncAddbranch

label_Synchronizer_read_add_end_5_64:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_64       // SyncAddbranch

label_Synchronizer_read_add_end_4_64:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_64       // SyncAddbranch

label_Synchronizer_read_add_end_3_64:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_64       // SyncAddbranch

label_Synchronizer_read_add_end_2_64:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_64       // SyncAddbranch

label_Synchronizer_read_add_end_1_64:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_64:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[32:35], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_65 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_65 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_65 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_65 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_65 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_65 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_65 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_65:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_65 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_65 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_65 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_65 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_65 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_65 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[192:193]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_65 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_65      // Syncbranchhere

label_Synchronizer_read_add_end_7_65:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_65       // SyncAddbranch

label_Synchronizer_read_add_end_6_65:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_65       // SyncAddbranch

label_Synchronizer_read_add_end_5_65:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_65       // SyncAddbranch

label_Synchronizer_read_add_end_4_65:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_65       // SyncAddbranch

label_Synchronizer_read_add_end_3_65:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_65       // SyncAddbranch

label_Synchronizer_read_add_end_2_65:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_65       // SyncAddbranch

label_Synchronizer_read_add_end_1_65:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_65:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[40:43], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_66 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_66 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_66 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_66 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_66 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_66 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_66 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_66:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_66 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_66 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_66 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_66 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_66 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[176:177]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_66 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[192:193]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_66 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_66      // Syncbranchhere

label_Synchronizer_read_add_end_7_66:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[176:177]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_66       // SyncAddbranch

label_Synchronizer_read_add_end_6_66:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_66       // SyncAddbranch

label_Synchronizer_read_add_end_5_66:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_66       // SyncAddbranch

label_Synchronizer_read_add_end_4_66:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_66       // SyncAddbranch

label_Synchronizer_read_add_end_3_66:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_66       // SyncAddbranch

label_Synchronizer_read_add_end_2_66:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_66       // SyncAddbranch

label_Synchronizer_read_add_end_1_66:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_66:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[48:51], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_67 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_67 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_67 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_67 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_67 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_67 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_67 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_67:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_67 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_67 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_67 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_67 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_67 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_67 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[192:193]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_67 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_67      // Syncbranchhere

label_Synchronizer_read_add_end_7_67:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_67       // SyncAddbranch

label_Synchronizer_read_add_end_6_67:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_67       // SyncAddbranch

label_Synchronizer_read_add_end_5_67:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_67       // SyncAddbranch

label_Synchronizer_read_add_end_4_67:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_67       // SyncAddbranch

label_Synchronizer_read_add_end_3_67:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_67       // SyncAddbranch

label_Synchronizer_read_add_end_2_67:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_67       // SyncAddbranch

label_Synchronizer_read_add_end_1_67:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_67:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha

/* apply mask, calc new C and issue writes */

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v[vgprValuC+24], s[sgpractivationAlpha], v[vgprValuC+24] // x * alpha
v_mul_f32 v[vgprValuC+24], 0x4038aa3b, v[vgprValuC+24] //  (fused 2)
v_exp_f32 v[vgprValuC+24], v[vgprValuC+24]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+24], 1.0, v[vgprValuC+24]    // e^2x + 1
v_rcp_f32 v[vgprValuC+24], v[vgprValuC+24]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+24], -2.0, v[vgprValuC+24], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+24], s[sgpractivationBeta], v[vgprValuC+24] // beta * tanh(x)
v_mul_f32 v[vgprValuC+25], s[sgpractivationAlpha], v[vgprValuC+25] // x * alpha
v_mul_f32 v[vgprValuC+25], 0x4038aa3b, v[vgprValuC+25] //  (fused 2)
v_exp_f32 v[vgprValuC+25], v[vgprValuC+25]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+25], 1.0, v[vgprValuC+25]    // e^2x + 1
v_rcp_f32 v[vgprValuC+25], v[vgprValuC+25]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+25], -2.0, v[vgprValuC+25], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+25], s[sgpractivationBeta], v[vgprValuC+25] // beta * tanh(x)
v_mul_f32 v[vgprValuC+26], s[sgpractivationAlpha], v[vgprValuC+26] // x * alpha
v_mul_f32 v[vgprValuC+26], 0x4038aa3b, v[vgprValuC+26] //  (fused 2)
v_exp_f32 v[vgprValuC+26], v[vgprValuC+26]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+26], 1.0, v[vgprValuC+26]    // e^2x + 1
v_rcp_f32 v[vgprValuC+26], v[vgprValuC+26]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+26], -2.0, v[vgprValuC+26], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+26], s[sgpractivationBeta], v[vgprValuC+26] // beta * tanh(x)
v_mul_f32 v[vgprValuC+27], s[sgpractivationAlpha], v[vgprValuC+27] // x * alpha
v_mul_f32 v[vgprValuC+27], 0x4038aa3b, v[vgprValuC+27] //  (fused 2)
v_exp_f32 v[vgprValuC+27], v[vgprValuC+27]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+27], 1.0, v[vgprValuC+27]    // e^2x + 1
v_rcp_f32 v[vgprValuC+27], v[vgprValuC+27]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+27], -2.0, v[vgprValuC+27], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+27], s[sgpractivationBeta], v[vgprValuC+27] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v28, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v28, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v29, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v29, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v[vgprValuC+32], s[sgpractivationAlpha], v[vgprValuC+32] // x * alpha
v_mul_f32 v[vgprValuC+32], 0x4038aa3b, v[vgprValuC+32] //  (fused 2)
v_exp_f32 v[vgprValuC+32], v[vgprValuC+32]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+32], 1.0, v[vgprValuC+32]    // e^2x + 1
v_rcp_f32 v[vgprValuC+32], v[vgprValuC+32]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+32], -2.0, v[vgprValuC+32], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+32], s[sgpractivationBeta], v[vgprValuC+32] // beta * tanh(x)
v_mul_f32 v[vgprValuC+33], s[sgpractivationAlpha], v[vgprValuC+33] // x * alpha
v_mul_f32 v[vgprValuC+33], 0x4038aa3b, v[vgprValuC+33] //  (fused 2)
v_exp_f32 v[vgprValuC+33], v[vgprValuC+33]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+33], 1.0, v[vgprValuC+33]    // e^2x + 1
v_rcp_f32 v[vgprValuC+33], v[vgprValuC+33]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+33], -2.0, v[vgprValuC+33], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+33], s[sgpractivationBeta], v[vgprValuC+33] // beta * tanh(x)
v_mul_f32 v[vgprValuC+34], s[sgpractivationAlpha], v[vgprValuC+34] // x * alpha
v_mul_f32 v[vgprValuC+34], 0x4038aa3b, v[vgprValuC+34] //  (fused 2)
v_exp_f32 v[vgprValuC+34], v[vgprValuC+34]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+34], 1.0, v[vgprValuC+34]    // e^2x + 1
v_rcp_f32 v[vgprValuC+34], v[vgprValuC+34]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+34], -2.0, v[vgprValuC+34], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+34], s[sgpractivationBeta], v[vgprValuC+34] // beta * tanh(x)
v_mul_f32 v[vgprValuC+35], s[sgpractivationAlpha], v[vgprValuC+35] // x * alpha
v_mul_f32 v[vgprValuC+35], 0x4038aa3b, v[vgprValuC+35] //  (fused 2)
v_exp_f32 v[vgprValuC+35], v[vgprValuC+35]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+35], 1.0, v[vgprValuC+35]    // e^2x + 1
v_rcp_f32 v[vgprValuC+35], v[vgprValuC+35]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+35], -2.0, v[vgprValuC+35], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+35], s[sgpractivationBeta], v[vgprValuC+35] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v36, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+41], s[sgprBeta], v36, v[vgprValuC+41] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+42], s[sgprBeta], v37, v[vgprValuC+42] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+43], s[sgprBeta], v37, v[vgprValuC+43] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_mul_f32 v[vgprValuC+40], s[sgpractivationAlpha], v[vgprValuC+40] // x * alpha
v_mul_f32 v[vgprValuC+40], 0x4038aa3b, v[vgprValuC+40] //  (fused 2)
v_exp_f32 v[vgprValuC+40], v[vgprValuC+40]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+40], 1.0, v[vgprValuC+40]    // e^2x + 1
v_rcp_f32 v[vgprValuC+40], v[vgprValuC+40]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+40], -2.0, v[vgprValuC+40], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+40], s[sgpractivationBeta], v[vgprValuC+40] // beta * tanh(x)
v_mul_f32 v[vgprValuC+41], s[sgpractivationAlpha], v[vgprValuC+41] // x * alpha
v_mul_f32 v[vgprValuC+41], 0x4038aa3b, v[vgprValuC+41] //  (fused 2)
v_exp_f32 v[vgprValuC+41], v[vgprValuC+41]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+41], 1.0, v[vgprValuC+41]    // e^2x + 1
v_rcp_f32 v[vgprValuC+41], v[vgprValuC+41]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+41], -2.0, v[vgprValuC+41], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+41], s[sgpractivationBeta], v[vgprValuC+41] // beta * tanh(x)
v_mul_f32 v[vgprValuC+42], s[sgpractivationAlpha], v[vgprValuC+42] // x * alpha
v_mul_f32 v[vgprValuC+42], 0x4038aa3b, v[vgprValuC+42] //  (fused 2)
v_exp_f32 v[vgprValuC+42], v[vgprValuC+42]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+42], 1.0, v[vgprValuC+42]    // e^2x + 1
v_rcp_f32 v[vgprValuC+42], v[vgprValuC+42]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+42], -2.0, v[vgprValuC+42], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+42], s[sgpractivationBeta], v[vgprValuC+42] // beta * tanh(x)
v_mul_f32 v[vgprValuC+43], s[sgpractivationAlpha], v[vgprValuC+43] // x * alpha
v_mul_f32 v[vgprValuC+43], 0x4038aa3b, v[vgprValuC+43] //  (fused 2)
v_exp_f32 v[vgprValuC+43], v[vgprValuC+43]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+43], 1.0, v[vgprValuC+43]    // e^2x + 1
v_rcp_f32 v[vgprValuC+43], v[vgprValuC+43]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+43], -2.0, v[vgprValuC+43], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+43], s[sgpractivationBeta], v[vgprValuC+43] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[40:41], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v44, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v44, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v45, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v45, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_mul_f32 v[vgprValuC+48], s[sgpractivationAlpha], v[vgprValuC+48] // x * alpha
v_mul_f32 v[vgprValuC+48], 0x4038aa3b, v[vgprValuC+48] //  (fused 2)
v_exp_f32 v[vgprValuC+48], v[vgprValuC+48]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+48], 1.0, v[vgprValuC+48]    // e^2x + 1
v_rcp_f32 v[vgprValuC+48], v[vgprValuC+48]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+48], -2.0, v[vgprValuC+48], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+48], s[sgpractivationBeta], v[vgprValuC+48] // beta * tanh(x)
v_mul_f32 v[vgprValuC+49], s[sgpractivationAlpha], v[vgprValuC+49] // x * alpha
v_mul_f32 v[vgprValuC+49], 0x4038aa3b, v[vgprValuC+49] //  (fused 2)
v_exp_f32 v[vgprValuC+49], v[vgprValuC+49]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+49], 1.0, v[vgprValuC+49]    // e^2x + 1
v_rcp_f32 v[vgprValuC+49], v[vgprValuC+49]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+49], -2.0, v[vgprValuC+49], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+49], s[sgpractivationBeta], v[vgprValuC+49] // beta * tanh(x)
v_mul_f32 v[vgprValuC+50], s[sgpractivationAlpha], v[vgprValuC+50] // x * alpha
v_mul_f32 v[vgprValuC+50], 0x4038aa3b, v[vgprValuC+50] //  (fused 2)
v_exp_f32 v[vgprValuC+50], v[vgprValuC+50]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+50], 1.0, v[vgprValuC+50]    // e^2x + 1
v_rcp_f32 v[vgprValuC+50], v[vgprValuC+50]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+50], -2.0, v[vgprValuC+50], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+50], s[sgpractivationBeta], v[vgprValuC+50] // beta * tanh(x)
v_mul_f32 v[vgprValuC+51], s[sgpractivationAlpha], v[vgprValuC+51] // x * alpha
v_mul_f32 v[vgprValuC+51], 0x4038aa3b, v[vgprValuC+51] //  (fused 2)
v_exp_f32 v[vgprValuC+51], v[vgprValuC+51]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+51], 1.0, v[vgprValuC+51]    // e^2x + 1
v_rcp_f32 v[vgprValuC+51], v[vgprValuC+51]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+51], -2.0, v[vgprValuC+51], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+51], s[sgpractivationBeta], v[vgprValuC+51] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[48:49], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Beta_7:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta
label_Activation_Geluscaling_Beta:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Beta Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_add_lshl_u32 v8, v2, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
buffer_load_dwordx2 v[12:13], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v9, v0, s66
v_lshlrev_b32 v9, 0x2, v9                          // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v9 offset:0                 // load bias
v_lshlrev_b32 v10, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v10, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[28:29], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[36:37], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[44:45], v8, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
v_add_lshl_u32 v6, v3, v0, 0x2                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_add_lshl_u32 v7, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+40], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+41], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+42], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+43], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+48], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+49], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+50], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+51], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[32:35], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[40:43], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

s_lshl_b32 s66, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[48:51], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s84, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s84, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s84, s84, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s84, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s91, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s90, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s94, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s94, s94, 1                              // Free1
s_mul_hi_u32 s93, s94, s[sgprStrideC1J]            // Free1
s_mul_i32 s92, s94, s[sgprStrideC1J]               // Free1
s_add_u32 s90, s90, s92                            // Free1
s_addc_u32 s91, s91, s93                           // Free1
s_sub_u32 s94, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s94, s94, 1                              // Free2
s_mul_hi_u32 s93, s94, s[sgprStrideCK]             // Free2
s_mul_i32 s92, s94, s[sgprStrideCK]                // Free2
s_add_u32 s90, s90, s92                            // Free2
s_addc_u32 s91, s91, s93                           // Free2
s_lshl_b64 s[86:87], s[90:91], 2                   // scale by bpe

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Beta_8
// check done end

// buffer load start
buffer_load_dwordx4 v[24:27], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_68 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_68 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_68 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_68 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_68 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_68 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_68 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_68:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_68 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_68 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_68 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_68 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_68 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_68 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_68 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_68      // Syncbranchhere

label_Synchronizer_read_add_end_7_68:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_68       // SyncAddbranch

label_Synchronizer_read_add_end_6_68:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_68       // SyncAddbranch

label_Synchronizer_read_add_end_5_68:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_68       // SyncAddbranch

label_Synchronizer_read_add_end_4_68:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_68       // SyncAddbranch

label_Synchronizer_read_add_end_3_68:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_68       // SyncAddbranch

label_Synchronizer_read_add_end_2_68:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_68       // SyncAddbranch

label_Synchronizer_read_add_end_1_68:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_68:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[32:35], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_69 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_69 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_69 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_69 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_69 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_69 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_69 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_69:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_69 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_69 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_69 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_69 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_69 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_69 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[192:193]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_69 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_69      // Syncbranchhere

label_Synchronizer_read_add_end_7_69:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[176:177]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_69       // SyncAddbranch

label_Synchronizer_read_add_end_6_69:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[160:161]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_69       // SyncAddbranch

label_Synchronizer_read_add_end_5_69:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[144:145]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_69       // SyncAddbranch

label_Synchronizer_read_add_end_4_69:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[128:129]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_69       // SyncAddbranch

label_Synchronizer_read_add_end_3_69:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[112:113]        // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_69       // SyncAddbranch

label_Synchronizer_read_add_end_2_69:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[32:33], v[32:33], v[96:97]          // buffer pk
v_pk_add_f32 v[34:35], v[34:35], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_69       // SyncAddbranch

label_Synchronizer_read_add_end_1_69:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_69:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[40:43], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_70 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_70 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_70 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_70 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_70 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_70 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_70 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_70:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_70 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_70 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_70 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_70 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_70 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[176:177]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_70 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[192:193]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_70 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_70      // Syncbranchhere

label_Synchronizer_read_add_end_7_70:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[176:177]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_70       // SyncAddbranch

label_Synchronizer_read_add_end_6_70:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[160:161]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_70       // SyncAddbranch

label_Synchronizer_read_add_end_5_70:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[144:145]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_70       // SyncAddbranch

label_Synchronizer_read_add_end_4_70:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[128:129]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_70       // SyncAddbranch

label_Synchronizer_read_add_end_3_70:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[112:113]        // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_70       // SyncAddbranch

label_Synchronizer_read_add_end_2_70:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[40:41], v[40:41], v[96:97]          // buffer pk
v_pk_add_f32 v[42:43], v[42:43], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_70       // SyncAddbranch

label_Synchronizer_read_add_end_1_70:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_70:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v11, BufferOOB
s_mov_b32 s92, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s93, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s94, s[sgprSrdD+2]
s_mov_b32 s95, s[sgprSrdD+3]
s_lshl_b32 s88, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s92, s92, s88                            // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s93, s93, 0                             // incToNextRow: gra SRD += inc(upper)
s_add_u32 s[sgprWSDstart+0], s[sgprWSDstart+0], s88
s_addc_u32 s[sgprWSDstart+1], s[sgprWSDstart+1], 0x0
buffer_load_dwordx4 v[48:51], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_71 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_71 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_71 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_71 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_71 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_71 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_71 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_71:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_71 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_71 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_71 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_71 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_71 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_71 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[192:193]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_71 // SyncAddbranch
s_add_u32 s92, s92, s86
s_addc_u32 s93, s93, s87
v_cmp_ge_i32 s[88:89], 0, s[sgprGSUSync]
v_cndmask_b32 v52, v6, v11, s[88:89]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v52, s[92:95], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_71      // Syncbranchhere

label_Synchronizer_read_add_end_7_71:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_71       // SyncAddbranch

label_Synchronizer_read_add_end_6_71:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_71       // SyncAddbranch

label_Synchronizer_read_add_end_5_71:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_71       // SyncAddbranch

label_Synchronizer_read_add_end_4_71:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_71       // SyncAddbranch

label_Synchronizer_read_add_end_3_71:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_71       // SyncAddbranch

label_Synchronizer_read_add_end_2_71:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_71       // SyncAddbranch

label_Synchronizer_read_add_end_1_71:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_71:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha

/* apply mask, calc new C and issue writes */

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+24]          // k1 * x
v_fma_f32 v4, v[vgprValuC+24], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+24], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+24], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+24], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+25]          // k1 * x
v_fma_f32 v4, v[vgprValuC+25], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+25], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+25], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+25], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+26]          // k1 * x
v_fma_f32 v4, v[vgprValuC+26], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+26], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+26], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+26], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+27]          // k1 * x
v_fma_f32 v4, v[vgprValuC+27], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+27], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+27], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+27], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v28, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v28, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v29, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v29, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+32]          // k1 * x
v_fma_f32 v4, v[vgprValuC+32], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+32], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+32], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+32], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+33]          // k1 * x
v_fma_f32 v4, v[vgprValuC+33], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+33], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+33], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+33], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+34]          // k1 * x
v_fma_f32 v4, v[vgprValuC+34], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+34], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+34], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+34], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+35]          // k1 * x
v_fma_f32 v4, v[vgprValuC+35], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+35], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+35], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+35], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v36, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+41], s[sgprBeta], v36, v[vgprValuC+41] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+42], s[sgprBeta], v37, v[vgprValuC+42] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+43], s[sgprBeta], v37, v[vgprValuC+43] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+40]          // k1 * x
v_fma_f32 v4, v[vgprValuC+40], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+40], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+40], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+40], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+41]          // k1 * x
v_fma_f32 v4, v[vgprValuC+41], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+41], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+41], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+41], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+42]          // k1 * x
v_fma_f32 v4, v[vgprValuC+42], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+42], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+42], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+42], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+43]          // k1 * x
v_fma_f32 v4, v[vgprValuC+43], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+43], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+43], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+43], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[40:41], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v44, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v44, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v45, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v45, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+48]          // k1 * x
v_fma_f32 v4, v[vgprValuC+48], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+48], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+48], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+48], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+49]          // k1 * x
v_fma_f32 v4, v[vgprValuC+49], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+49], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+49], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+49], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+50]          // k1 * x
v_fma_f32 v4, v[vgprValuC+50], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+50], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+50], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+50], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+51]          // k1 * x
v_fma_f32 v4, v[vgprValuC+51], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+51], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+51], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+51], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdTD+0], s[sgprSrdTD+0], s66      // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdTD+1], s[sgprSrdTD+1], 0       // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[48:49], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Beta_8:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
label_Activation_End_Beta:
s_branch label_GW_End                              // jump to end
label_GW_B1_E1:

/* edge=1, allocate 6 sgpr. perBatchTmpS=4 perBatchMaskS=2 perElementMaskS=0 elementsPerBatch=12 */
s_cmpk_eq_u32 s[sgprActivationType], 0             // activationType == 0
s_cbranch_scc1 label_Activation_None_Beta_Edge     // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 1             // activationType == 1
s_cbranch_scc1 label_Activation_Abs_Beta_Edge      // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 2             // activationType == 2
s_cbranch_scc1 label_Activation_Clippedrelu_Beta_Edge // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 3             // activationType == 3
s_cbranch_scc1 label_Activation_Gelu_Beta_Edge     // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 4             // activationType == 4
s_cbranch_scc1 label_Activation_Leakyrelu_Beta_Edge // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 5             // activationType == 5
s_cbranch_scc1 label_Activation_Relu_Beta_Edge     // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 6             // activationType == 6
s_cbranch_scc1 label_Activation_Sigmoid_Beta_Edge  // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 7             // activationType == 7
s_cbranch_scc1 label_Activation_Tanh_Beta_Edge     // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 9             // activationType == 9
s_cbranch_scc1 label_Activation_Geluscaling_Beta_Edge // Branch if true
label_Activation_None_Beta_Edge:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v54, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v6, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v54, v6, s[88:89]                // LDC clip if OOB. offset
buffer_load_dwordx2 v[12:13], v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s84
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
v_cndmask_b32 v8, v54, v8, s[88:89]                // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v8 offset:0                 // load bias
v_lshlrev_b32 v9, 0x2, v0                          // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v9, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v54, v6, s[88:89]                // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v54, v7, s[88:89]                // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v10, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v54, v10, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[32:33], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v0, s84
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
v_cndmask_b32 v28, v54, v28, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v29, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v54, v10, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v11, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v54, v11, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v30, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v54, v30, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[44:45], v30, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s84
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_cndmask_b32 v40, v54, v40, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v41, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v30, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v54, v30, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v31, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v54, v31, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v42, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v54, v42, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[56:57], v42, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v0, s84
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
v_cndmask_b32 v52, v54, v52, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v53, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v42, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v54, v42, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v43, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v54, v43, s[88:89]              // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+36], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+37], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+38], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+39], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+48], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+49], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+50], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+51], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+60], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+61], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+62], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+63], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

buffer_store_dwordx4 v[36:39], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

buffer_store_dwordx4 v[48:51], v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

buffer_store_dwordx4 v[60:63], v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s66, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s66, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s66, s66, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s66, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s95, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s94, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s98, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s98, s98, 1                              // Free1
s_mul_hi_u32 s97, s98, s[sgprStrideC1J]            // Free1
s_mul_i32 s96, s98, s[sgprStrideC1J]               // Free1
s_add_u32 s94, s94, s96                            // Free1
s_addc_u32 s95, s95, s97                           // Free1
s_sub_u32 s98, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s98, s98, 1                              // Free2
s_mul_hi_u32 s97, s98, s[sgprStrideCK]             // Free2
s_mul_i32 s96, s98, s[sgprStrideCK]                // Free2
s_add_u32 s94, s94, s96                            // Free2
s_addc_u32 s95, s95, s97                           // Free2
s_lshl_b64 s[90:91], s[94:95], 2                   // scale by bpe

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Beta_Edge_0
// check done end

// buffer load start
buffer_load_dwordx4 v[24:27], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1   // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2   // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3   // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4   // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5   // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6   // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7   // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip    // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip    // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip    // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip    // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip    // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip    // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip    // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add         // Syncbranchhere

label_Synchronizer_read_add_end_7:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip          // SyncAddbranch

label_Synchronizer_read_add_end_6:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip          // SyncAddbranch

label_Synchronizer_read_add_end_5:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip          // SyncAddbranch

label_Synchronizer_read_add_end_4:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip          // SyncAddbranch

label_Synchronizer_read_add_end_3:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip          // SyncAddbranch

label_Synchronizer_read_add_end_2:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip          // SyncAddbranch

label_Synchronizer_read_add_end_1:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[36:39], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_1 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_1 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_1 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_1 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_1 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_1 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_1 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_1:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_1  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_1  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_1  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_1  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_1  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_1  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[192:193]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_1  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_1       // Syncbranchhere

label_Synchronizer_read_add_end_7_1:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_1        // SyncAddbranch

label_Synchronizer_read_add_end_6_1:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_1        // SyncAddbranch

label_Synchronizer_read_add_end_5_1:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_1        // SyncAddbranch

label_Synchronizer_read_add_end_4_1:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_1        // SyncAddbranch

label_Synchronizer_read_add_end_3_1:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_1        // SyncAddbranch

label_Synchronizer_read_add_end_2_1:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_1        // SyncAddbranch

label_Synchronizer_read_add_end_1_1:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_1:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[48:51], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_2 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_2 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_2 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_2 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_2 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_2 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_2 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_2:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_2  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_2  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_2  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_2  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_2  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_2  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[192:193]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_2  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_2       // Syncbranchhere

label_Synchronizer_read_add_end_7_2:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_2        // SyncAddbranch

label_Synchronizer_read_add_end_6_2:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_2        // SyncAddbranch

label_Synchronizer_read_add_end_5_2:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_2        // SyncAddbranch

label_Synchronizer_read_add_end_4_2:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_2        // SyncAddbranch

label_Synchronizer_read_add_end_3_2:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_2        // SyncAddbranch

label_Synchronizer_read_add_end_2_2:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_2        // SyncAddbranch

label_Synchronizer_read_add_end_1_2:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_2:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[60:63], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_3 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_3 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_3 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_3 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_3 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_3 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_3 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_3:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_3  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_3  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_3  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_3  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_3  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[176:177]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_3  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[192:193]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_3  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_3       // Syncbranchhere

label_Synchronizer_read_add_end_7_3:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[176:177]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_3        // SyncAddbranch

label_Synchronizer_read_add_end_6_3:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_3        // SyncAddbranch

label_Synchronizer_read_add_end_5_3:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_3        // SyncAddbranch

label_Synchronizer_read_add_end_4_3:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_3        // SyncAddbranch

label_Synchronizer_read_add_end_3_3:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_3        // SyncAddbranch

label_Synchronizer_read_add_end_2_3:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_3        // SyncAddbranch

label_Synchronizer_read_add_end_1_3:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_3:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v32, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v32, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v33, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v33, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v11, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v44, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v44, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v45, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v45, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v31, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[20:21], v[vgprValuC+60:vgprValuC+60+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[22:23], v[vgprValuC+62:vgprValuC+62+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+60], s[sgprBeta], v56, v[vgprValuC+60] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+61], s[sgprBeta], v56, v[vgprValuC+61] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+62], s[sgprBeta], v57, v[vgprValuC+62] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+63], s[sgprBeta], v57, v[vgprValuC+63] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
buffer_store_dwordx2 v[60:61], v43, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Beta_Edge_0:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta_Edge
label_Activation_Abs_Beta_Edge:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v54, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v6, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v54, v6, s[88:89]                // LDC clip if OOB. offset
buffer_load_dwordx2 v[12:13], v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s84
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
v_cndmask_b32 v8, v54, v8, s[88:89]                // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v8 offset:0                 // load bias
v_lshlrev_b32 v9, 0x2, v0                          // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v9, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v54, v6, s[88:89]                // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v54, v7, s[88:89]                // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v10, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v54, v10, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[32:33], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v0, s84
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
v_cndmask_b32 v28, v54, v28, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v29, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v54, v10, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v11, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v54, v11, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v30, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v54, v30, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[44:45], v30, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s84
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_cndmask_b32 v40, v54, v40, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v41, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v30, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v54, v30, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v31, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v54, v31, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v42, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v54, v42, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[56:57], v42, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v0, s84
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
v_cndmask_b32 v52, v54, v52, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v53, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v42, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v54, v42, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v43, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v54, v43, s[88:89]              // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+36], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+37], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+38], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+39], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+48], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+49], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+50], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+51], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+60], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+61], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+62], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+63], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

buffer_store_dwordx4 v[36:39], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

buffer_store_dwordx4 v[48:51], v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

buffer_store_dwordx4 v[60:63], v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s66, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s66, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s66, s66, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s66, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s95, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s94, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s98, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s98, s98, 1                              // Free1
s_mul_hi_u32 s97, s98, s[sgprStrideC1J]            // Free1
s_mul_i32 s96, s98, s[sgprStrideC1J]               // Free1
s_add_u32 s94, s94, s96                            // Free1
s_addc_u32 s95, s95, s97                           // Free1
s_sub_u32 s98, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s98, s98, 1                              // Free2
s_mul_hi_u32 s97, s98, s[sgprStrideCK]             // Free2
s_mul_i32 s96, s98, s[sgprStrideCK]                // Free2
s_add_u32 s94, s94, s96                            // Free2
s_addc_u32 s95, s95, s97                           // Free2
s_lshl_b64 s[90:91], s[94:95], 2                   // scale by bpe

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Beta_Edge_1
// check done end

// buffer load start
buffer_load_dwordx4 v[24:27], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_4 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_4 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_4 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_4 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_4 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_4 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_4 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_4:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_4  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_4  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_4  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_4  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_4  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_4  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_4  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_4       // Syncbranchhere

label_Synchronizer_read_add_end_7_4:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_4        // SyncAddbranch

label_Synchronizer_read_add_end_6_4:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_4        // SyncAddbranch

label_Synchronizer_read_add_end_5_4:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_4        // SyncAddbranch

label_Synchronizer_read_add_end_4_4:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_4        // SyncAddbranch

label_Synchronizer_read_add_end_3_4:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_4        // SyncAddbranch

label_Synchronizer_read_add_end_2_4:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_4        // SyncAddbranch

label_Synchronizer_read_add_end_1_4:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_4:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[36:39], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_5 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_5 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_5 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_5 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_5 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_5 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_5 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_5:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_5  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_5  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_5  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_5  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_5  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_5  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[192:193]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_5  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_5       // Syncbranchhere

label_Synchronizer_read_add_end_7_5:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_5        // SyncAddbranch

label_Synchronizer_read_add_end_6_5:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_5        // SyncAddbranch

label_Synchronizer_read_add_end_5_5:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_5        // SyncAddbranch

label_Synchronizer_read_add_end_4_5:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_5        // SyncAddbranch

label_Synchronizer_read_add_end_3_5:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_5        // SyncAddbranch

label_Synchronizer_read_add_end_2_5:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_5        // SyncAddbranch

label_Synchronizer_read_add_end_1_5:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_5:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[48:51], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_6 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_6 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_6 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_6 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_6 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_6 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_6 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_6:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_6  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_6  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_6  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_6  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_6  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_6  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[192:193]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_6  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_6       // Syncbranchhere

label_Synchronizer_read_add_end_7_6:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_6        // SyncAddbranch

label_Synchronizer_read_add_end_6_6:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_6        // SyncAddbranch

label_Synchronizer_read_add_end_5_6:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_6        // SyncAddbranch

label_Synchronizer_read_add_end_4_6:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_6        // SyncAddbranch

label_Synchronizer_read_add_end_3_6:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_6        // SyncAddbranch

label_Synchronizer_read_add_end_2_6:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_6        // SyncAddbranch

label_Synchronizer_read_add_end_1_6:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_6:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[60:63], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_7 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_7 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_7 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_7 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_7 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_7 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_7 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_7:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_7  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_7  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_7  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_7  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_7  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[176:177]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_7  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[192:193]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_7  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_7       // Syncbranchhere

label_Synchronizer_read_add_end_7_7:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[176:177]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_7        // SyncAddbranch

label_Synchronizer_read_add_end_6_7:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_7        // SyncAddbranch

label_Synchronizer_read_add_end_5_7:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_7        // SyncAddbranch

label_Synchronizer_read_add_end_4_7:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_7        // SyncAddbranch

label_Synchronizer_read_add_end_3_7:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_7        // SyncAddbranch

label_Synchronizer_read_add_end_2_7:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_7        // SyncAddbranch

label_Synchronizer_read_add_end_1_7:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_7:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_and_b32 v[vgprValuC+24], 0x7fffffff, v[vgprValuC+24] // Remove sign bit
v_and_b32 v[vgprValuC+25], 0x7fffffff, v[vgprValuC+25] // Remove sign bit
v_and_b32 v[vgprValuC+26], 0x7fffffff, v[vgprValuC+26] // Remove sign bit
v_and_b32 v[vgprValuC+27], 0x7fffffff, v[vgprValuC+27] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v32, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v32, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v33, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v33, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_and_b32 v[vgprValuC+36], 0x7fffffff, v[vgprValuC+36] // Remove sign bit
v_and_b32 v[vgprValuC+37], 0x7fffffff, v[vgprValuC+37] // Remove sign bit
v_and_b32 v[vgprValuC+38], 0x7fffffff, v[vgprValuC+38] // Remove sign bit
v_and_b32 v[vgprValuC+39], 0x7fffffff, v[vgprValuC+39] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v11, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v44, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v44, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v45, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v45, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_and_b32 v[vgprValuC+48], 0x7fffffff, v[vgprValuC+48] // Remove sign bit
v_and_b32 v[vgprValuC+49], 0x7fffffff, v[vgprValuC+49] // Remove sign bit
v_and_b32 v[vgprValuC+50], 0x7fffffff, v[vgprValuC+50] // Remove sign bit
v_and_b32 v[vgprValuC+51], 0x7fffffff, v[vgprValuC+51] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v31, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[20:21], v[vgprValuC+60:vgprValuC+60+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[22:23], v[vgprValuC+62:vgprValuC+62+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+60], s[sgprBeta], v56, v[vgprValuC+60] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+61], s[sgprBeta], v56, v[vgprValuC+61] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+62], s[sgprBeta], v57, v[vgprValuC+62] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+63], s[sgprBeta], v57, v[vgprValuC+63] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_and_b32 v[vgprValuC+60], 0x7fffffff, v[vgprValuC+60] // Remove sign bit
v_and_b32 v[vgprValuC+61], 0x7fffffff, v[vgprValuC+61] // Remove sign bit
v_and_b32 v[vgprValuC+62], 0x7fffffff, v[vgprValuC+62] // Remove sign bit
v_and_b32 v[vgprValuC+63], 0x7fffffff, v[vgprValuC+63] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
buffer_store_dwordx2 v[60:61], v43, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Beta_Edge_1:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta_Edge
label_Activation_Clippedrelu_Beta_Edge:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v54, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v6, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v54, v6, s[88:89]                // LDC clip if OOB. offset
buffer_load_dwordx2 v[12:13], v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s84
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
v_cndmask_b32 v8, v54, v8, s[88:89]                // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v8 offset:0                 // load bias
v_lshlrev_b32 v9, 0x2, v0                          // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v9, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v54, v6, s[88:89]                // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v54, v7, s[88:89]                // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v10, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v54, v10, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[32:33], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v0, s84
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
v_cndmask_b32 v28, v54, v28, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v29, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v54, v10, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v11, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v54, v11, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v30, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v54, v30, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[44:45], v30, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s84
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_cndmask_b32 v40, v54, v40, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v41, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v30, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v54, v30, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v31, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v54, v31, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v42, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v54, v42, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[56:57], v42, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v0, s84
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
v_cndmask_b32 v52, v54, v52, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v53, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v42, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v54, v42, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v43, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v54, v43, s[88:89]              // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+36], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+37], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+38], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+39], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+48], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+49], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+50], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+51], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+60], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+61], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+62], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+63], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

buffer_store_dwordx4 v[36:39], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

buffer_store_dwordx4 v[48:51], v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

buffer_store_dwordx4 v[60:63], v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s66, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s66, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s66, s66, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s66, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s95, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s94, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s98, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s98, s98, 1                              // Free1
s_mul_hi_u32 s97, s98, s[sgprStrideC1J]            // Free1
s_mul_i32 s96, s98, s[sgprStrideC1J]               // Free1
s_add_u32 s94, s94, s96                            // Free1
s_addc_u32 s95, s95, s97                           // Free1
s_sub_u32 s98, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s98, s98, 1                              // Free2
s_mul_hi_u32 s97, s98, s[sgprStrideCK]             // Free2
s_mul_i32 s96, s98, s[sgprStrideCK]                // Free2
s_add_u32 s94, s94, s96                            // Free2
s_addc_u32 s95, s95, s97                           // Free2
s_lshl_b64 s[90:91], s[94:95], 2                   // scale by bpe

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Beta_Edge_2
// check done end

// buffer load start
buffer_load_dwordx4 v[24:27], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_8 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_8 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_8 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_8 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_8 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_8 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_8 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_8:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_8  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_8  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_8  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_8  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_8  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_8  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_8  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_8       // Syncbranchhere

label_Synchronizer_read_add_end_7_8:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_8        // SyncAddbranch

label_Synchronizer_read_add_end_6_8:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_8        // SyncAddbranch

label_Synchronizer_read_add_end_5_8:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_8        // SyncAddbranch

label_Synchronizer_read_add_end_4_8:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_8        // SyncAddbranch

label_Synchronizer_read_add_end_3_8:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_8        // SyncAddbranch

label_Synchronizer_read_add_end_2_8:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_8        // SyncAddbranch

label_Synchronizer_read_add_end_1_8:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_8:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[36:39], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_9 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_9 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_9 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_9 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_9 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_9 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_9 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_9:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_9  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_9  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_9  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_9  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_9  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_9  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[192:193]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_9  // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_9       // Syncbranchhere

label_Synchronizer_read_add_end_7_9:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_9        // SyncAddbranch

label_Synchronizer_read_add_end_6_9:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_9        // SyncAddbranch

label_Synchronizer_read_add_end_5_9:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_9        // SyncAddbranch

label_Synchronizer_read_add_end_4_9:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_9        // SyncAddbranch

label_Synchronizer_read_add_end_3_9:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_9        // SyncAddbranch

label_Synchronizer_read_add_end_2_9:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_9        // SyncAddbranch

label_Synchronizer_read_add_end_1_9:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_9:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[48:51], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_10 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_10 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_10 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_10 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_10 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_10 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_10 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_10:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_10 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_10 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_10 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_10 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_10 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_10 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[192:193]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_10 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_10      // Syncbranchhere

label_Synchronizer_read_add_end_7_10:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_10       // SyncAddbranch

label_Synchronizer_read_add_end_6_10:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_10       // SyncAddbranch

label_Synchronizer_read_add_end_5_10:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_10       // SyncAddbranch

label_Synchronizer_read_add_end_4_10:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_10       // SyncAddbranch

label_Synchronizer_read_add_end_3_10:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_10       // SyncAddbranch

label_Synchronizer_read_add_end_2_10:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_10       // SyncAddbranch

label_Synchronizer_read_add_end_1_10:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_10:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[60:63], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_11 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_11 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_11 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_11 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_11 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_11 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_11 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_11:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_11 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_11 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_11 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_11 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_11 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[176:177]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_11 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[192:193]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_11 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_11      // Syncbranchhere

label_Synchronizer_read_add_end_7_11:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[176:177]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_11       // SyncAddbranch

label_Synchronizer_read_add_end_6_11:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_11       // SyncAddbranch

label_Synchronizer_read_add_end_5_11:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_11       // SyncAddbranch

label_Synchronizer_read_add_end_4_11:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_11       // SyncAddbranch

label_Synchronizer_read_add_end_3_11:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_11       // SyncAddbranch

label_Synchronizer_read_add_end_2_11:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_11       // SyncAddbranch

label_Synchronizer_read_add_end_1_11:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_11:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+24], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+24], s[sgpractivationBeta], v[vgprValuC+24] // min(x, beta)
v_cndmask_b32 v[vgprValuC+24], 0.0, v[vgprValuC+24], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+25], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+25], s[sgpractivationBeta], v[vgprValuC+25] // min(x, beta)
v_cndmask_b32 v[vgprValuC+25], 0.0, v[vgprValuC+25], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+26], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+26], s[sgpractivationBeta], v[vgprValuC+26] // min(x, beta)
v_cndmask_b32 v[vgprValuC+26], 0.0, v[vgprValuC+26], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+27], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+27], s[sgpractivationBeta], v[vgprValuC+27] // min(x, beta)
v_cndmask_b32 v[vgprValuC+27], 0.0, v[vgprValuC+27], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v32, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v32, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v33, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v33, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+36], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+36], s[sgpractivationBeta], v[vgprValuC+36] // min(x, beta)
v_cndmask_b32 v[vgprValuC+36], 0.0, v[vgprValuC+36], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+37], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+37], s[sgpractivationBeta], v[vgprValuC+37] // min(x, beta)
v_cndmask_b32 v[vgprValuC+37], 0.0, v[vgprValuC+37], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+38], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+38], s[sgpractivationBeta], v[vgprValuC+38] // min(x, beta)
v_cndmask_b32 v[vgprValuC+38], 0.0, v[vgprValuC+38], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+39], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+39], s[sgpractivationBeta], v[vgprValuC+39] // min(x, beta)
v_cndmask_b32 v[vgprValuC+39], 0.0, v[vgprValuC+39], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v11, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v44, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v44, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v45, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v45, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+48], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+48], s[sgpractivationBeta], v[vgprValuC+48] // min(x, beta)
v_cndmask_b32 v[vgprValuC+48], 0.0, v[vgprValuC+48], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+49], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+49], s[sgpractivationBeta], v[vgprValuC+49] // min(x, beta)
v_cndmask_b32 v[vgprValuC+49], 0.0, v[vgprValuC+49], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+50], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+50], s[sgpractivationBeta], v[vgprValuC+50] // min(x, beta)
v_cndmask_b32 v[vgprValuC+50], 0.0, v[vgprValuC+50], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+51], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+51], s[sgpractivationBeta], v[vgprValuC+51] // min(x, beta)
v_cndmask_b32 v[vgprValuC+51], 0.0, v[vgprValuC+51], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v31, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[20:21], v[vgprValuC+60:vgprValuC+60+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[22:23], v[vgprValuC+62:vgprValuC+62+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+60], s[sgprBeta], v56, v[vgprValuC+60] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+61], s[sgprBeta], v56, v[vgprValuC+61] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+62], s[sgprBeta], v57, v[vgprValuC+62] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+63], s[sgprBeta], v57, v[vgprValuC+63] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+60], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+60], s[sgpractivationBeta], v[vgprValuC+60] // min(x, beta)
v_cndmask_b32 v[vgprValuC+60], 0.0, v[vgprValuC+60], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+61], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+61], s[sgpractivationBeta], v[vgprValuC+61] // min(x, beta)
v_cndmask_b32 v[vgprValuC+61], 0.0, v[vgprValuC+61], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+62], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+62], s[sgpractivationBeta], v[vgprValuC+62] // min(x, beta)
v_cndmask_b32 v[vgprValuC+62], 0.0, v[vgprValuC+62], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+63], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+63], s[sgpractivationBeta], v[vgprValuC+63] // min(x, beta)
v_cndmask_b32 v[vgprValuC+63], 0.0, v[vgprValuC+63], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
buffer_store_dwordx2 v[60:61], v43, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Beta_Edge_2:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta_Edge
label_Activation_Gelu_Beta_Edge:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v54, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v6, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v54, v6, s[88:89]                // LDC clip if OOB. offset
buffer_load_dwordx2 v[12:13], v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s84
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
v_cndmask_b32 v8, v54, v8, s[88:89]                // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v8 offset:0                 // load bias
v_lshlrev_b32 v9, 0x2, v0                          // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v9, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v54, v6, s[88:89]                // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v54, v7, s[88:89]                // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v10, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v54, v10, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[32:33], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v0, s84
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
v_cndmask_b32 v28, v54, v28, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v29, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v54, v10, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v11, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v54, v11, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v30, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v54, v30, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[44:45], v30, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s84
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_cndmask_b32 v40, v54, v40, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v41, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v30, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v54, v30, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v31, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v54, v31, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v42, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v54, v42, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[56:57], v42, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v0, s84
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
v_cndmask_b32 v52, v54, v52, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v53, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v42, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v54, v42, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v43, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v54, v43, s[88:89]              // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+36], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+37], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+38], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+39], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+48], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+49], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+50], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+51], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+60], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+61], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+62], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+63], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

buffer_store_dwordx4 v[36:39], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

buffer_store_dwordx4 v[48:51], v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

buffer_store_dwordx4 v[60:63], v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s66, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s66, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s66, s66, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s66, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s95, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s94, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s98, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s98, s98, 1                              // Free1
s_mul_hi_u32 s97, s98, s[sgprStrideC1J]            // Free1
s_mul_i32 s96, s98, s[sgprStrideC1J]               // Free1
s_add_u32 s94, s94, s96                            // Free1
s_addc_u32 s95, s95, s97                           // Free1
s_sub_u32 s98, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s98, s98, 1                              // Free2
s_mul_hi_u32 s97, s98, s[sgprStrideCK]             // Free2
s_mul_i32 s96, s98, s[sgprStrideCK]                // Free2
s_add_u32 s94, s94, s96                            // Free2
s_addc_u32 s95, s95, s97                           // Free2
s_lshl_b64 s[90:91], s[94:95], 2                   // scale by bpe

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Beta_Edge_3
// check done end

// buffer load start
buffer_load_dwordx4 v[24:27], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_12 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_12 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_12 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_12 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_12 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_12 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_12 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_12:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_12 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_12 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_12 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_12 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_12 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_12 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_12 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_12      // Syncbranchhere

label_Synchronizer_read_add_end_7_12:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_12       // SyncAddbranch

label_Synchronizer_read_add_end_6_12:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_12       // SyncAddbranch

label_Synchronizer_read_add_end_5_12:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_12       // SyncAddbranch

label_Synchronizer_read_add_end_4_12:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_12       // SyncAddbranch

label_Synchronizer_read_add_end_3_12:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_12       // SyncAddbranch

label_Synchronizer_read_add_end_2_12:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_12       // SyncAddbranch

label_Synchronizer_read_add_end_1_12:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_12:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[36:39], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_13 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_13 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_13 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_13 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_13 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_13 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_13 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_13:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_13 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_13 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_13 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_13 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_13 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_13 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[192:193]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_13 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_13      // Syncbranchhere

label_Synchronizer_read_add_end_7_13:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_13       // SyncAddbranch

label_Synchronizer_read_add_end_6_13:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_13       // SyncAddbranch

label_Synchronizer_read_add_end_5_13:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_13       // SyncAddbranch

label_Synchronizer_read_add_end_4_13:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_13       // SyncAddbranch

label_Synchronizer_read_add_end_3_13:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_13       // SyncAddbranch

label_Synchronizer_read_add_end_2_13:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_13       // SyncAddbranch

label_Synchronizer_read_add_end_1_13:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_13:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[48:51], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_14 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_14 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_14 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_14 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_14 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_14 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_14 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_14:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_14 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_14 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_14 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_14 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_14 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_14 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[192:193]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_14 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_14      // Syncbranchhere

label_Synchronizer_read_add_end_7_14:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_14       // SyncAddbranch

label_Synchronizer_read_add_end_6_14:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_14       // SyncAddbranch

label_Synchronizer_read_add_end_5_14:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_14       // SyncAddbranch

label_Synchronizer_read_add_end_4_14:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_14       // SyncAddbranch

label_Synchronizer_read_add_end_3_14:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_14       // SyncAddbranch

label_Synchronizer_read_add_end_2_14:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_14       // SyncAddbranch

label_Synchronizer_read_add_end_1_14:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_14:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[60:63], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_15 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_15 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_15 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_15 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_15 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_15 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_15 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_15:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_15 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_15 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_15 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_15 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_15 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[176:177]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_15 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[192:193]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_15 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_15      // Syncbranchhere

label_Synchronizer_read_add_end_7_15:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[176:177]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_15       // SyncAddbranch

label_Synchronizer_read_add_end_6_15:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_15       // SyncAddbranch

label_Synchronizer_read_add_end_5_15:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_15       // SyncAddbranch

label_Synchronizer_read_add_end_4_15:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_15       // SyncAddbranch

label_Synchronizer_read_add_end_3_15:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_15       // SyncAddbranch

label_Synchronizer_read_add_end_2_15:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_15       // SyncAddbranch

label_Synchronizer_read_add_end_1_15:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_15:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+24]          // k1 * x
v_fma_f32 v4, v[vgprValuC+24], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+24], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+24], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+24], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+25]          // k1 * x
v_fma_f32 v4, v[vgprValuC+25], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+25], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+25], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+25], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+26]          // k1 * x
v_fma_f32 v4, v[vgprValuC+26], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+26], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+26], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+26], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+27]          // k1 * x
v_fma_f32 v4, v[vgprValuC+27], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+27], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+27], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+27], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v32, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v32, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v33, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v33, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+36]          // k1 * x
v_fma_f32 v4, v[vgprValuC+36], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+36], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+36], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+36], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+37]          // k1 * x
v_fma_f32 v4, v[vgprValuC+37], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+37], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+37], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+37], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+38]          // k1 * x
v_fma_f32 v4, v[vgprValuC+38], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+38], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+38], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+38], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+39]          // k1 * x
v_fma_f32 v4, v[vgprValuC+39], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+39], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+39], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+39], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v11, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v44, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v44, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v45, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v45, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+48]          // k1 * x
v_fma_f32 v4, v[vgprValuC+48], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+48], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+48], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+48], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+49]          // k1 * x
v_fma_f32 v4, v[vgprValuC+49], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+49], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+49], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+49], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+50]          // k1 * x
v_fma_f32 v4, v[vgprValuC+50], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+50], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+50], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+50], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+51]          // k1 * x
v_fma_f32 v4, v[vgprValuC+51], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+51], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+51], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+51], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v31, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[20:21], v[vgprValuC+60:vgprValuC+60+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[22:23], v[vgprValuC+62:vgprValuC+62+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+60], s[sgprBeta], v56, v[vgprValuC+60] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+61], s[sgprBeta], v56, v[vgprValuC+61] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+62], s[sgprBeta], v57, v[vgprValuC+62] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+63], s[sgprBeta], v57, v[vgprValuC+63] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+60]          // k1 * x
v_fma_f32 v4, v[vgprValuC+60], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+60], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+60], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+60], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+61]          // k1 * x
v_fma_f32 v4, v[vgprValuC+61], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+61], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+61], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+61], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+62]          // k1 * x
v_fma_f32 v4, v[vgprValuC+62], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+62], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+62], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+62], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+63]          // k1 * x
v_fma_f32 v4, v[vgprValuC+63], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+63], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+63], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+63], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
buffer_store_dwordx2 v[60:61], v43, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Beta_Edge_3:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta_Edge
label_Activation_Leakyrelu_Beta_Edge:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v54, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v6, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v54, v6, s[88:89]                // LDC clip if OOB. offset
buffer_load_dwordx2 v[12:13], v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s84
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
v_cndmask_b32 v8, v54, v8, s[88:89]                // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v8 offset:0                 // load bias
v_lshlrev_b32 v9, 0x2, v0                          // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v9, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v54, v6, s[88:89]                // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v54, v7, s[88:89]                // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v10, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v54, v10, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[32:33], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v0, s84
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
v_cndmask_b32 v28, v54, v28, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v29, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v54, v10, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v11, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v54, v11, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v30, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v54, v30, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[44:45], v30, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s84
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_cndmask_b32 v40, v54, v40, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v41, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v30, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v54, v30, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v31, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v54, v31, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v42, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v54, v42, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[56:57], v42, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v0, s84
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
v_cndmask_b32 v52, v54, v52, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v53, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v42, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v54, v42, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v43, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v54, v43, s[88:89]              // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+36], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+37], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+38], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+39], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+48], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+49], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+50], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+51], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+60], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+61], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+62], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+63], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

buffer_store_dwordx4 v[36:39], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

buffer_store_dwordx4 v[48:51], v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

buffer_store_dwordx4 v[60:63], v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s66, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s66, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s66, s66, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s66, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s95, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s94, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s98, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s98, s98, 1                              // Free1
s_mul_hi_u32 s97, s98, s[sgprStrideC1J]            // Free1
s_mul_i32 s96, s98, s[sgprStrideC1J]               // Free1
s_add_u32 s94, s94, s96                            // Free1
s_addc_u32 s95, s95, s97                           // Free1
s_sub_u32 s98, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s98, s98, 1                              // Free2
s_mul_hi_u32 s97, s98, s[sgprStrideCK]             // Free2
s_mul_i32 s96, s98, s[sgprStrideCK]                // Free2
s_add_u32 s94, s94, s96                            // Free2
s_addc_u32 s95, s95, s97                           // Free2
s_lshl_b64 s[90:91], s[94:95], 2                   // scale by bpe

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Beta_Edge_4
// check done end

// buffer load start
buffer_load_dwordx4 v[24:27], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_16 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_16 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_16 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_16 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_16 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_16 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_16 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_16:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_16 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_16 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_16 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_16 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_16 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_16 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_16 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_16      // Syncbranchhere

label_Synchronizer_read_add_end_7_16:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_16       // SyncAddbranch

label_Synchronizer_read_add_end_6_16:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_16       // SyncAddbranch

label_Synchronizer_read_add_end_5_16:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_16       // SyncAddbranch

label_Synchronizer_read_add_end_4_16:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_16       // SyncAddbranch

label_Synchronizer_read_add_end_3_16:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_16       // SyncAddbranch

label_Synchronizer_read_add_end_2_16:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_16       // SyncAddbranch

label_Synchronizer_read_add_end_1_16:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_16:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[36:39], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_17 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_17 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_17 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_17 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_17 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_17 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_17 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_17:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_17 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_17 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_17 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_17 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_17 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_17 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[192:193]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_17 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_17      // Syncbranchhere

label_Synchronizer_read_add_end_7_17:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_17       // SyncAddbranch

label_Synchronizer_read_add_end_6_17:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_17       // SyncAddbranch

label_Synchronizer_read_add_end_5_17:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_17       // SyncAddbranch

label_Synchronizer_read_add_end_4_17:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_17       // SyncAddbranch

label_Synchronizer_read_add_end_3_17:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_17       // SyncAddbranch

label_Synchronizer_read_add_end_2_17:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_17       // SyncAddbranch

label_Synchronizer_read_add_end_1_17:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_17:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[48:51], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_18 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_18 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_18 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_18 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_18 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_18 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_18 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_18:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_18 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_18 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_18 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_18 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_18 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_18 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[192:193]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_18 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_18      // Syncbranchhere

label_Synchronizer_read_add_end_7_18:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_18       // SyncAddbranch

label_Synchronizer_read_add_end_6_18:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_18       // SyncAddbranch

label_Synchronizer_read_add_end_5_18:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_18       // SyncAddbranch

label_Synchronizer_read_add_end_4_18:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_18       // SyncAddbranch

label_Synchronizer_read_add_end_3_18:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_18       // SyncAddbranch

label_Synchronizer_read_add_end_2_18:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_18       // SyncAddbranch

label_Synchronizer_read_add_end_1_18:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_18:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[60:63], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_19 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_19 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_19 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_19 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_19 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_19 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_19 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_19:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_19 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_19 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_19 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_19 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_19 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[176:177]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_19 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[192:193]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_19 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_19      // Syncbranchhere

label_Synchronizer_read_add_end_7_19:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[176:177]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_19       // SyncAddbranch

label_Synchronizer_read_add_end_6_19:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_19       // SyncAddbranch

label_Synchronizer_read_add_end_5_19:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_19       // SyncAddbranch

label_Synchronizer_read_add_end_4_19:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_19       // SyncAddbranch

label_Synchronizer_read_add_end_3_19:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_19       // SyncAddbranch

label_Synchronizer_read_add_end_2_19:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_19       // SyncAddbranch

label_Synchronizer_read_add_end_1_19:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_19:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+24] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+24], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+24], v4, v[vgprValuC+24], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+25] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+25], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+25], v4, v[vgprValuC+25], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+26] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+26], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+26], v4, v[vgprValuC+26], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+27] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+27], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+27], v4, v[vgprValuC+27], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v32, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v32, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v33, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v33, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+36] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+36], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+36], v4, v[vgprValuC+36], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+37] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+37], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+37], v4, v[vgprValuC+37], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+38] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+38], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+38], v4, v[vgprValuC+38], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+39] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+39], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+39], v4, v[vgprValuC+39], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v11, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v44, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v44, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v45, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v45, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+48] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+48], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+48], v4, v[vgprValuC+48], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+49] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+49], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+49], v4, v[vgprValuC+49], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+50] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+50], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+50], v4, v[vgprValuC+50], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+51] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+51], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+51], v4, v[vgprValuC+51], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v31, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[20:21], v[vgprValuC+60:vgprValuC+60+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[22:23], v[vgprValuC+62:vgprValuC+62+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+60], s[sgprBeta], v56, v[vgprValuC+60] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+61], s[sgprBeta], v56, v[vgprValuC+61] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+62], s[sgprBeta], v57, v[vgprValuC+62] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+63], s[sgprBeta], v57, v[vgprValuC+63] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+60] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+60], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+60], v4, v[vgprValuC+60], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+61] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+61], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+61], v4, v[vgprValuC+61], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+62] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+62], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+62], v4, v[vgprValuC+62], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+63] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+63], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+63], v4, v[vgprValuC+63], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
buffer_store_dwordx2 v[60:61], v43, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Beta_Edge_4:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta_Edge
label_Activation_Relu_Beta_Edge:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v54, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v6, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v54, v6, s[88:89]                // LDC clip if OOB. offset
buffer_load_dwordx2 v[12:13], v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s84
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
v_cndmask_b32 v8, v54, v8, s[88:89]                // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v8 offset:0                 // load bias
v_lshlrev_b32 v9, 0x2, v0                          // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v9, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v54, v6, s[88:89]                // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v54, v7, s[88:89]                // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v10, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v54, v10, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[32:33], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v0, s84
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
v_cndmask_b32 v28, v54, v28, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v29, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v54, v10, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v11, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v54, v11, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v30, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v54, v30, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[44:45], v30, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s84
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_cndmask_b32 v40, v54, v40, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v41, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v30, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v54, v30, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v31, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v54, v31, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v42, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v54, v42, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[56:57], v42, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v0, s84
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
v_cndmask_b32 v52, v54, v52, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v53, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v42, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v54, v42, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v43, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v54, v43, s[88:89]              // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+36], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+37], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+38], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+39], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+48], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+49], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+50], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+51], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+60], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+61], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+62], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+63], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

buffer_store_dwordx4 v[36:39], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

buffer_store_dwordx4 v[48:51], v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

buffer_store_dwordx4 v[60:63], v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s66, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s66, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s66, s66, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s66, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s95, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s94, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s98, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s98, s98, 1                              // Free1
s_mul_hi_u32 s97, s98, s[sgprStrideC1J]            // Free1
s_mul_i32 s96, s98, s[sgprStrideC1J]               // Free1
s_add_u32 s94, s94, s96                            // Free1
s_addc_u32 s95, s95, s97                           // Free1
s_sub_u32 s98, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s98, s98, 1                              // Free2
s_mul_hi_u32 s97, s98, s[sgprStrideCK]             // Free2
s_mul_i32 s96, s98, s[sgprStrideCK]                // Free2
s_add_u32 s94, s94, s96                            // Free2
s_addc_u32 s95, s95, s97                           // Free2
s_lshl_b64 s[90:91], s[94:95], 2                   // scale by bpe

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Beta_Edge_5
// check done end

// buffer load start
buffer_load_dwordx4 v[24:27], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_20 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_20 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_20 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_20 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_20 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_20 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_20 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_20:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_20 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_20 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_20 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_20 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_20 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_20 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_20 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_20      // Syncbranchhere

label_Synchronizer_read_add_end_7_20:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_20       // SyncAddbranch

label_Synchronizer_read_add_end_6_20:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_20       // SyncAddbranch

label_Synchronizer_read_add_end_5_20:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_20       // SyncAddbranch

label_Synchronizer_read_add_end_4_20:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_20       // SyncAddbranch

label_Synchronizer_read_add_end_3_20:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_20       // SyncAddbranch

label_Synchronizer_read_add_end_2_20:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_20       // SyncAddbranch

label_Synchronizer_read_add_end_1_20:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_20:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[36:39], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_21 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_21 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_21 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_21 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_21 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_21 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_21 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_21:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_21 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_21 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_21 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_21 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_21 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_21 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[192:193]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_21 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_21      // Syncbranchhere

label_Synchronizer_read_add_end_7_21:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_21       // SyncAddbranch

label_Synchronizer_read_add_end_6_21:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_21       // SyncAddbranch

label_Synchronizer_read_add_end_5_21:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_21       // SyncAddbranch

label_Synchronizer_read_add_end_4_21:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_21       // SyncAddbranch

label_Synchronizer_read_add_end_3_21:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_21       // SyncAddbranch

label_Synchronizer_read_add_end_2_21:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_21       // SyncAddbranch

label_Synchronizer_read_add_end_1_21:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_21:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[48:51], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_22 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_22 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_22 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_22 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_22 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_22 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_22 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_22:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_22 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_22 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_22 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_22 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_22 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_22 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[192:193]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_22 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_22      // Syncbranchhere

label_Synchronizer_read_add_end_7_22:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_22       // SyncAddbranch

label_Synchronizer_read_add_end_6_22:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_22       // SyncAddbranch

label_Synchronizer_read_add_end_5_22:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_22       // SyncAddbranch

label_Synchronizer_read_add_end_4_22:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_22       // SyncAddbranch

label_Synchronizer_read_add_end_3_22:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_22       // SyncAddbranch

label_Synchronizer_read_add_end_2_22:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_22       // SyncAddbranch

label_Synchronizer_read_add_end_1_22:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_22:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[60:63], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_23 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_23 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_23 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_23 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_23 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_23 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_23 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_23:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_23 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_23 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_23 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_23 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_23 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[176:177]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_23 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[192:193]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_23 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_23      // Syncbranchhere

label_Synchronizer_read_add_end_7_23:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[176:177]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_23       // SyncAddbranch

label_Synchronizer_read_add_end_6_23:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_23       // SyncAddbranch

label_Synchronizer_read_add_end_5_23:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_23       // SyncAddbranch

label_Synchronizer_read_add_end_4_23:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_23       // SyncAddbranch

label_Synchronizer_read_add_end_3_23:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_23       // SyncAddbranch

label_Synchronizer_read_add_end_2_23:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_23       // SyncAddbranch

label_Synchronizer_read_add_end_1_23:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_23:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_max_f32 v[vgprValuC+24], v[vgprValuC+24], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+25], v[vgprValuC+25], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+26], v[vgprValuC+26], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+27], v[vgprValuC+27], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v32, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v32, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v33, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v33, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_max_f32 v[vgprValuC+36], v[vgprValuC+36], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+37], v[vgprValuC+37], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+38], v[vgprValuC+38], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+39], v[vgprValuC+39], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v11, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v44, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v44, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v45, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v45, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_max_f32 v[vgprValuC+48], v[vgprValuC+48], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+49], v[vgprValuC+49], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+50], v[vgprValuC+50], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+51], v[vgprValuC+51], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v31, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[20:21], v[vgprValuC+60:vgprValuC+60+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[22:23], v[vgprValuC+62:vgprValuC+62+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+60], s[sgprBeta], v56, v[vgprValuC+60] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+61], s[sgprBeta], v56, v[vgprValuC+61] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+62], s[sgprBeta], v57, v[vgprValuC+62] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+63], s[sgprBeta], v57, v[vgprValuC+63] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_max_f32 v[vgprValuC+60], v[vgprValuC+60], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+61], v[vgprValuC+61], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+62], v[vgprValuC+62], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+63], v[vgprValuC+63], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
buffer_store_dwordx2 v[60:61], v43, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Beta_Edge_5:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta_Edge
label_Activation_Sigmoid_Beta_Edge:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v54, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v6, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v54, v6, s[88:89]                // LDC clip if OOB. offset
buffer_load_dwordx2 v[12:13], v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s84
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
v_cndmask_b32 v8, v54, v8, s[88:89]                // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v8 offset:0                 // load bias
v_lshlrev_b32 v9, 0x2, v0                          // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v9, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v54, v6, s[88:89]                // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v54, v7, s[88:89]                // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v10, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v54, v10, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[32:33], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v0, s84
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
v_cndmask_b32 v28, v54, v28, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v29, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v54, v10, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v11, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v54, v11, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v30, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v54, v30, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[44:45], v30, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s84
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_cndmask_b32 v40, v54, v40, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v41, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v30, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v54, v30, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v31, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v54, v31, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v42, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v54, v42, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[56:57], v42, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v0, s84
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
v_cndmask_b32 v52, v54, v52, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v53, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v42, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v54, v42, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v43, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v54, v43, s[88:89]              // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+36], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+37], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+38], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+39], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+48], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+49], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+50], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+51], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+60], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+61], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+62], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+63], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

buffer_store_dwordx4 v[36:39], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

buffer_store_dwordx4 v[48:51], v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

buffer_store_dwordx4 v[60:63], v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s66, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s66, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s66, s66, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s66, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s95, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s94, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s98, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s98, s98, 1                              // Free1
s_mul_hi_u32 s97, s98, s[sgprStrideC1J]            // Free1
s_mul_i32 s96, s98, s[sgprStrideC1J]               // Free1
s_add_u32 s94, s94, s96                            // Free1
s_addc_u32 s95, s95, s97                           // Free1
s_sub_u32 s98, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s98, s98, 1                              // Free2
s_mul_hi_u32 s97, s98, s[sgprStrideCK]             // Free2
s_mul_i32 s96, s98, s[sgprStrideCK]                // Free2
s_add_u32 s94, s94, s96                            // Free2
s_addc_u32 s95, s95, s97                           // Free2
s_lshl_b64 s[90:91], s[94:95], 2                   // scale by bpe

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Beta_Edge_6
// check done end

// buffer load start
buffer_load_dwordx4 v[24:27], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_24 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_24 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_24 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_24 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_24 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_24 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_24 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_24:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_24 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_24 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_24 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_24 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_24 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_24 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_24 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_24      // Syncbranchhere

label_Synchronizer_read_add_end_7_24:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_24       // SyncAddbranch

label_Synchronizer_read_add_end_6_24:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_24       // SyncAddbranch

label_Synchronizer_read_add_end_5_24:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_24       // SyncAddbranch

label_Synchronizer_read_add_end_4_24:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_24       // SyncAddbranch

label_Synchronizer_read_add_end_3_24:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_24       // SyncAddbranch

label_Synchronizer_read_add_end_2_24:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_24       // SyncAddbranch

label_Synchronizer_read_add_end_1_24:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_24:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[36:39], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_25 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_25 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_25 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_25 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_25 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_25 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_25 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_25:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_25 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_25 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_25 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_25 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_25 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_25 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[192:193]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_25 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_25      // Syncbranchhere

label_Synchronizer_read_add_end_7_25:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_25       // SyncAddbranch

label_Synchronizer_read_add_end_6_25:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_25       // SyncAddbranch

label_Synchronizer_read_add_end_5_25:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_25       // SyncAddbranch

label_Synchronizer_read_add_end_4_25:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_25       // SyncAddbranch

label_Synchronizer_read_add_end_3_25:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_25       // SyncAddbranch

label_Synchronizer_read_add_end_2_25:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_25       // SyncAddbranch

label_Synchronizer_read_add_end_1_25:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_25:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[48:51], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_26 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_26 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_26 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_26 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_26 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_26 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_26 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_26:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_26 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_26 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_26 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_26 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_26 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_26 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[192:193]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_26 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_26      // Syncbranchhere

label_Synchronizer_read_add_end_7_26:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_26       // SyncAddbranch

label_Synchronizer_read_add_end_6_26:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_26       // SyncAddbranch

label_Synchronizer_read_add_end_5_26:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_26       // SyncAddbranch

label_Synchronizer_read_add_end_4_26:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_26       // SyncAddbranch

label_Synchronizer_read_add_end_3_26:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_26       // SyncAddbranch

label_Synchronizer_read_add_end_2_26:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_26       // SyncAddbranch

label_Synchronizer_read_add_end_1_26:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_26:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[60:63], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_27 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_27 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_27 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_27 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_27 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_27 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_27 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_27:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_27 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_27 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_27 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_27 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_27 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[176:177]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_27 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[192:193]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_27 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_27      // Syncbranchhere

label_Synchronizer_read_add_end_7_27:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[176:177]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_27       // SyncAddbranch

label_Synchronizer_read_add_end_6_27:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_27       // SyncAddbranch

label_Synchronizer_read_add_end_5_27:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_27       // SyncAddbranch

label_Synchronizer_read_add_end_4_27:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_27       // SyncAddbranch

label_Synchronizer_read_add_end_3_27:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_27       // SyncAddbranch

label_Synchronizer_read_add_end_2_27:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_27       // SyncAddbranch

label_Synchronizer_read_add_end_1_27:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_27:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v[vgprValuC+24], 0xbfb8aa3b, v[vgprValuC+24] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+24], v[vgprValuC+24]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+24], 1.0, v[vgprValuC+24]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+24], v[vgprValuC+24]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+25], 0xbfb8aa3b, v[vgprValuC+25] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+25], v[vgprValuC+25]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+25], 1.0, v[vgprValuC+25]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+25], v[vgprValuC+25]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+26], 0xbfb8aa3b, v[vgprValuC+26] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+26], v[vgprValuC+26]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+26], 1.0, v[vgprValuC+26]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+26], v[vgprValuC+26]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+27], 0xbfb8aa3b, v[vgprValuC+27] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+27], v[vgprValuC+27]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+27], 1.0, v[vgprValuC+27]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+27], v[vgprValuC+27]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v32, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v32, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v33, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v33, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v[vgprValuC+36], 0xbfb8aa3b, v[vgprValuC+36] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+36], v[vgprValuC+36]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+36], 1.0, v[vgprValuC+36]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+36], v[vgprValuC+36]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+37], 0xbfb8aa3b, v[vgprValuC+37] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+37], v[vgprValuC+37]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+37], 1.0, v[vgprValuC+37]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+37], v[vgprValuC+37]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+38], 0xbfb8aa3b, v[vgprValuC+38] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+38], v[vgprValuC+38]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+38], 1.0, v[vgprValuC+38]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+38], v[vgprValuC+38]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+39], 0xbfb8aa3b, v[vgprValuC+39] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+39], v[vgprValuC+39]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+39], 1.0, v[vgprValuC+39]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+39], v[vgprValuC+39]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v11, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v44, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v44, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v45, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v45, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_mul_f32 v[vgprValuC+48], 0xbfb8aa3b, v[vgprValuC+48] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+48], v[vgprValuC+48]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+48], 1.0, v[vgprValuC+48]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+48], v[vgprValuC+48]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+49], 0xbfb8aa3b, v[vgprValuC+49] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+49], v[vgprValuC+49]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+49], 1.0, v[vgprValuC+49]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+49], v[vgprValuC+49]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+50], 0xbfb8aa3b, v[vgprValuC+50] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+50], v[vgprValuC+50]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+50], 1.0, v[vgprValuC+50]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+50], v[vgprValuC+50]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+51], 0xbfb8aa3b, v[vgprValuC+51] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+51], v[vgprValuC+51]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+51], 1.0, v[vgprValuC+51]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+51], v[vgprValuC+51]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v31, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[20:21], v[vgprValuC+60:vgprValuC+60+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[22:23], v[vgprValuC+62:vgprValuC+62+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+60], s[sgprBeta], v56, v[vgprValuC+60] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+61], s[sgprBeta], v56, v[vgprValuC+61] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+62], s[sgprBeta], v57, v[vgprValuC+62] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+63], s[sgprBeta], v57, v[vgprValuC+63] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_mul_f32 v[vgprValuC+60], 0xbfb8aa3b, v[vgprValuC+60] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+60], v[vgprValuC+60]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+60], 1.0, v[vgprValuC+60]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+60], v[vgprValuC+60]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+61], 0xbfb8aa3b, v[vgprValuC+61] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+61], v[vgprValuC+61]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+61], 1.0, v[vgprValuC+61]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+61], v[vgprValuC+61]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+62], 0xbfb8aa3b, v[vgprValuC+62] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+62], v[vgprValuC+62]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+62], 1.0, v[vgprValuC+62]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+62], v[vgprValuC+62]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+63], 0xbfb8aa3b, v[vgprValuC+63] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+63], v[vgprValuC+63]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+63], 1.0, v[vgprValuC+63]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+63], v[vgprValuC+63]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
buffer_store_dwordx2 v[60:61], v43, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Beta_Edge_6:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta_Edge
label_Activation_Tanh_Beta_Edge:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v54, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v6, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v54, v6, s[88:89]                // LDC clip if OOB. offset
buffer_load_dwordx2 v[12:13], v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s84
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
v_cndmask_b32 v8, v54, v8, s[88:89]                // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v8 offset:0                 // load bias
v_lshlrev_b32 v9, 0x2, v0                          // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v9, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v54, v6, s[88:89]                // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v54, v7, s[88:89]                // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v10, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v54, v10, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[32:33], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v0, s84
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
v_cndmask_b32 v28, v54, v28, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v29, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v54, v10, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v11, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v54, v11, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v30, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v54, v30, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[44:45], v30, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s84
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_cndmask_b32 v40, v54, v40, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v41, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v30, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v54, v30, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v31, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v54, v31, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v42, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v54, v42, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[56:57], v42, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v0, s84
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
v_cndmask_b32 v52, v54, v52, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v53, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v42, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v54, v42, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v43, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v54, v43, s[88:89]              // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+36], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+37], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+38], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+39], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+48], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+49], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+50], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+51], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+60], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+61], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+62], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+63], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

buffer_store_dwordx4 v[36:39], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

buffer_store_dwordx4 v[48:51], v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

buffer_store_dwordx4 v[60:63], v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s66, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s66, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s66, s66, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s66, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s95, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s94, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s98, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s98, s98, 1                              // Free1
s_mul_hi_u32 s97, s98, s[sgprStrideC1J]            // Free1
s_mul_i32 s96, s98, s[sgprStrideC1J]               // Free1
s_add_u32 s94, s94, s96                            // Free1
s_addc_u32 s95, s95, s97                           // Free1
s_sub_u32 s98, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s98, s98, 1                              // Free2
s_mul_hi_u32 s97, s98, s[sgprStrideCK]             // Free2
s_mul_i32 s96, s98, s[sgprStrideCK]                // Free2
s_add_u32 s94, s94, s96                            // Free2
s_addc_u32 s95, s95, s97                           // Free2
s_lshl_b64 s[90:91], s[94:95], 2                   // scale by bpe

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Beta_Edge_7
// check done end

// buffer load start
buffer_load_dwordx4 v[24:27], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_28 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_28 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_28 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_28 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_28 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_28 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_28 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_28:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_28 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_28 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_28 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_28 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_28 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_28 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_28 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_28      // Syncbranchhere

label_Synchronizer_read_add_end_7_28:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_28       // SyncAddbranch

label_Synchronizer_read_add_end_6_28:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_28       // SyncAddbranch

label_Synchronizer_read_add_end_5_28:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_28       // SyncAddbranch

label_Synchronizer_read_add_end_4_28:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_28       // SyncAddbranch

label_Synchronizer_read_add_end_3_28:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_28       // SyncAddbranch

label_Synchronizer_read_add_end_2_28:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_28       // SyncAddbranch

label_Synchronizer_read_add_end_1_28:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_28:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[36:39], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_29 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_29 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_29 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_29 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_29 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_29 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_29 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_29:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_29 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_29 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_29 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_29 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_29 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_29 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[192:193]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_29 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_29      // Syncbranchhere

label_Synchronizer_read_add_end_7_29:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_29       // SyncAddbranch

label_Synchronizer_read_add_end_6_29:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_29       // SyncAddbranch

label_Synchronizer_read_add_end_5_29:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_29       // SyncAddbranch

label_Synchronizer_read_add_end_4_29:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_29       // SyncAddbranch

label_Synchronizer_read_add_end_3_29:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_29       // SyncAddbranch

label_Synchronizer_read_add_end_2_29:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_29       // SyncAddbranch

label_Synchronizer_read_add_end_1_29:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_29:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[48:51], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_30 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_30 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_30 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_30 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_30 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_30 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_30 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_30:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_30 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_30 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_30 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_30 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_30 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_30 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[192:193]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_30 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_30      // Syncbranchhere

label_Synchronizer_read_add_end_7_30:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_30       // SyncAddbranch

label_Synchronizer_read_add_end_6_30:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_30       // SyncAddbranch

label_Synchronizer_read_add_end_5_30:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_30       // SyncAddbranch

label_Synchronizer_read_add_end_4_30:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_30       // SyncAddbranch

label_Synchronizer_read_add_end_3_30:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_30       // SyncAddbranch

label_Synchronizer_read_add_end_2_30:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_30       // SyncAddbranch

label_Synchronizer_read_add_end_1_30:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_30:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[60:63], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_31 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_31 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_31 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_31 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_31 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_31 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_31 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_31:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_31 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_31 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_31 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_31 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_31 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[176:177]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_31 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[192:193]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_31 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_31      // Syncbranchhere

label_Synchronizer_read_add_end_7_31:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[176:177]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_31       // SyncAddbranch

label_Synchronizer_read_add_end_6_31:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_31       // SyncAddbranch

label_Synchronizer_read_add_end_5_31:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_31       // SyncAddbranch

label_Synchronizer_read_add_end_4_31:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_31       // SyncAddbranch

label_Synchronizer_read_add_end_3_31:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_31       // SyncAddbranch

label_Synchronizer_read_add_end_2_31:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_31       // SyncAddbranch

label_Synchronizer_read_add_end_1_31:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_31:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v[vgprValuC+24], s[sgpractivationAlpha], v[vgprValuC+24] // x * alpha
v_mul_f32 v[vgprValuC+24], 0x4038aa3b, v[vgprValuC+24] //  (fused 2)
v_exp_f32 v[vgprValuC+24], v[vgprValuC+24]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+24], 1.0, v[vgprValuC+24]    // e^2x + 1
v_rcp_f32 v[vgprValuC+24], v[vgprValuC+24]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+24], -2.0, v[vgprValuC+24], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+24], s[sgpractivationBeta], v[vgprValuC+24] // beta * tanh(x)
v_mul_f32 v[vgprValuC+25], s[sgpractivationAlpha], v[vgprValuC+25] // x * alpha
v_mul_f32 v[vgprValuC+25], 0x4038aa3b, v[vgprValuC+25] //  (fused 2)
v_exp_f32 v[vgprValuC+25], v[vgprValuC+25]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+25], 1.0, v[vgprValuC+25]    // e^2x + 1
v_rcp_f32 v[vgprValuC+25], v[vgprValuC+25]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+25], -2.0, v[vgprValuC+25], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+25], s[sgpractivationBeta], v[vgprValuC+25] // beta * tanh(x)
v_mul_f32 v[vgprValuC+26], s[sgpractivationAlpha], v[vgprValuC+26] // x * alpha
v_mul_f32 v[vgprValuC+26], 0x4038aa3b, v[vgprValuC+26] //  (fused 2)
v_exp_f32 v[vgprValuC+26], v[vgprValuC+26]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+26], 1.0, v[vgprValuC+26]    // e^2x + 1
v_rcp_f32 v[vgprValuC+26], v[vgprValuC+26]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+26], -2.0, v[vgprValuC+26], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+26], s[sgpractivationBeta], v[vgprValuC+26] // beta * tanh(x)
v_mul_f32 v[vgprValuC+27], s[sgpractivationAlpha], v[vgprValuC+27] // x * alpha
v_mul_f32 v[vgprValuC+27], 0x4038aa3b, v[vgprValuC+27] //  (fused 2)
v_exp_f32 v[vgprValuC+27], v[vgprValuC+27]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+27], 1.0, v[vgprValuC+27]    // e^2x + 1
v_rcp_f32 v[vgprValuC+27], v[vgprValuC+27]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+27], -2.0, v[vgprValuC+27], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+27], s[sgpractivationBeta], v[vgprValuC+27] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v32, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v32, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v33, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v33, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v[vgprValuC+36], s[sgpractivationAlpha], v[vgprValuC+36] // x * alpha
v_mul_f32 v[vgprValuC+36], 0x4038aa3b, v[vgprValuC+36] //  (fused 2)
v_exp_f32 v[vgprValuC+36], v[vgprValuC+36]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+36], 1.0, v[vgprValuC+36]    // e^2x + 1
v_rcp_f32 v[vgprValuC+36], v[vgprValuC+36]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+36], -2.0, v[vgprValuC+36], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+36], s[sgpractivationBeta], v[vgprValuC+36] // beta * tanh(x)
v_mul_f32 v[vgprValuC+37], s[sgpractivationAlpha], v[vgprValuC+37] // x * alpha
v_mul_f32 v[vgprValuC+37], 0x4038aa3b, v[vgprValuC+37] //  (fused 2)
v_exp_f32 v[vgprValuC+37], v[vgprValuC+37]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+37], 1.0, v[vgprValuC+37]    // e^2x + 1
v_rcp_f32 v[vgprValuC+37], v[vgprValuC+37]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+37], -2.0, v[vgprValuC+37], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+37], s[sgpractivationBeta], v[vgprValuC+37] // beta * tanh(x)
v_mul_f32 v[vgprValuC+38], s[sgpractivationAlpha], v[vgprValuC+38] // x * alpha
v_mul_f32 v[vgprValuC+38], 0x4038aa3b, v[vgprValuC+38] //  (fused 2)
v_exp_f32 v[vgprValuC+38], v[vgprValuC+38]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+38], 1.0, v[vgprValuC+38]    // e^2x + 1
v_rcp_f32 v[vgprValuC+38], v[vgprValuC+38]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+38], -2.0, v[vgprValuC+38], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+38], s[sgpractivationBeta], v[vgprValuC+38] // beta * tanh(x)
v_mul_f32 v[vgprValuC+39], s[sgpractivationAlpha], v[vgprValuC+39] // x * alpha
v_mul_f32 v[vgprValuC+39], 0x4038aa3b, v[vgprValuC+39] //  (fused 2)
v_exp_f32 v[vgprValuC+39], v[vgprValuC+39]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+39], 1.0, v[vgprValuC+39]    // e^2x + 1
v_rcp_f32 v[vgprValuC+39], v[vgprValuC+39]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+39], -2.0, v[vgprValuC+39], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+39], s[sgpractivationBeta], v[vgprValuC+39] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v11, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v44, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v44, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v45, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v45, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_mul_f32 v[vgprValuC+48], s[sgpractivationAlpha], v[vgprValuC+48] // x * alpha
v_mul_f32 v[vgprValuC+48], 0x4038aa3b, v[vgprValuC+48] //  (fused 2)
v_exp_f32 v[vgprValuC+48], v[vgprValuC+48]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+48], 1.0, v[vgprValuC+48]    // e^2x + 1
v_rcp_f32 v[vgprValuC+48], v[vgprValuC+48]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+48], -2.0, v[vgprValuC+48], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+48], s[sgpractivationBeta], v[vgprValuC+48] // beta * tanh(x)
v_mul_f32 v[vgprValuC+49], s[sgpractivationAlpha], v[vgprValuC+49] // x * alpha
v_mul_f32 v[vgprValuC+49], 0x4038aa3b, v[vgprValuC+49] //  (fused 2)
v_exp_f32 v[vgprValuC+49], v[vgprValuC+49]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+49], 1.0, v[vgprValuC+49]    // e^2x + 1
v_rcp_f32 v[vgprValuC+49], v[vgprValuC+49]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+49], -2.0, v[vgprValuC+49], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+49], s[sgpractivationBeta], v[vgprValuC+49] // beta * tanh(x)
v_mul_f32 v[vgprValuC+50], s[sgpractivationAlpha], v[vgprValuC+50] // x * alpha
v_mul_f32 v[vgprValuC+50], 0x4038aa3b, v[vgprValuC+50] //  (fused 2)
v_exp_f32 v[vgprValuC+50], v[vgprValuC+50]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+50], 1.0, v[vgprValuC+50]    // e^2x + 1
v_rcp_f32 v[vgprValuC+50], v[vgprValuC+50]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+50], -2.0, v[vgprValuC+50], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+50], s[sgpractivationBeta], v[vgprValuC+50] // beta * tanh(x)
v_mul_f32 v[vgprValuC+51], s[sgpractivationAlpha], v[vgprValuC+51] // x * alpha
v_mul_f32 v[vgprValuC+51], 0x4038aa3b, v[vgprValuC+51] //  (fused 2)
v_exp_f32 v[vgprValuC+51], v[vgprValuC+51]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+51], 1.0, v[vgprValuC+51]    // e^2x + 1
v_rcp_f32 v[vgprValuC+51], v[vgprValuC+51]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+51], -2.0, v[vgprValuC+51], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+51], s[sgpractivationBeta], v[vgprValuC+51] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v31, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[20:21], v[vgprValuC+60:vgprValuC+60+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[22:23], v[vgprValuC+62:vgprValuC+62+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+60], s[sgprBeta], v56, v[vgprValuC+60] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+61], s[sgprBeta], v56, v[vgprValuC+61] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+62], s[sgprBeta], v57, v[vgprValuC+62] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+63], s[sgprBeta], v57, v[vgprValuC+63] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_mul_f32 v[vgprValuC+60], s[sgpractivationAlpha], v[vgprValuC+60] // x * alpha
v_mul_f32 v[vgprValuC+60], 0x4038aa3b, v[vgprValuC+60] //  (fused 2)
v_exp_f32 v[vgprValuC+60], v[vgprValuC+60]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+60], 1.0, v[vgprValuC+60]    // e^2x + 1
v_rcp_f32 v[vgprValuC+60], v[vgprValuC+60]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+60], -2.0, v[vgprValuC+60], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+60], s[sgpractivationBeta], v[vgprValuC+60] // beta * tanh(x)
v_mul_f32 v[vgprValuC+61], s[sgpractivationAlpha], v[vgprValuC+61] // x * alpha
v_mul_f32 v[vgprValuC+61], 0x4038aa3b, v[vgprValuC+61] //  (fused 2)
v_exp_f32 v[vgprValuC+61], v[vgprValuC+61]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+61], 1.0, v[vgprValuC+61]    // e^2x + 1
v_rcp_f32 v[vgprValuC+61], v[vgprValuC+61]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+61], -2.0, v[vgprValuC+61], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+61], s[sgpractivationBeta], v[vgprValuC+61] // beta * tanh(x)
v_mul_f32 v[vgprValuC+62], s[sgpractivationAlpha], v[vgprValuC+62] // x * alpha
v_mul_f32 v[vgprValuC+62], 0x4038aa3b, v[vgprValuC+62] //  (fused 2)
v_exp_f32 v[vgprValuC+62], v[vgprValuC+62]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+62], 1.0, v[vgprValuC+62]    // e^2x + 1
v_rcp_f32 v[vgprValuC+62], v[vgprValuC+62]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+62], -2.0, v[vgprValuC+62], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+62], s[sgpractivationBeta], v[vgprValuC+62] // beta * tanh(x)
v_mul_f32 v[vgprValuC+63], s[sgpractivationAlpha], v[vgprValuC+63] // x * alpha
v_mul_f32 v[vgprValuC+63], 0x4038aa3b, v[vgprValuC+63] //  (fused 2)
v_exp_f32 v[vgprValuC+63], v[vgprValuC+63]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+63], 1.0, v[vgprValuC+63]    // e^2x + 1
v_rcp_f32 v[vgprValuC+63], v[vgprValuC+63]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+63], -2.0, v[vgprValuC+63], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+63], s[sgpractivationBeta], v[vgprValuC+63] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
buffer_store_dwordx2 v[60:61], v43, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Beta_Edge_7:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta_Edge
label_Activation_Geluscaling_Beta_Edge:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v54, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v6, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v54, v6, s[88:89]                // LDC clip if OOB. offset
buffer_load_dwordx2 v[12:13], v6, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v8, v0, s84
v_lshlrev_b32 v8, 0x2, v8                          // Bias address scaled by BPE
v_cndmask_b32 v8, v54, v8, s[88:89]                // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v8 offset:0                 // load bias
v_lshlrev_b32 v9, 0x2, v0                          // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v9, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v6, v3, v0, 0x2                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v6, v54, v6, s[88:89]                // LDD clip if OOB. offset
v_add_lshl_u32 v7, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v7, v54, v7, s[88:89]                // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v10, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v54, v10, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[32:33], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v0, s84
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
v_cndmask_b32 v28, v54, v28, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v29, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v54, v10, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v11, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v54, v11, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v30, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v54, v30, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[44:45], v30, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s84
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_cndmask_b32 v40, v54, v40, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v41, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v30, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v54, v30, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v31, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v54, v31, s[88:89]              // LDTD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v42, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v54, v42, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[56:57], v42, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v0, s84
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
v_cndmask_b32 v52, v54, v52, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v53, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v42, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v54, v42, s[88:89]              // LDD clip if OOB. offset
v_add_lshl_u32 v43, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v54, v43, s[88:89]              // LDTD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+36], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+37], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+38], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+39], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+48], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+49], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+50], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+51], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+60], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+61], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+62], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+63], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: MultipleBufferSingleKernel */

/* GlobalSplitU: 2 */

buffer_store_dwordx4 v[24:27], v6, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 0

buffer_store_dwordx4 v[36:39], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 1

buffer_store_dwordx4 v[48:51], v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 2

buffer_store_dwordx4 v[60:63], v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D 3
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 2

//s_endpgm
// check done start
// synchronizer offset cal
s_mul_i32 s66, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s51, s66, s[sgprWorkGroup2]
s_mul_i32 s65, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_add_u32 s65, s65, s[sgprWorkGroup0]
s_add_u32 s65, s65, s51
v_readfirstlane_b32 s51, v[vgprSerial]
s_mul_i32 s66, s66, s[sgprSizeK]                   // cal a wave offset
s_lshr_b32 s51, s51, 0x6
s_mul_i32 s51, s66, s51                            // wave offset at batch
s_add_u32 s65, s51, s65
s_lshl_b32 s65, s65, 0x2
s_add_u32 s[sgprSrdSync+0], s[sgprSynchronizer+0], s65
s_addc_u32 s[sgprSrdSync+1], s[sgprSynchronizer+1], 0x0
s_waitcnt 0                                        // (Wait all)
s_sub_u32 s51, s[sgprGSU], 0x1
s_atomic_dec s51, s[sgprSrdSync:sgprSrdSync+1],  glc

// synchronizer sum offset cal
s_mul_hi_u32 s95, s[sgprSizesFree+0], 1            // Free0
s_mul_i32 s94, s[sgprSizesFree+0], 1               // Free0
s_sub_u32 s98, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s98, s98, 1                              // Free1
s_mul_hi_u32 s97, s98, s[sgprStrideC1J]            // Free1
s_mul_i32 s96, s98, s[sgprStrideC1J]               // Free1
s_add_u32 s94, s94, s96                            // Free1
s_addc_u32 s95, s95, s97                           // Free1
s_sub_u32 s98, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s98, s98, 1                              // Free2
s_mul_hi_u32 s97, s98, s[sgprStrideCK]             // Free2
s_mul_i32 s96, s98, s[sgprStrideCK]                // Free2
s_add_u32 s94, s94, s96                            // Free2
s_addc_u32 s95, s95, s97                           // Free2
s_lshl_b64 s[90:91], s[94:95], 2                   // scale by bpe

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
// check synchronizer done
s_waitcnt lgkmcnt(0)                               // Wait for synchronizer
s_cmp_eq_u32 s51, 0x1
s_cbranch_scc0 label_Sync_EDN_Beta_Edge_8
// check done end

// buffer load start
buffer_load_dwordx4 v[24:27], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_32 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_32 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_32 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_32 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_32 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_32 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_32 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v6, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_32:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_32 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_32 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_32 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_32 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_32 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_32 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[192:193]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_32 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v6, v54, s[92:93]               // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_32      // Syncbranchhere

label_Synchronizer_read_add_end_7_32:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[176:177]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_32       // SyncAddbranch

label_Synchronizer_read_add_end_6_32:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[160:161]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_32       // SyncAddbranch

label_Synchronizer_read_add_end_5_32:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[144:145]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_32       // SyncAddbranch

label_Synchronizer_read_add_end_4_32:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[128:129]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_32       // SyncAddbranch

label_Synchronizer_read_add_end_3_32:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[112:113]        // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_32       // SyncAddbranch

label_Synchronizer_read_add_end_2_32:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[24:25], v[24:25], v[96:97]          // buffer pk
v_pk_add_f32 v[26:27], v[26:27], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_32       // SyncAddbranch

label_Synchronizer_read_add_end_1_32:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_32:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[36:39], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_33 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_33 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_33 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_33 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_33 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_33 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_33 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v10, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_33:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_33 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_33 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_33 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_33 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_33 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_33 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[192:193]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_33 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v10, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_33      // Syncbranchhere

label_Synchronizer_read_add_end_7_33:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[176:177]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_33       // SyncAddbranch

label_Synchronizer_read_add_end_6_33:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[160:161]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_33       // SyncAddbranch

label_Synchronizer_read_add_end_5_33:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[144:145]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_33       // SyncAddbranch

label_Synchronizer_read_add_end_4_33:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[128:129]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_33       // SyncAddbranch

label_Synchronizer_read_add_end_3_33:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[112:113]        // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_33       // SyncAddbranch

label_Synchronizer_read_add_end_2_33:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[36:37], v[36:37], v[96:97]          // buffer pk
v_pk_add_f32 v[38:39], v[38:39], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_33       // SyncAddbranch

label_Synchronizer_read_add_end_1_33:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_33:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[48:51], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_34 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_34 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_34 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_34 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_34 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_34 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_34 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v30, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_34:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_34 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_34 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_34 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_34 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_34 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_34 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[192:193]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_34 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v30, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_34      // Syncbranchhere

label_Synchronizer_read_add_end_7_34:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[176:177]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_34       // SyncAddbranch

label_Synchronizer_read_add_end_6_34:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[160:161]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_34       // SyncAddbranch

label_Synchronizer_read_add_end_5_34:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[144:145]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_34       // SyncAddbranch

label_Synchronizer_read_add_end_4_34:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[128:129]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_34       // SyncAddbranch

label_Synchronizer_read_add_end_3_34:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[112:113]        // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_34       // SyncAddbranch

label_Synchronizer_read_add_end_2_34:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[48:49], v[48:49], v[96:97]          // buffer pk
v_pk_add_f32 v[50:51], v[50:51], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_34       // SyncAddbranch

label_Synchronizer_read_add_end_1_34:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_34:  /// Synchronizer read add skip
// buffer add end2

v_mov_b32 v54, BufferOOB
s_mov_b32 s96, s[sgprWSDstart+0]                   // Move workspace start
s_mov_b32 s97, s[sgprWSDstart+1]                   // Move workspace start
s_mov_b32 s98, s[sgprSrdD+2]
s_mov_b32 s99, s[sgprSrdD+3]
buffer_load_dwordx4 v[60:63], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU D 0
s_mov_b32 s[sgprGSUSync], s[sgprGSU]
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_1_35 // SyncAddbranchhere
buffer_load_dwordx4 v[96:99], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_2_35 // SyncAddbranchhere
buffer_load_dwordx4 v[112:115], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_3_35 // SyncAddbranchhere
buffer_load_dwordx4 v[128:131], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_4_35 // SyncAddbranchhere
buffer_load_dwordx4 v[144:147], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_5_35 // SyncAddbranchhere
buffer_load_dwordx4 v[160:163], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_6_35 // SyncAddbranchhere
buffer_load_dwordx4 v[176:179], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
s_cmp_eq_i32 s[sgprGSUSync], 0
s_cbranch_scc1 label_Synchronizer_read_add_end_7_35 // SyncAddbranchhere
buffer_load_dwordx4 v[192:195], v42, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer load end

// 96
// 112
// buffer add start
label_Synchronizer_read_add_35:  /// Synchronizer read add

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 0
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_35 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[96:99], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 1
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_35 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[112:115], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 2
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_35 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[128:131], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 3
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_35 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[144:147], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 4
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_35 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[160:163], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[176:177]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[178:179]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 5
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_35 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[176:179], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD

s_waitcnt vmcnt(6)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[192:193]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[194:195]        // buffer pk
s_sub_i32 s[sgprGSUSync], s[sgprGSUSync], 1        // 6
s_cmp_le_i32 s[sgprGSUSync], -6
s_cbranch_scc1 label_Synchronizer_read_add_skip_35 // SyncAddbranch
s_add_u32 s96, s96, s90
s_addc_u32 s97, s97, s91
v_cmp_ge_i32 s[92:93], 0, s[sgprGSUSync]
v_cndmask_b32 v55, v42, v54, s[92:93]              // 1. mul 1 if 0
buffer_load_dwordx4 v[192:195], v55, s[96:99], 0 offen offset:0, sc0 sc1 // load GSU DD
// buffer add end

s_cmp_gt_i32 s[sgprGSUSync], -0x6
s_cbranch_scc1 label_Synchronizer_read_add_35      // Syncbranchhere

label_Synchronizer_read_add_end_7_35:  /// Synchronizer read add end_7

s_waitcnt vmcnt(5)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[176:177]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[178:179]        // buffer pk
s_branch label_Synchronizer_read_add_skip_35       // SyncAddbranch

label_Synchronizer_read_add_end_6_35:  /// Synchronizer read add end_6

s_waitcnt vmcnt(4)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[160:161]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[162:163]        // buffer pk
s_branch label_Synchronizer_read_add_skip_35       // SyncAddbranch

label_Synchronizer_read_add_end_5_35:  /// Synchronizer read add end_5

s_waitcnt vmcnt(3)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[144:145]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[146:147]        // buffer pk
s_branch label_Synchronizer_read_add_skip_35       // SyncAddbranch

label_Synchronizer_read_add_end_4_35:  /// Synchronizer read add end_4

s_waitcnt vmcnt(2)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[128:129]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[130:131]        // buffer pk
s_branch label_Synchronizer_read_add_skip_35       // SyncAddbranch

label_Synchronizer_read_add_end_3_35:  /// Synchronizer read add end_3

s_waitcnt vmcnt(1)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[112:113]        // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[114:115]        // buffer pk
s_branch label_Synchronizer_read_add_skip_35       // SyncAddbranch

label_Synchronizer_read_add_end_2_35:  /// Synchronizer read add end_2

s_waitcnt vmcnt(0)                                 // (wait for buffer ready)
v_pk_add_f32 v[60:61], v[60:61], v[96:97]          // buffer pk
v_pk_add_f32 v[62:63], v[62:63], v[98:99]          // buffer pk
s_branch label_Synchronizer_read_add_skip_35       // SyncAddbranch

label_Synchronizer_read_add_end_1_35:  /// Synchronizer read add end_1
label_Synchronizer_read_add_skip_35:  /// Synchronizer read add skip
// buffer add end2


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+24]          // k1 * x
v_fma_f32 v4, v[vgprValuC+24], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+24], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+24], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+24], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+25]          // k1 * x
v_fma_f32 v4, v[vgprValuC+25], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+25], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+25], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+25], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+26]          // k1 * x
v_fma_f32 v4, v[vgprValuC+26], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+26], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+26], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+26], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+27]          // k1 * x
v_fma_f32 v4, v[vgprValuC+27], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+27], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+27], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+27], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v7, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v32, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v32, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v33, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v33, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+36]          // k1 * x
v_fma_f32 v4, v[vgprValuC+36], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+36], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+36], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+36], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+37]          // k1 * x
v_fma_f32 v4, v[vgprValuC+37], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+37], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+37], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+37], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+38]          // k1 * x
v_fma_f32 v4, v[vgprValuC+38], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+38], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+38], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+38], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+39]          // k1 * x
v_fma_f32 v4, v[vgprValuC+39], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+39], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+39], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+39], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v11, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+48], s[sgprBeta], v44, v[vgprValuC+48] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+49], s[sgprBeta], v44, v[vgprValuC+49] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+50], s[sgprBeta], v45, v[vgprValuC+50] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+51], s[sgprBeta], v45, v[vgprValuC+51] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+48:vgprValuC+48+1], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[vgprValuC+50:vgprValuC+50+1], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+48]          // k1 * x
v_fma_f32 v4, v[vgprValuC+48], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+48], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+48], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+48], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+49]          // k1 * x
v_fma_f32 v4, v[vgprValuC+49], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+49], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+49], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+49], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+50]          // k1 * x
v_fma_f32 v4, v[vgprValuC+50], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+50], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+50], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+50], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+51]          // k1 * x
v_fma_f32 v4, v[vgprValuC+51], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+51], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+51], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+51], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+48], v[vgprValuC+48]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+49], v[vgprValuC+49]     // convert C to fp16
v_pack_b32_f16 v48, v[vgprValuC+48], v[vgprValuC+49] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+50], v[vgprValuC+50]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+51], v[vgprValuC+51]     // convert C to fp16
v_pack_b32_f16 v49, v[vgprValuC+50], v[vgprValuC+51] // Pack with neighbor
buffer_store_dwordx2 v[48:49], v31, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[20:21], v[vgprValuC+60:vgprValuC+60+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[22:23], v[vgprValuC+62:vgprValuC+62+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+60], s[sgprBeta], v56, v[vgprValuC+60] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+61], s[sgprBeta], v56, v[vgprValuC+61] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+62], s[sgprBeta], v57, v[vgprValuC+62] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+63], s[sgprBeta], v57, v[vgprValuC+63] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+60:vgprValuC+60+1], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[vgprValuC+62:vgprValuC+62+1], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+60]          // k1 * x
v_fma_f32 v4, v[vgprValuC+60], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+60], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+60], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+60], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+61]          // k1 * x
v_fma_f32 v4, v[vgprValuC+61], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+61], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+61], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+61], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+62]          // k1 * x
v_fma_f32 v4, v[vgprValuC+62], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+62], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+62], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+62], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+63]          // k1 * x
v_fma_f32 v4, v[vgprValuC+63], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+63], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+63], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+63], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+60], v[vgprValuC+60]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+61], v[vgprValuC+61]     // convert C to fp16
v_pack_b32_f16 v60, v[vgprValuC+60], v[vgprValuC+61] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+62], v[vgprValuC+62]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+63], v[vgprValuC+63]     // convert C to fp16
v_pack_b32_f16 v61, v[vgprValuC+62], v[vgprValuC+63] // Pack with neighbor
buffer_store_dwordx2 v[60:61], v43, s[sgprSrdTD:sgprSrdTD+3], 0 offen offset:0, sc0 sc1 // store TD not StoreRemapVectorWidth
// synchronizer store
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
label_Sync_EDN_Beta_Edge_8:  /// Sync_EDN

//synchronizer store end

s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
label_Activation_End_Beta_Edge:
s_branch label_GW_End                              // jump to end
label_GW_End:
s_getpc_b64 s[84:85]                               // addr of next instr
s_add_i32 s86, label_KernelEnd, 0x4                // target branch offset
s_add_u32 s84, s84, s86                            // add target branch offset
s_addc_u32 s85, s85, 0                             // add high and carry
s_setpc_b64 s[84:85]                               // branch to label_KernelEnd
label_GSU_5:
s_mov_b32 s51, 1.0                                 // init as 1
s_cmp_eq_u64 s[sgprAddressScaleA:sgprAddressScaleA+1], 0 // s[AddressScaleA] == 0 ?
s_cbranch_scc1 label_ScaleAValid_1                 // branch if s[AddressScaleA] == 0
s_load_dword s51, s[sgprAddressScaleA:sgprAddressScaleA+1], 0 // load scaleA
label_ScaleAValid_1:
s_mov_b32 s65, 1.0                                 // init as 1
s_cmp_eq_u64 s[sgprAddressScaleB:sgprAddressScaleB+1], 0 // s[AddressScaleB] == 0 ?
s_cbranch_scc1 label_ScaleBValid_1                 // branch if s[AddressScaleB] == 0
s_load_dword s65, s[sgprAddressScaleB:sgprAddressScaleB+1], 0 // load scaleB
label_ScaleBValid_1:
s_mov_b32 s[sgprSrdScaleAlphaVec+0], s[sgprAddressScaleAlphaVec+0] // init SRD base address (lower)
s_mov_b32 s[sgprSrdScaleAlphaVec+1], s[sgprAddressScaleAlphaVec+1] // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdScaleAlphaVec+3], Srd127_96     // Set bits 127_96 in post-loop SRD
s_cmp_eq_u64 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], 0 // s[AddressScaleAlphaVec] == 0 ?
s_cbranch_scc0 label_ScaleAlphaVec_1AddrValid      // branch if s[AddressScaleAlphaVec] != 0
s_mov_b32 s[sgprSrdScaleAlphaVec+2], 0
s_branch label_ScaleAlphaVec_1AddrValid_End
label_ScaleAlphaVec_1AddrValid:
s_mov_b32 s[sgprSrdScaleAlphaVec+2], s[sgprSizeI]
label_ScaleAlphaVec_1AddrValid_End:

s_mul_i32 s[sgprSrdScaleAlphaVec+2], 0x4, s[sgprSrdScaleAlphaVec+2] // ScaleAlphaVec scaled by BPE
s_add_u32 s66, s[sgprWorkGroup2], 0x1
s_mul_i32 s66, s[sgprBiasStride], s66              // stride * (wg+1)
s_cmp_eq_u32 s66, 0x0                              // bias stride = 0?
s_cselect_b32 s66, s[sgprSizeI], s66
s_mov_b32 s[sgprSrdBias+0], s[sgprAddressBias+0]   // init SRD base address (lower)
s_mov_b32 s[sgprSrdBias+1], s[sgprAddressBias+1]   // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdBias+3], Srd127_96              // Set bits 127_96 in post-loop SRD
s_cmp_eq_u64 s[sgprAddressBias:sgprAddressBias+1], 0 // s[AddressBias] == 0 ?
s_cbranch_scc0 label_Bias_1AddrValid               // branch if s[AddressBias] != 0
s_mov_b32 s[sgprSrdBias+2], 0
s_branch label_Bias_1AddrValid_End
label_Bias_1AddrValid:
s_mov_b32 s[sgprSrdBias+2], s66
label_Bias_1AddrValid_End:

label_Load_Biasf32_1:
s_cmpk_lg_u32 s[sgprBiasType], 0                   // BiasType != 0
s_cbranch_scc1 label_Load_Biasf16_1                // Branch if true

/******************************************/
/* Read Bias to LDS                       */
/******************************************/
s_mul_i32 s[sgprSrdBias+2], 0x4, s[sgprSrdBias+2]  // scaled by BPE
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_add_u32 v8, s66, v[vgprSerial]                   // coord 0 = wgp0 * MT0 + thread offset
s_mul_i32 s66, s[sgprBiasStride], s[sgprWorkGroup2] // Stride * WG
v_add_u32 v8, s66, v8                              // coord 0 = wgp0 * MT0 + thread offset + Stride * WG
v_lshlrev_b32 v8, 0x2, v8                          // Global bias address scaled by BPE
buffer_load_dword v4, v8, s[sgprSrdBias:sgprSrdBias+3], 0 offen offset:0 // load bias
v_lshlrev_b32 v8, 0x2, v[vgprSerial]               // Local bias address scaled by BPE
s_waitcnt vmcnt(0)                                 // wait for bias load
s_barrier                                          // Wait for all wavefronts
ds_write_b32 v8, v4 offset:0                       // store bias
s_branch label_Load_Bias_End_1                     // Branch to load bias end
label_Load_Biasf16_1:
s_cmpk_lg_u32 s[sgprBiasType], 4                   // BiasType != 4
s_cbranch_scc1 label_Load_Bias_End_1               // Branch if true

/******************************************/
/* Read Bias to LDS                       */
/******************************************/
s_mul_i32 s[sgprSrdBias+2], 0x2, s[sgprSrdBias+2]  // scaled by BPE
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_add_u32 v8, s66, v[vgprSerial]                   // coord 0 = wgp0 * MT0 + thread offset
s_mul_i32 s66, s[sgprBiasStride], s[sgprWorkGroup2] // Stride * WG
v_add_u32 v8, s66, v8                              // coord 0 = wgp0 * MT0 + thread offset + Stride * WG
v_lshlrev_b32 v8, 0x1, v8                          // Global bias address scaled by BPE
buffer_load_short_d16 v4, v8, s[sgprSrdBias:sgprSrdBias+3], 0 offen offset:0 // load bias
v_lshlrev_b32 v8, 0x2, v[vgprSerial]               // Local bias address scaled by BPE
s_waitcnt vmcnt(0)                                 // wait for bias load
s_barrier                                          // Wait for all wavefronts
v_cvt_f32_f16 v4, v4                               // convert to FP32
ds_write_b32 v8, v4 offset:0                       // store bias
s_branch label_Load_Bias_End_1                     // Branch to load bias end
label_Load_Bias_End_1:
v_mov_b32 v4, s[sgprAlpha]
s_waitcnt lgkmcnt(0)                               // wait for scaleAB load
v_mul_f32 v4, v4, s51
v_mul_f32 v4, v4, s65
s_nop 0                                            // 1 wait states
v_readfirstlane_b32 s[sgprAlpha], v4               // Update Alpha
s_cmpk_eq_u32 s[sgprBeta], 0x0                     // Beta == 0
s_cbranch_scc0 label_GW_Beta_1                     // Branch if Beta is not zero

s_and_b32 s84, 255, s[sgprSizeI]                   // s84 = s[sgprSizeI] % 256
s_add_u32 s85, -0x1, s[sgprNumWorkGroups0]
s_cmp_ge_u32 s[sgprWorkGroup0], s85                // wg0 >= nwg0-1 ?
s_cselect_b32 s84, s84, 0                          // set rMT0
s_cmpk_gt_u32 s84, 0x0                             // rMT0 > 0
s_cbranch_scc1 label_GW_B0_E1_1                    // jump if edges required
s_and_b32 s84, 15, s[sgprSizeJ]                    // s84 = s[sgprSizeJ] % 16
s_add_u32 s85, -0x1, s[sgprNumWorkGroups1]
s_cmp_ge_u32 s[sgprWorkGroup1], s85                // wg1 >= nwg1-1
s_cselect_b32 s84, s84, 0                          // set rMT1
s_cmpk_gt_u32 s84, 0x0                             // rMT1 > 0
s_cbranch_scc1 label_GW_B0_E1_1                    // jump if edges required
label_GW_B0_E0_1:

/* edge=0, allocate 2 sgpr. perBatchTmpS=2 perBatchMaskS=0 perElementMaskS=0 elementsPerBatch=18 */
s_cmpk_eq_u32 s[sgprActivationType], 0             // activationType == 0
s_cbranch_scc1 label_Activation_None_1             // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 1             // activationType == 1
s_cbranch_scc1 label_Activation_Abs_1              // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 2             // activationType == 2
s_cbranch_scc1 label_Activation_Clippedrelu_1      // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 3             // activationType == 3
s_cbranch_scc1 label_Activation_Gelu_1             // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 4             // activationType == 4
s_cbranch_scc1 label_Activation_Leakyrelu_1        // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 5             // activationType == 5
s_cbranch_scc1 label_Activation_Relu_1             // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 6             // activationType == 6
s_cbranch_scc1 label_Activation_Sigmoid_1          // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 7             // activationType == 7
s_cbranch_scc1 label_Activation_Tanh_1             // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 9             // activationType == 9
s_cbranch_scc1 label_Activation_Geluscaling_1      // Branch if true
label_Activation_None_1:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v11, v0, s66
v_lshlrev_b32 v11, 0x2, v11                        // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v11 offset:0                // load bias
v_lshlrev_b32 v12, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v12, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_lshl_u32 v9, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+32], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+33], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+34], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+35], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+36], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+37], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+38], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+39], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt 0                                        // vmcnt(0) = 1 - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_1
label_Activation_Abs_1:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v11, v0, s66
v_lshlrev_b32 v11, 0x2, v11                        // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v11 offset:0                // load bias
v_lshlrev_b32 v12, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v12, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_lshl_u32 v9, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+32], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+33], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+34], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+35], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+36], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+37], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+38], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+39], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt 0                                        // vmcnt(0) = 1 - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_and_b32 v[vgprValuC+24], 0x7fffffff, v[vgprValuC+24] // Remove sign bit
v_and_b32 v[vgprValuC+25], 0x7fffffff, v[vgprValuC+25] // Remove sign bit
v_and_b32 v[vgprValuC+26], 0x7fffffff, v[vgprValuC+26] // Remove sign bit
v_and_b32 v[vgprValuC+27], 0x7fffffff, v[vgprValuC+27] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_and_b32 v[vgprValuC+28], 0x7fffffff, v[vgprValuC+28] // Remove sign bit
v_and_b32 v[vgprValuC+29], 0x7fffffff, v[vgprValuC+29] // Remove sign bit
v_and_b32 v[vgprValuC+30], 0x7fffffff, v[vgprValuC+30] // Remove sign bit
v_and_b32 v[vgprValuC+31], 0x7fffffff, v[vgprValuC+31] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_and_b32 v[vgprValuC+32], 0x7fffffff, v[vgprValuC+32] // Remove sign bit
v_and_b32 v[vgprValuC+33], 0x7fffffff, v[vgprValuC+33] // Remove sign bit
v_and_b32 v[vgprValuC+34], 0x7fffffff, v[vgprValuC+34] // Remove sign bit
v_and_b32 v[vgprValuC+35], 0x7fffffff, v[vgprValuC+35] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_and_b32 v[vgprValuC+36], 0x7fffffff, v[vgprValuC+36] // Remove sign bit
v_and_b32 v[vgprValuC+37], 0x7fffffff, v[vgprValuC+37] // Remove sign bit
v_and_b32 v[vgprValuC+38], 0x7fffffff, v[vgprValuC+38] // Remove sign bit
v_and_b32 v[vgprValuC+39], 0x7fffffff, v[vgprValuC+39] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_1
label_Activation_Clippedrelu_1:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v11, v0, s66
v_lshlrev_b32 v11, 0x2, v11                        // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v11 offset:0                // load bias
v_lshlrev_b32 v12, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v12, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_lshl_u32 v9, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+32], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+33], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+34], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+35], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+36], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+37], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+38], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+39], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt 0                                        // vmcnt(0) = 1 - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+24], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+24], s[sgpractivationBeta], v[vgprValuC+24] // min(x, beta)
v_cndmask_b32 v[vgprValuC+24], 0.0, v[vgprValuC+24], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+25], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+25], s[sgpractivationBeta], v[vgprValuC+25] // min(x, beta)
v_cndmask_b32 v[vgprValuC+25], 0.0, v[vgprValuC+25], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+26], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+26], s[sgpractivationBeta], v[vgprValuC+26] // min(x, beta)
v_cndmask_b32 v[vgprValuC+26], 0.0, v[vgprValuC+26], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+27], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+27], s[sgpractivationBeta], v[vgprValuC+27] // min(x, beta)
v_cndmask_b32 v[vgprValuC+27], 0.0, v[vgprValuC+27], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+28], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+28], s[sgpractivationBeta], v[vgprValuC+28] // min(x, beta)
v_cndmask_b32 v[vgprValuC+28], 0.0, v[vgprValuC+28], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+29], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+29], s[sgpractivationBeta], v[vgprValuC+29] // min(x, beta)
v_cndmask_b32 v[vgprValuC+29], 0.0, v[vgprValuC+29], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+30], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+30], s[sgpractivationBeta], v[vgprValuC+30] // min(x, beta)
v_cndmask_b32 v[vgprValuC+30], 0.0, v[vgprValuC+30], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+31], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+31], s[sgpractivationBeta], v[vgprValuC+31] // min(x, beta)
v_cndmask_b32 v[vgprValuC+31], 0.0, v[vgprValuC+31], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+32], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+32], s[sgpractivationBeta], v[vgprValuC+32] // min(x, beta)
v_cndmask_b32 v[vgprValuC+32], 0.0, v[vgprValuC+32], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+33], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+33], s[sgpractivationBeta], v[vgprValuC+33] // min(x, beta)
v_cndmask_b32 v[vgprValuC+33], 0.0, v[vgprValuC+33], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+34], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+34], s[sgpractivationBeta], v[vgprValuC+34] // min(x, beta)
v_cndmask_b32 v[vgprValuC+34], 0.0, v[vgprValuC+34], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+35], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+35], s[sgpractivationBeta], v[vgprValuC+35] // min(x, beta)
v_cndmask_b32 v[vgprValuC+35], 0.0, v[vgprValuC+35], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+36], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+36], s[sgpractivationBeta], v[vgprValuC+36] // min(x, beta)
v_cndmask_b32 v[vgprValuC+36], 0.0, v[vgprValuC+36], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+37], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+37], s[sgpractivationBeta], v[vgprValuC+37] // min(x, beta)
v_cndmask_b32 v[vgprValuC+37], 0.0, v[vgprValuC+37], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+38], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+38], s[sgpractivationBeta], v[vgprValuC+38] // min(x, beta)
v_cndmask_b32 v[vgprValuC+38], 0.0, v[vgprValuC+38], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+39], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+39], s[sgpractivationBeta], v[vgprValuC+39] // min(x, beta)
v_cndmask_b32 v[vgprValuC+39], 0.0, v[vgprValuC+39], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_1
label_Activation_Gelu_1:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v11, v0, s66
v_lshlrev_b32 v11, 0x2, v11                        // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v11 offset:0                // load bias
v_lshlrev_b32 v12, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v12, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_lshl_u32 v9, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+32], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+33], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+34], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+35], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+36], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+37], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+38], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+39], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt 0                                        // vmcnt(0) = 1 - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+24]          // k1 * x
v_fma_f32 v4, v[vgprValuC+24], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+24], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+24], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+24], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+25]          // k1 * x
v_fma_f32 v4, v[vgprValuC+25], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+25], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+25], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+25], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+26]          // k1 * x
v_fma_f32 v4, v[vgprValuC+26], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+26], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+26], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+26], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+27]          // k1 * x
v_fma_f32 v4, v[vgprValuC+27], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+27], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+27], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+27], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+28]          // k1 * x
v_fma_f32 v4, v[vgprValuC+28], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+28], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+28], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+28], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+29]          // k1 * x
v_fma_f32 v4, v[vgprValuC+29], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+29], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+29], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+29], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+30]          // k1 * x
v_fma_f32 v4, v[vgprValuC+30], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+30], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+30], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+30], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+31]          // k1 * x
v_fma_f32 v4, v[vgprValuC+31], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+31], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+31], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+31], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+32]          // k1 * x
v_fma_f32 v4, v[vgprValuC+32], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+32], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+32], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+32], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+33]          // k1 * x
v_fma_f32 v4, v[vgprValuC+33], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+33], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+33], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+33], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+34]          // k1 * x
v_fma_f32 v4, v[vgprValuC+34], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+34], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+34], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+34], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+35]          // k1 * x
v_fma_f32 v4, v[vgprValuC+35], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+35], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+35], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+35], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+36]          // k1 * x
v_fma_f32 v4, v[vgprValuC+36], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+36], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+36], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+36], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+37]          // k1 * x
v_fma_f32 v4, v[vgprValuC+37], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+37], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+37], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+37], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+38]          // k1 * x
v_fma_f32 v4, v[vgprValuC+38], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+38], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+38], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+38], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+39]          // k1 * x
v_fma_f32 v4, v[vgprValuC+39], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+39], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+39], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+39], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_1
label_Activation_Leakyrelu_1:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v11, v0, s66
v_lshlrev_b32 v11, 0x2, v11                        // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v11 offset:0                // load bias
v_lshlrev_b32 v12, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v12, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_lshl_u32 v9, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+32], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+33], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+34], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+35], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+36], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+37], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+38], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+39], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt 0                                        // vmcnt(0) = 1 - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+24] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+24], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+24], v4, v[vgprValuC+24], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+25] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+25], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+25], v4, v[vgprValuC+25], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+26] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+26], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+26], v4, v[vgprValuC+26], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+27] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+27], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+27], v4, v[vgprValuC+27], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+28] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+28], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+28], v4, v[vgprValuC+28], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+29] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+29], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+29], v4, v[vgprValuC+29], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+30] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+30], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+30], v4, v[vgprValuC+30], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+31] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+31], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+31], v4, v[vgprValuC+31], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+32] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+32], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+32], v4, v[vgprValuC+32], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+33] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+33], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+33], v4, v[vgprValuC+33], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+34] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+34], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+34], v4, v[vgprValuC+34], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+35] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+35], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+35], v4, v[vgprValuC+35], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+36] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+36], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+36], v4, v[vgprValuC+36], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+37] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+37], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+37], v4, v[vgprValuC+37], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+38] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+38], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+38], v4, v[vgprValuC+38], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+39] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+39], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+39], v4, v[vgprValuC+39], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_1
label_Activation_Relu_1:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v11, v0, s66
v_lshlrev_b32 v11, 0x2, v11                        // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v11 offset:0                // load bias
v_lshlrev_b32 v12, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v12, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_lshl_u32 v9, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+32], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+33], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+34], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+35], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+36], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+37], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+38], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+39], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt 0                                        // vmcnt(0) = 1 - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_max_f32 v[vgprValuC+24], v[vgprValuC+24], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+25], v[vgprValuC+25], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+26], v[vgprValuC+26], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+27], v[vgprValuC+27], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_max_f32 v[vgprValuC+28], v[vgprValuC+28], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+29], v[vgprValuC+29], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+30], v[vgprValuC+30], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+31], v[vgprValuC+31], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_max_f32 v[vgprValuC+32], v[vgprValuC+32], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+33], v[vgprValuC+33], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+34], v[vgprValuC+34], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+35], v[vgprValuC+35], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_max_f32 v[vgprValuC+36], v[vgprValuC+36], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+37], v[vgprValuC+37], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+38], v[vgprValuC+38], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+39], v[vgprValuC+39], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_1
label_Activation_Sigmoid_1:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v11, v0, s66
v_lshlrev_b32 v11, 0x2, v11                        // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v11 offset:0                // load bias
v_lshlrev_b32 v12, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v12, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_lshl_u32 v9, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+32], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+33], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+34], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+35], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+36], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+37], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+38], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+39], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt 0                                        // vmcnt(0) = 1 - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v[vgprValuC+24], 0xbfb8aa3b, v[vgprValuC+24] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+24], v[vgprValuC+24]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+24], 1.0, v[vgprValuC+24]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+24], v[vgprValuC+24]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+25], 0xbfb8aa3b, v[vgprValuC+25] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+25], v[vgprValuC+25]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+25], 1.0, v[vgprValuC+25]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+25], v[vgprValuC+25]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+26], 0xbfb8aa3b, v[vgprValuC+26] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+26], v[vgprValuC+26]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+26], 1.0, v[vgprValuC+26]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+26], v[vgprValuC+26]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+27], 0xbfb8aa3b, v[vgprValuC+27] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+27], v[vgprValuC+27]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+27], 1.0, v[vgprValuC+27]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+27], v[vgprValuC+27]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_mul_f32 v[vgprValuC+28], 0xbfb8aa3b, v[vgprValuC+28] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+28], v[vgprValuC+28]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+28], 1.0, v[vgprValuC+28]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+28], v[vgprValuC+28]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+29], 0xbfb8aa3b, v[vgprValuC+29] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+29], v[vgprValuC+29]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+29], 1.0, v[vgprValuC+29]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+29], v[vgprValuC+29]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+30], 0xbfb8aa3b, v[vgprValuC+30] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+30], v[vgprValuC+30]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+30], 1.0, v[vgprValuC+30]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+30], v[vgprValuC+30]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+31], 0xbfb8aa3b, v[vgprValuC+31] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+31], v[vgprValuC+31]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+31], 1.0, v[vgprValuC+31]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+31], v[vgprValuC+31]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v[vgprValuC+32], 0xbfb8aa3b, v[vgprValuC+32] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+32], v[vgprValuC+32]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+32], 1.0, v[vgprValuC+32]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+32], v[vgprValuC+32]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+33], 0xbfb8aa3b, v[vgprValuC+33] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+33], v[vgprValuC+33]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+33], 1.0, v[vgprValuC+33]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+33], v[vgprValuC+33]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+34], 0xbfb8aa3b, v[vgprValuC+34] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+34], v[vgprValuC+34]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+34], 1.0, v[vgprValuC+34]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+34], v[vgprValuC+34]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+35], 0xbfb8aa3b, v[vgprValuC+35] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+35], v[vgprValuC+35]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+35], 1.0, v[vgprValuC+35]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+35], v[vgprValuC+35]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v[vgprValuC+36], 0xbfb8aa3b, v[vgprValuC+36] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+36], v[vgprValuC+36]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+36], 1.0, v[vgprValuC+36]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+36], v[vgprValuC+36]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+37], 0xbfb8aa3b, v[vgprValuC+37] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+37], v[vgprValuC+37]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+37], 1.0, v[vgprValuC+37]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+37], v[vgprValuC+37]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+38], 0xbfb8aa3b, v[vgprValuC+38] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+38], v[vgprValuC+38]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+38], 1.0, v[vgprValuC+38]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+38], v[vgprValuC+38]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+39], 0xbfb8aa3b, v[vgprValuC+39] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+39], v[vgprValuC+39]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+39], 1.0, v[vgprValuC+39]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+39], v[vgprValuC+39]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_1
label_Activation_Tanh_1:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v11, v0, s66
v_lshlrev_b32 v11, 0x2, v11                        // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v11 offset:0                // load bias
v_lshlrev_b32 v12, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v12, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_lshl_u32 v9, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+32], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+33], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+34], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+35], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+36], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+37], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+38], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+39], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt 0                                        // vmcnt(0) = 1 - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v[vgprValuC+24], s[sgpractivationAlpha], v[vgprValuC+24] // x * alpha
v_mul_f32 v[vgprValuC+24], 0x4038aa3b, v[vgprValuC+24] //  (fused 2)
v_exp_f32 v[vgprValuC+24], v[vgprValuC+24]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+24], 1.0, v[vgprValuC+24]    // e^2x + 1
v_rcp_f32 v[vgprValuC+24], v[vgprValuC+24]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+24], -2.0, v[vgprValuC+24], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+24], s[sgpractivationBeta], v[vgprValuC+24] // beta * tanh(x)
v_mul_f32 v[vgprValuC+25], s[sgpractivationAlpha], v[vgprValuC+25] // x * alpha
v_mul_f32 v[vgprValuC+25], 0x4038aa3b, v[vgprValuC+25] //  (fused 2)
v_exp_f32 v[vgprValuC+25], v[vgprValuC+25]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+25], 1.0, v[vgprValuC+25]    // e^2x + 1
v_rcp_f32 v[vgprValuC+25], v[vgprValuC+25]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+25], -2.0, v[vgprValuC+25], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+25], s[sgpractivationBeta], v[vgprValuC+25] // beta * tanh(x)
v_mul_f32 v[vgprValuC+26], s[sgpractivationAlpha], v[vgprValuC+26] // x * alpha
v_mul_f32 v[vgprValuC+26], 0x4038aa3b, v[vgprValuC+26] //  (fused 2)
v_exp_f32 v[vgprValuC+26], v[vgprValuC+26]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+26], 1.0, v[vgprValuC+26]    // e^2x + 1
v_rcp_f32 v[vgprValuC+26], v[vgprValuC+26]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+26], -2.0, v[vgprValuC+26], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+26], s[sgpractivationBeta], v[vgprValuC+26] // beta * tanh(x)
v_mul_f32 v[vgprValuC+27], s[sgpractivationAlpha], v[vgprValuC+27] // x * alpha
v_mul_f32 v[vgprValuC+27], 0x4038aa3b, v[vgprValuC+27] //  (fused 2)
v_exp_f32 v[vgprValuC+27], v[vgprValuC+27]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+27], 1.0, v[vgprValuC+27]    // e^2x + 1
v_rcp_f32 v[vgprValuC+27], v[vgprValuC+27]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+27], -2.0, v[vgprValuC+27], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+27], s[sgpractivationBeta], v[vgprValuC+27] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_mul_f32 v[vgprValuC+28], s[sgpractivationAlpha], v[vgprValuC+28] // x * alpha
v_mul_f32 v[vgprValuC+28], 0x4038aa3b, v[vgprValuC+28] //  (fused 2)
v_exp_f32 v[vgprValuC+28], v[vgprValuC+28]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+28], 1.0, v[vgprValuC+28]    // e^2x + 1
v_rcp_f32 v[vgprValuC+28], v[vgprValuC+28]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+28], -2.0, v[vgprValuC+28], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+28], s[sgpractivationBeta], v[vgprValuC+28] // beta * tanh(x)
v_mul_f32 v[vgprValuC+29], s[sgpractivationAlpha], v[vgprValuC+29] // x * alpha
v_mul_f32 v[vgprValuC+29], 0x4038aa3b, v[vgprValuC+29] //  (fused 2)
v_exp_f32 v[vgprValuC+29], v[vgprValuC+29]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+29], 1.0, v[vgprValuC+29]    // e^2x + 1
v_rcp_f32 v[vgprValuC+29], v[vgprValuC+29]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+29], -2.0, v[vgprValuC+29], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+29], s[sgpractivationBeta], v[vgprValuC+29] // beta * tanh(x)
v_mul_f32 v[vgprValuC+30], s[sgpractivationAlpha], v[vgprValuC+30] // x * alpha
v_mul_f32 v[vgprValuC+30], 0x4038aa3b, v[vgprValuC+30] //  (fused 2)
v_exp_f32 v[vgprValuC+30], v[vgprValuC+30]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+30], 1.0, v[vgprValuC+30]    // e^2x + 1
v_rcp_f32 v[vgprValuC+30], v[vgprValuC+30]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+30], -2.0, v[vgprValuC+30], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+30], s[sgpractivationBeta], v[vgprValuC+30] // beta * tanh(x)
v_mul_f32 v[vgprValuC+31], s[sgpractivationAlpha], v[vgprValuC+31] // x * alpha
v_mul_f32 v[vgprValuC+31], 0x4038aa3b, v[vgprValuC+31] //  (fused 2)
v_exp_f32 v[vgprValuC+31], v[vgprValuC+31]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+31], 1.0, v[vgprValuC+31]    // e^2x + 1
v_rcp_f32 v[vgprValuC+31], v[vgprValuC+31]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+31], -2.0, v[vgprValuC+31], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+31], s[sgpractivationBeta], v[vgprValuC+31] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v[vgprValuC+32], s[sgpractivationAlpha], v[vgprValuC+32] // x * alpha
v_mul_f32 v[vgprValuC+32], 0x4038aa3b, v[vgprValuC+32] //  (fused 2)
v_exp_f32 v[vgprValuC+32], v[vgprValuC+32]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+32], 1.0, v[vgprValuC+32]    // e^2x + 1
v_rcp_f32 v[vgprValuC+32], v[vgprValuC+32]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+32], -2.0, v[vgprValuC+32], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+32], s[sgpractivationBeta], v[vgprValuC+32] // beta * tanh(x)
v_mul_f32 v[vgprValuC+33], s[sgpractivationAlpha], v[vgprValuC+33] // x * alpha
v_mul_f32 v[vgprValuC+33], 0x4038aa3b, v[vgprValuC+33] //  (fused 2)
v_exp_f32 v[vgprValuC+33], v[vgprValuC+33]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+33], 1.0, v[vgprValuC+33]    // e^2x + 1
v_rcp_f32 v[vgprValuC+33], v[vgprValuC+33]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+33], -2.0, v[vgprValuC+33], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+33], s[sgpractivationBeta], v[vgprValuC+33] // beta * tanh(x)
v_mul_f32 v[vgprValuC+34], s[sgpractivationAlpha], v[vgprValuC+34] // x * alpha
v_mul_f32 v[vgprValuC+34], 0x4038aa3b, v[vgprValuC+34] //  (fused 2)
v_exp_f32 v[vgprValuC+34], v[vgprValuC+34]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+34], 1.0, v[vgprValuC+34]    // e^2x + 1
v_rcp_f32 v[vgprValuC+34], v[vgprValuC+34]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+34], -2.0, v[vgprValuC+34], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+34], s[sgpractivationBeta], v[vgprValuC+34] // beta * tanh(x)
v_mul_f32 v[vgprValuC+35], s[sgpractivationAlpha], v[vgprValuC+35] // x * alpha
v_mul_f32 v[vgprValuC+35], 0x4038aa3b, v[vgprValuC+35] //  (fused 2)
v_exp_f32 v[vgprValuC+35], v[vgprValuC+35]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+35], 1.0, v[vgprValuC+35]    // e^2x + 1
v_rcp_f32 v[vgprValuC+35], v[vgprValuC+35]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+35], -2.0, v[vgprValuC+35], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+35], s[sgpractivationBeta], v[vgprValuC+35] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v[vgprValuC+36], s[sgpractivationAlpha], v[vgprValuC+36] // x * alpha
v_mul_f32 v[vgprValuC+36], 0x4038aa3b, v[vgprValuC+36] //  (fused 2)
v_exp_f32 v[vgprValuC+36], v[vgprValuC+36]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+36], 1.0, v[vgprValuC+36]    // e^2x + 1
v_rcp_f32 v[vgprValuC+36], v[vgprValuC+36]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+36], -2.0, v[vgprValuC+36], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+36], s[sgpractivationBeta], v[vgprValuC+36] // beta * tanh(x)
v_mul_f32 v[vgprValuC+37], s[sgpractivationAlpha], v[vgprValuC+37] // x * alpha
v_mul_f32 v[vgprValuC+37], 0x4038aa3b, v[vgprValuC+37] //  (fused 2)
v_exp_f32 v[vgprValuC+37], v[vgprValuC+37]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+37], 1.0, v[vgprValuC+37]    // e^2x + 1
v_rcp_f32 v[vgprValuC+37], v[vgprValuC+37]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+37], -2.0, v[vgprValuC+37], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+37], s[sgpractivationBeta], v[vgprValuC+37] // beta * tanh(x)
v_mul_f32 v[vgprValuC+38], s[sgpractivationAlpha], v[vgprValuC+38] // x * alpha
v_mul_f32 v[vgprValuC+38], 0x4038aa3b, v[vgprValuC+38] //  (fused 2)
v_exp_f32 v[vgprValuC+38], v[vgprValuC+38]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+38], 1.0, v[vgprValuC+38]    // e^2x + 1
v_rcp_f32 v[vgprValuC+38], v[vgprValuC+38]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+38], -2.0, v[vgprValuC+38], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+38], s[sgpractivationBeta], v[vgprValuC+38] // beta * tanh(x)
v_mul_f32 v[vgprValuC+39], s[sgpractivationAlpha], v[vgprValuC+39] // x * alpha
v_mul_f32 v[vgprValuC+39], 0x4038aa3b, v[vgprValuC+39] //  (fused 2)
v_exp_f32 v[vgprValuC+39], v[vgprValuC+39]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+39], 1.0, v[vgprValuC+39]    // e^2x + 1
v_rcp_f32 v[vgprValuC+39], v[vgprValuC+39]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+39], -2.0, v[vgprValuC+39], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+39], s[sgpractivationBeta], v[vgprValuC+39] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_1
label_Activation_Geluscaling_1:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v11, v0, s66
v_lshlrev_b32 v11, 0x2, v11                        // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v11 offset:0                // load bias
v_lshlrev_b32 v12, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v12, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_lshl_u32 v9, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+32], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+33], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+34], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+35], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+36], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+37], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+38], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+39], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt 0                                        // vmcnt(0) = 1 - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+24]          // k1 * x
v_fma_f32 v4, v[vgprValuC+24], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+24], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+24], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+24], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+25]          // k1 * x
v_fma_f32 v4, v[vgprValuC+25], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+25], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+25], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+25], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+26]          // k1 * x
v_fma_f32 v4, v[vgprValuC+26], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+26], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+26], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+26], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+27]          // k1 * x
v_fma_f32 v4, v[vgprValuC+27], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+27], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+27], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+27], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+28]          // k1 * x
v_fma_f32 v4, v[vgprValuC+28], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+28], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+28], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+28], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+29]          // k1 * x
v_fma_f32 v4, v[vgprValuC+29], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+29], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+29], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+29], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+30]          // k1 * x
v_fma_f32 v4, v[vgprValuC+30], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+30], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+30], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+30], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+31]          // k1 * x
v_fma_f32 v4, v[vgprValuC+31], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+31], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+31], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+31], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+32]          // k1 * x
v_fma_f32 v4, v[vgprValuC+32], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+32], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+32], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+32], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+33]          // k1 * x
v_fma_f32 v4, v[vgprValuC+33], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+33], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+33], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+33], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+34]          // k1 * x
v_fma_f32 v4, v[vgprValuC+34], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+34], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+34], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+34], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+35]          // k1 * x
v_fma_f32 v4, v[vgprValuC+35], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+35], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+35], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+35], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+36]          // k1 * x
v_fma_f32 v4, v[vgprValuC+36], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+36], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+36], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+36], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+37]          // k1 * x
v_fma_f32 v4, v[vgprValuC+37], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+37], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+37], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+37], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+38]          // k1 * x
v_fma_f32 v4, v[vgprValuC+38], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+38], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+38], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+38], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+39]          // k1 * x
v_fma_f32 v4, v[vgprValuC+39], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+39], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+39], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+39], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
label_Activation_End_1:
s_branch label_GW_End_1                            // jump to end
label_GW_B0_E1_1:

/* edge=1, allocate 6 sgpr. perBatchTmpS=4 perBatchMaskS=2 perElementMaskS=0 elementsPerBatch=14 */
s_cmpk_eq_u32 s[sgprActivationType], 0             // activationType == 0
s_cbranch_scc1 label_Activation_None_Edge_1        // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 1             // activationType == 1
s_cbranch_scc1 label_Activation_Abs_Edge_1         // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 2             // activationType == 2
s_cbranch_scc1 label_Activation_Clippedrelu_Edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 3             // activationType == 3
s_cbranch_scc1 label_Activation_Gelu_Edge_1        // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 4             // activationType == 4
s_cbranch_scc1 label_Activation_Leakyrelu_Edge_1   // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 5             // activationType == 5
s_cbranch_scc1 label_Activation_Relu_Edge_1        // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 6             // activationType == 6
s_cbranch_scc1 label_Activation_Sigmoid_Edge_1     // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 7             // activationType == 7
s_cbranch_scc1 label_Activation_Tanh_Edge_1        // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 9             // activationType == 9
s_cbranch_scc1 label_Activation_Geluscaling_Edge_1 // Branch if true
label_Activation_None_Edge_1:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v41, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v0, s84
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
v_cndmask_b32 v10, v41, v10, s[88:89]              // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v10 offset:0                // load bias
v_lshlrev_b32 v11, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v11, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v9, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v41, v9, s[88:89]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v25, v0, s84
v_lshlrev_b32 v25, 0x2, v25                        // Bias address scaled by BPE
v_cndmask_b32 v25, v41, v25, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v26, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v24, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v41, v24, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v0, s84
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v41, v32, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v33, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v27, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v41, v27, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v0, s84
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
v_cndmask_b32 v35, v41, v35, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v40, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v34, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v41, v34, s[88:89]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
buffer_store_dwordx2 v[28:29], v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[12:13], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[14:15], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[12:13], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[14:15], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Edge_1
label_Activation_Abs_Edge_1:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v41, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v0, s84
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
v_cndmask_b32 v10, v41, v10, s[88:89]              // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v10 offset:0                // load bias
v_lshlrev_b32 v11, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v11, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v9, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v41, v9, s[88:89]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v25, v0, s84
v_lshlrev_b32 v25, 0x2, v25                        // Bias address scaled by BPE
v_cndmask_b32 v25, v41, v25, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v26, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v24, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v41, v24, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v0, s84
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v41, v32, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v33, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v27, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v41, v27, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v0, s84
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
v_cndmask_b32 v35, v41, v35, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v40, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v34, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v41, v34, s[88:89]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_and_b32 v[vgprValuC+20], 0x7fffffff, v[vgprValuC+20] // Remove sign bit
v_and_b32 v[vgprValuC+21], 0x7fffffff, v[vgprValuC+21] // Remove sign bit
v_and_b32 v[vgprValuC+22], 0x7fffffff, v[vgprValuC+22] // Remove sign bit
v_and_b32 v[vgprValuC+23], 0x7fffffff, v[vgprValuC+23] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_and_b32 v[vgprValuC+28], 0x7fffffff, v[vgprValuC+28] // Remove sign bit
v_and_b32 v[vgprValuC+29], 0x7fffffff, v[vgprValuC+29] // Remove sign bit
v_and_b32 v[vgprValuC+30], 0x7fffffff, v[vgprValuC+30] // Remove sign bit
v_and_b32 v[vgprValuC+31], 0x7fffffff, v[vgprValuC+31] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
buffer_store_dwordx2 v[28:29], v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[12:13], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[14:15], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_and_b32 v[vgprValuC+36], 0x7fffffff, v[vgprValuC+36] // Remove sign bit
v_and_b32 v[vgprValuC+37], 0x7fffffff, v[vgprValuC+37] // Remove sign bit
v_and_b32 v[vgprValuC+38], 0x7fffffff, v[vgprValuC+38] // Remove sign bit
v_and_b32 v[vgprValuC+39], 0x7fffffff, v[vgprValuC+39] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[12:13], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[14:15], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_and_b32 v[vgprValuC+44], 0x7fffffff, v[vgprValuC+44] // Remove sign bit
v_and_b32 v[vgprValuC+45], 0x7fffffff, v[vgprValuC+45] // Remove sign bit
v_and_b32 v[vgprValuC+46], 0x7fffffff, v[vgprValuC+46] // Remove sign bit
v_and_b32 v[vgprValuC+47], 0x7fffffff, v[vgprValuC+47] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Edge_1
label_Activation_Clippedrelu_Edge_1:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v41, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v0, s84
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
v_cndmask_b32 v10, v41, v10, s[88:89]              // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v10 offset:0                // load bias
v_lshlrev_b32 v11, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v11, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v9, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v41, v9, s[88:89]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v25, v0, s84
v_lshlrev_b32 v25, 0x2, v25                        // Bias address scaled by BPE
v_cndmask_b32 v25, v41, v25, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v26, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v24, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v41, v24, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v0, s84
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v41, v32, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v33, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v27, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v41, v27, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v0, s84
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
v_cndmask_b32 v35, v41, v35, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v40, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v34, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v41, v34, s[88:89]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+20], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+20], s[sgpractivationBeta], v[vgprValuC+20] // min(x, beta)
v_cndmask_b32 v[vgprValuC+20], 0.0, v[vgprValuC+20], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+21], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+21], s[sgpractivationBeta], v[vgprValuC+21] // min(x, beta)
v_cndmask_b32 v[vgprValuC+21], 0.0, v[vgprValuC+21], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+22], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+22], s[sgpractivationBeta], v[vgprValuC+22] // min(x, beta)
v_cndmask_b32 v[vgprValuC+22], 0.0, v[vgprValuC+22], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+23], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+23], s[sgpractivationBeta], v[vgprValuC+23] // min(x, beta)
v_cndmask_b32 v[vgprValuC+23], 0.0, v[vgprValuC+23], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+28], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+28], s[sgpractivationBeta], v[vgprValuC+28] // min(x, beta)
v_cndmask_b32 v[vgprValuC+28], 0.0, v[vgprValuC+28], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+29], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+29], s[sgpractivationBeta], v[vgprValuC+29] // min(x, beta)
v_cndmask_b32 v[vgprValuC+29], 0.0, v[vgprValuC+29], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+30], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+30], s[sgpractivationBeta], v[vgprValuC+30] // min(x, beta)
v_cndmask_b32 v[vgprValuC+30], 0.0, v[vgprValuC+30], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+31], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+31], s[sgpractivationBeta], v[vgprValuC+31] // min(x, beta)
v_cndmask_b32 v[vgprValuC+31], 0.0, v[vgprValuC+31], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
buffer_store_dwordx2 v[28:29], v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[12:13], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[14:15], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+36], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+36], s[sgpractivationBeta], v[vgprValuC+36] // min(x, beta)
v_cndmask_b32 v[vgprValuC+36], 0.0, v[vgprValuC+36], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+37], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+37], s[sgpractivationBeta], v[vgprValuC+37] // min(x, beta)
v_cndmask_b32 v[vgprValuC+37], 0.0, v[vgprValuC+37], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+38], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+38], s[sgpractivationBeta], v[vgprValuC+38] // min(x, beta)
v_cndmask_b32 v[vgprValuC+38], 0.0, v[vgprValuC+38], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+39], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+39], s[sgpractivationBeta], v[vgprValuC+39] // min(x, beta)
v_cndmask_b32 v[vgprValuC+39], 0.0, v[vgprValuC+39], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[12:13], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[14:15], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+44], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+44], s[sgpractivationBeta], v[vgprValuC+44] // min(x, beta)
v_cndmask_b32 v[vgprValuC+44], 0.0, v[vgprValuC+44], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+45], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+45], s[sgpractivationBeta], v[vgprValuC+45] // min(x, beta)
v_cndmask_b32 v[vgprValuC+45], 0.0, v[vgprValuC+45], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+46], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+46], s[sgpractivationBeta], v[vgprValuC+46] // min(x, beta)
v_cndmask_b32 v[vgprValuC+46], 0.0, v[vgprValuC+46], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+47], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+47], s[sgpractivationBeta], v[vgprValuC+47] // min(x, beta)
v_cndmask_b32 v[vgprValuC+47], 0.0, v[vgprValuC+47], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Edge_1
label_Activation_Gelu_Edge_1:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v41, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v0, s84
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
v_cndmask_b32 v10, v41, v10, s[88:89]              // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v10 offset:0                // load bias
v_lshlrev_b32 v11, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v11, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v9, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v41, v9, s[88:89]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v25, v0, s84
v_lshlrev_b32 v25, 0x2, v25                        // Bias address scaled by BPE
v_cndmask_b32 v25, v41, v25, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v26, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v24, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v41, v24, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v0, s84
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v41, v32, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v33, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v27, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v41, v27, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v0, s84
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
v_cndmask_b32 v35, v41, v35, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v40, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v34, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v41, v34, s[88:89]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+20]          // k1 * x
v_fma_f32 v4, v[vgprValuC+20], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+20], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+20], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+20], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+21]          // k1 * x
v_fma_f32 v4, v[vgprValuC+21], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+21], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+21], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+21], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+22]          // k1 * x
v_fma_f32 v4, v[vgprValuC+22], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+22], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+22], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+22], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+23]          // k1 * x
v_fma_f32 v4, v[vgprValuC+23], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+23], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+23], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+23], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+28]          // k1 * x
v_fma_f32 v4, v[vgprValuC+28], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+28], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+28], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+28], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+29]          // k1 * x
v_fma_f32 v4, v[vgprValuC+29], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+29], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+29], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+29], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+30]          // k1 * x
v_fma_f32 v4, v[vgprValuC+30], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+30], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+30], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+30], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+31]          // k1 * x
v_fma_f32 v4, v[vgprValuC+31], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+31], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+31], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+31], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
buffer_store_dwordx2 v[28:29], v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[12:13], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[14:15], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+36]          // k1 * x
v_fma_f32 v4, v[vgprValuC+36], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+36], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+36], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+36], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+37]          // k1 * x
v_fma_f32 v4, v[vgprValuC+37], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+37], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+37], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+37], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+38]          // k1 * x
v_fma_f32 v4, v[vgprValuC+38], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+38], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+38], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+38], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+39]          // k1 * x
v_fma_f32 v4, v[vgprValuC+39], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+39], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+39], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+39], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[12:13], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[14:15], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+44]          // k1 * x
v_fma_f32 v4, v[vgprValuC+44], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+44], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+44], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+44], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+45]          // k1 * x
v_fma_f32 v4, v[vgprValuC+45], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+45], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+45], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+45], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+46]          // k1 * x
v_fma_f32 v4, v[vgprValuC+46], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+46], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+46], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+46], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+47]          // k1 * x
v_fma_f32 v4, v[vgprValuC+47], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+47], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+47], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+47], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Edge_1
label_Activation_Leakyrelu_Edge_1:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v41, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v0, s84
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
v_cndmask_b32 v10, v41, v10, s[88:89]              // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v10 offset:0                // load bias
v_lshlrev_b32 v11, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v11, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v9, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v41, v9, s[88:89]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v25, v0, s84
v_lshlrev_b32 v25, 0x2, v25                        // Bias address scaled by BPE
v_cndmask_b32 v25, v41, v25, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v26, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v24, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v41, v24, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v0, s84
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v41, v32, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v33, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v27, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v41, v27, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v0, s84
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
v_cndmask_b32 v35, v41, v35, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v40, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v34, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v41, v34, s[88:89]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+20] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+20], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+20], v4, v[vgprValuC+20], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+21] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+21], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+21], v4, v[vgprValuC+21], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+22] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+22], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+22], v4, v[vgprValuC+22], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+23] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+23], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+23], v4, v[vgprValuC+23], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+28] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+28], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+28], v4, v[vgprValuC+28], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+29] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+29], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+29], v4, v[vgprValuC+29], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+30] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+30], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+30], v4, v[vgprValuC+30], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+31] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+31], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+31], v4, v[vgprValuC+31], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
buffer_store_dwordx2 v[28:29], v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[12:13], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[14:15], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+36] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+36], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+36], v4, v[vgprValuC+36], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+37] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+37], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+37], v4, v[vgprValuC+37], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+38] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+38], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+38], v4, v[vgprValuC+38], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+39] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+39], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+39], v4, v[vgprValuC+39], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[12:13], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[14:15], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+44] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+44], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+44], v4, v[vgprValuC+44], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+45] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+45], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+45], v4, v[vgprValuC+45], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+46] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+46], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+46], v4, v[vgprValuC+46], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+47] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+47], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+47], v4, v[vgprValuC+47], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Edge_1
label_Activation_Relu_Edge_1:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v41, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v0, s84
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
v_cndmask_b32 v10, v41, v10, s[88:89]              // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v10 offset:0                // load bias
v_lshlrev_b32 v11, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v11, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v9, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v41, v9, s[88:89]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v25, v0, s84
v_lshlrev_b32 v25, 0x2, v25                        // Bias address scaled by BPE
v_cndmask_b32 v25, v41, v25, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v26, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v24, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v41, v24, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v0, s84
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v41, v32, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v33, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v27, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v41, v27, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v0, s84
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
v_cndmask_b32 v35, v41, v35, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v40, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v34, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v41, v34, s[88:89]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_max_f32 v[vgprValuC+20], v[vgprValuC+20], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+21], v[vgprValuC+21], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+22], v[vgprValuC+22], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+23], v[vgprValuC+23], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_max_f32 v[vgprValuC+28], v[vgprValuC+28], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+29], v[vgprValuC+29], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+30], v[vgprValuC+30], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+31], v[vgprValuC+31], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
buffer_store_dwordx2 v[28:29], v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[12:13], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[14:15], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_max_f32 v[vgprValuC+36], v[vgprValuC+36], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+37], v[vgprValuC+37], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+38], v[vgprValuC+38], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+39], v[vgprValuC+39], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[12:13], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[14:15], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_max_f32 v[vgprValuC+44], v[vgprValuC+44], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+45], v[vgprValuC+45], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+46], v[vgprValuC+46], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+47], v[vgprValuC+47], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Edge_1
label_Activation_Sigmoid_Edge_1:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v41, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v0, s84
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
v_cndmask_b32 v10, v41, v10, s[88:89]              // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v10 offset:0                // load bias
v_lshlrev_b32 v11, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v11, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v9, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v41, v9, s[88:89]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v25, v0, s84
v_lshlrev_b32 v25, 0x2, v25                        // Bias address scaled by BPE
v_cndmask_b32 v25, v41, v25, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v26, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v24, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v41, v24, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v0, s84
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v41, v32, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v33, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v27, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v41, v27, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v0, s84
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
v_cndmask_b32 v35, v41, v35, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v40, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v34, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v41, v34, s[88:89]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_mul_f32 v[vgprValuC+20], 0xbfb8aa3b, v[vgprValuC+20] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+20], v[vgprValuC+20]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+20], 1.0, v[vgprValuC+20]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+20], v[vgprValuC+20]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+21], 0xbfb8aa3b, v[vgprValuC+21] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+21], v[vgprValuC+21]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+21], 1.0, v[vgprValuC+21]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+21], v[vgprValuC+21]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+22], 0xbfb8aa3b, v[vgprValuC+22] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+22], v[vgprValuC+22]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+22], 1.0, v[vgprValuC+22]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+22], v[vgprValuC+22]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+23], 0xbfb8aa3b, v[vgprValuC+23] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+23], v[vgprValuC+23]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+23], 1.0, v[vgprValuC+23]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+23], v[vgprValuC+23]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_mul_f32 v[vgprValuC+28], 0xbfb8aa3b, v[vgprValuC+28] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+28], v[vgprValuC+28]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+28], 1.0, v[vgprValuC+28]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+28], v[vgprValuC+28]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+29], 0xbfb8aa3b, v[vgprValuC+29] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+29], v[vgprValuC+29]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+29], 1.0, v[vgprValuC+29]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+29], v[vgprValuC+29]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+30], 0xbfb8aa3b, v[vgprValuC+30] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+30], v[vgprValuC+30]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+30], 1.0, v[vgprValuC+30]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+30], v[vgprValuC+30]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+31], 0xbfb8aa3b, v[vgprValuC+31] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+31], v[vgprValuC+31]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+31], 1.0, v[vgprValuC+31]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+31], v[vgprValuC+31]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
buffer_store_dwordx2 v[28:29], v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[12:13], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[14:15], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v[vgprValuC+36], 0xbfb8aa3b, v[vgprValuC+36] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+36], v[vgprValuC+36]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+36], 1.0, v[vgprValuC+36]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+36], v[vgprValuC+36]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+37], 0xbfb8aa3b, v[vgprValuC+37] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+37], v[vgprValuC+37]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+37], 1.0, v[vgprValuC+37]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+37], v[vgprValuC+37]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+38], 0xbfb8aa3b, v[vgprValuC+38] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+38], v[vgprValuC+38]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+38], 1.0, v[vgprValuC+38]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+38], v[vgprValuC+38]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+39], 0xbfb8aa3b, v[vgprValuC+39] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+39], v[vgprValuC+39]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+39], 1.0, v[vgprValuC+39]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+39], v[vgprValuC+39]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[12:13], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[14:15], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_mul_f32 v[vgprValuC+44], 0xbfb8aa3b, v[vgprValuC+44] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+44], v[vgprValuC+44]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+44], 1.0, v[vgprValuC+44]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+44], v[vgprValuC+44]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+45], 0xbfb8aa3b, v[vgprValuC+45] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+45], v[vgprValuC+45]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+45], 1.0, v[vgprValuC+45]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+45], v[vgprValuC+45]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+46], 0xbfb8aa3b, v[vgprValuC+46] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+46], v[vgprValuC+46]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+46], 1.0, v[vgprValuC+46]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+46], v[vgprValuC+46]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+47], 0xbfb8aa3b, v[vgprValuC+47] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+47], v[vgprValuC+47]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+47], 1.0, v[vgprValuC+47]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+47], v[vgprValuC+47]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Edge_1
label_Activation_Tanh_Edge_1:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v41, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v0, s84
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
v_cndmask_b32 v10, v41, v10, s[88:89]              // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v10 offset:0                // load bias
v_lshlrev_b32 v11, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v11, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v9, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v41, v9, s[88:89]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v25, v0, s84
v_lshlrev_b32 v25, 0x2, v25                        // Bias address scaled by BPE
v_cndmask_b32 v25, v41, v25, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v26, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v24, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v41, v24, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v0, s84
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v41, v32, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v33, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v27, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v41, v27, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v0, s84
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
v_cndmask_b32 v35, v41, v35, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v40, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v34, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v41, v34, s[88:89]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_mul_f32 v[vgprValuC+20], s[sgpractivationAlpha], v[vgprValuC+20] // x * alpha
v_mul_f32 v[vgprValuC+20], 0x4038aa3b, v[vgprValuC+20] //  (fused 2)
v_exp_f32 v[vgprValuC+20], v[vgprValuC+20]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+20], 1.0, v[vgprValuC+20]    // e^2x + 1
v_rcp_f32 v[vgprValuC+20], v[vgprValuC+20]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+20], -2.0, v[vgprValuC+20], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+20], s[sgpractivationBeta], v[vgprValuC+20] // beta * tanh(x)
v_mul_f32 v[vgprValuC+21], s[sgpractivationAlpha], v[vgprValuC+21] // x * alpha
v_mul_f32 v[vgprValuC+21], 0x4038aa3b, v[vgprValuC+21] //  (fused 2)
v_exp_f32 v[vgprValuC+21], v[vgprValuC+21]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+21], 1.0, v[vgprValuC+21]    // e^2x + 1
v_rcp_f32 v[vgprValuC+21], v[vgprValuC+21]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+21], -2.0, v[vgprValuC+21], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+21], s[sgpractivationBeta], v[vgprValuC+21] // beta * tanh(x)
v_mul_f32 v[vgprValuC+22], s[sgpractivationAlpha], v[vgprValuC+22] // x * alpha
v_mul_f32 v[vgprValuC+22], 0x4038aa3b, v[vgprValuC+22] //  (fused 2)
v_exp_f32 v[vgprValuC+22], v[vgprValuC+22]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+22], 1.0, v[vgprValuC+22]    // e^2x + 1
v_rcp_f32 v[vgprValuC+22], v[vgprValuC+22]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+22], -2.0, v[vgprValuC+22], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+22], s[sgpractivationBeta], v[vgprValuC+22] // beta * tanh(x)
v_mul_f32 v[vgprValuC+23], s[sgpractivationAlpha], v[vgprValuC+23] // x * alpha
v_mul_f32 v[vgprValuC+23], 0x4038aa3b, v[vgprValuC+23] //  (fused 2)
v_exp_f32 v[vgprValuC+23], v[vgprValuC+23]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+23], 1.0, v[vgprValuC+23]    // e^2x + 1
v_rcp_f32 v[vgprValuC+23], v[vgprValuC+23]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+23], -2.0, v[vgprValuC+23], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+23], s[sgpractivationBeta], v[vgprValuC+23] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_mul_f32 v[vgprValuC+28], s[sgpractivationAlpha], v[vgprValuC+28] // x * alpha
v_mul_f32 v[vgprValuC+28], 0x4038aa3b, v[vgprValuC+28] //  (fused 2)
v_exp_f32 v[vgprValuC+28], v[vgprValuC+28]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+28], 1.0, v[vgprValuC+28]    // e^2x + 1
v_rcp_f32 v[vgprValuC+28], v[vgprValuC+28]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+28], -2.0, v[vgprValuC+28], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+28], s[sgpractivationBeta], v[vgprValuC+28] // beta * tanh(x)
v_mul_f32 v[vgprValuC+29], s[sgpractivationAlpha], v[vgprValuC+29] // x * alpha
v_mul_f32 v[vgprValuC+29], 0x4038aa3b, v[vgprValuC+29] //  (fused 2)
v_exp_f32 v[vgprValuC+29], v[vgprValuC+29]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+29], 1.0, v[vgprValuC+29]    // e^2x + 1
v_rcp_f32 v[vgprValuC+29], v[vgprValuC+29]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+29], -2.0, v[vgprValuC+29], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+29], s[sgpractivationBeta], v[vgprValuC+29] // beta * tanh(x)
v_mul_f32 v[vgprValuC+30], s[sgpractivationAlpha], v[vgprValuC+30] // x * alpha
v_mul_f32 v[vgprValuC+30], 0x4038aa3b, v[vgprValuC+30] //  (fused 2)
v_exp_f32 v[vgprValuC+30], v[vgprValuC+30]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+30], 1.0, v[vgprValuC+30]    // e^2x + 1
v_rcp_f32 v[vgprValuC+30], v[vgprValuC+30]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+30], -2.0, v[vgprValuC+30], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+30], s[sgpractivationBeta], v[vgprValuC+30] // beta * tanh(x)
v_mul_f32 v[vgprValuC+31], s[sgpractivationAlpha], v[vgprValuC+31] // x * alpha
v_mul_f32 v[vgprValuC+31], 0x4038aa3b, v[vgprValuC+31] //  (fused 2)
v_exp_f32 v[vgprValuC+31], v[vgprValuC+31]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+31], 1.0, v[vgprValuC+31]    // e^2x + 1
v_rcp_f32 v[vgprValuC+31], v[vgprValuC+31]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+31], -2.0, v[vgprValuC+31], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+31], s[sgpractivationBeta], v[vgprValuC+31] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
buffer_store_dwordx2 v[28:29], v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[12:13], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[14:15], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v[vgprValuC+36], s[sgpractivationAlpha], v[vgprValuC+36] // x * alpha
v_mul_f32 v[vgprValuC+36], 0x4038aa3b, v[vgprValuC+36] //  (fused 2)
v_exp_f32 v[vgprValuC+36], v[vgprValuC+36]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+36], 1.0, v[vgprValuC+36]    // e^2x + 1
v_rcp_f32 v[vgprValuC+36], v[vgprValuC+36]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+36], -2.0, v[vgprValuC+36], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+36], s[sgpractivationBeta], v[vgprValuC+36] // beta * tanh(x)
v_mul_f32 v[vgprValuC+37], s[sgpractivationAlpha], v[vgprValuC+37] // x * alpha
v_mul_f32 v[vgprValuC+37], 0x4038aa3b, v[vgprValuC+37] //  (fused 2)
v_exp_f32 v[vgprValuC+37], v[vgprValuC+37]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+37], 1.0, v[vgprValuC+37]    // e^2x + 1
v_rcp_f32 v[vgprValuC+37], v[vgprValuC+37]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+37], -2.0, v[vgprValuC+37], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+37], s[sgpractivationBeta], v[vgprValuC+37] // beta * tanh(x)
v_mul_f32 v[vgprValuC+38], s[sgpractivationAlpha], v[vgprValuC+38] // x * alpha
v_mul_f32 v[vgprValuC+38], 0x4038aa3b, v[vgprValuC+38] //  (fused 2)
v_exp_f32 v[vgprValuC+38], v[vgprValuC+38]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+38], 1.0, v[vgprValuC+38]    // e^2x + 1
v_rcp_f32 v[vgprValuC+38], v[vgprValuC+38]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+38], -2.0, v[vgprValuC+38], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+38], s[sgpractivationBeta], v[vgprValuC+38] // beta * tanh(x)
v_mul_f32 v[vgprValuC+39], s[sgpractivationAlpha], v[vgprValuC+39] // x * alpha
v_mul_f32 v[vgprValuC+39], 0x4038aa3b, v[vgprValuC+39] //  (fused 2)
v_exp_f32 v[vgprValuC+39], v[vgprValuC+39]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+39], 1.0, v[vgprValuC+39]    // e^2x + 1
v_rcp_f32 v[vgprValuC+39], v[vgprValuC+39]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+39], -2.0, v[vgprValuC+39], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+39], s[sgpractivationBeta], v[vgprValuC+39] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[12:13], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[14:15], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_mul_f32 v[vgprValuC+44], s[sgpractivationAlpha], v[vgprValuC+44] // x * alpha
v_mul_f32 v[vgprValuC+44], 0x4038aa3b, v[vgprValuC+44] //  (fused 2)
v_exp_f32 v[vgprValuC+44], v[vgprValuC+44]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+44], 1.0, v[vgprValuC+44]    // e^2x + 1
v_rcp_f32 v[vgprValuC+44], v[vgprValuC+44]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+44], -2.0, v[vgprValuC+44], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+44], s[sgpractivationBeta], v[vgprValuC+44] // beta * tanh(x)
v_mul_f32 v[vgprValuC+45], s[sgpractivationAlpha], v[vgprValuC+45] // x * alpha
v_mul_f32 v[vgprValuC+45], 0x4038aa3b, v[vgprValuC+45] //  (fused 2)
v_exp_f32 v[vgprValuC+45], v[vgprValuC+45]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+45], 1.0, v[vgprValuC+45]    // e^2x + 1
v_rcp_f32 v[vgprValuC+45], v[vgprValuC+45]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+45], -2.0, v[vgprValuC+45], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+45], s[sgpractivationBeta], v[vgprValuC+45] // beta * tanh(x)
v_mul_f32 v[vgprValuC+46], s[sgpractivationAlpha], v[vgprValuC+46] // x * alpha
v_mul_f32 v[vgprValuC+46], 0x4038aa3b, v[vgprValuC+46] //  (fused 2)
v_exp_f32 v[vgprValuC+46], v[vgprValuC+46]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+46], 1.0, v[vgprValuC+46]    // e^2x + 1
v_rcp_f32 v[vgprValuC+46], v[vgprValuC+46]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+46], -2.0, v[vgprValuC+46], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+46], s[sgpractivationBeta], v[vgprValuC+46] // beta * tanh(x)
v_mul_f32 v[vgprValuC+47], s[sgpractivationAlpha], v[vgprValuC+47] // x * alpha
v_mul_f32 v[vgprValuC+47], 0x4038aa3b, v[vgprValuC+47] //  (fused 2)
v_exp_f32 v[vgprValuC+47], v[vgprValuC+47]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+47], 1.0, v[vgprValuC+47]    // e^2x + 1
v_rcp_f32 v[vgprValuC+47], v[vgprValuC+47]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+47], -2.0, v[vgprValuC+47], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+47], s[sgpractivationBeta], v[vgprValuC+47] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Edge_1
label_Activation_Geluscaling_Edge_1:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v41, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v0, s84
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
v_cndmask_b32 v10, v41, v10, s[88:89]              // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[12:15], v10 offset:0                // load bias
v_lshlrev_b32 v11, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[16:19], v11, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v9, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v41, v9, s[88:89]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v25, v0, s84
v_lshlrev_b32 v25, 0x2, v25                        // Bias address scaled by BPE
v_cndmask_b32 v25, v41, v25, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v26, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v24, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v41, v24, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v0, s84
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v41, v32, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v33, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v27, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v41, v27, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v0, s84
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
v_cndmask_b32 v35, v41, v35, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v40, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v34, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v41, v34, s[88:89]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+20], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+21], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+22], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+23], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+28], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+29], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+30], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+31], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+20], s[sgprAlpha], v[vgprValuC+20] // *= alpha
v_mul_f32 v[vgprValuC+21], s[sgprAlpha], v[vgprValuC+21] // *= alpha
v_mul_f32 v[vgprValuC+22], s[sgprAlpha], v[vgprValuC+22] // *= alpha
v_mul_f32 v[vgprValuC+23], s[sgprAlpha], v[vgprValuC+23] // *= alpha
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+20:vgprValuC+20+1], v[16:17], v[vgprValuC+20:vgprValuC+20+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+22:vgprValuC+22+1], v[18:19], v[vgprValuC+22:vgprValuC+22+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+20:vgprValuC+20+1], v[12:13], v[vgprValuC+20:vgprValuC+20+1] // C += bias
v_pk_add_f32 v[vgprValuC+22:vgprValuC+22+1], v[14:15], v[vgprValuC+22:vgprValuC+22+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+20]          // k1 * x
v_fma_f32 v4, v[vgprValuC+20], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+20], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+20], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+20], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+21]          // k1 * x
v_fma_f32 v4, v[vgprValuC+21], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+21], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+21], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+21], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+22]          // k1 * x
v_fma_f32 v4, v[vgprValuC+22], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+22], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+22], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+22], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+23]          // k1 * x
v_fma_f32 v4, v[vgprValuC+23], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+23], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+23], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+23], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+20], v[vgprValuC+20]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+21], v[vgprValuC+21]     // convert C to fp16
v_pack_b32_f16 v20, v[vgprValuC+20], v[vgprValuC+21] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+22], v[vgprValuC+22]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+23], v[vgprValuC+23]     // convert C to fp16
v_pack_b32_f16 v21, v[vgprValuC+22], v[vgprValuC+23] // Pack with neighbor
buffer_store_dwordx2 v[20:21], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[16:17], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[18:19], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+28:vgprValuC+28+1], v[12:13], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[vgprValuC+30:vgprValuC+30+1], v[14:15], v[vgprValuC+30:vgprValuC+30+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+28]          // k1 * x
v_fma_f32 v4, v[vgprValuC+28], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+28], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+28], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+28], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+29]          // k1 * x
v_fma_f32 v4, v[vgprValuC+29], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+29], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+29], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+29], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+30]          // k1 * x
v_fma_f32 v4, v[vgprValuC+30], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+30], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+30], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+30], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+31]          // k1 * x
v_fma_f32 v4, v[vgprValuC+31], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+31], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+31], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+31], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+28], v[vgprValuC+28]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+29], v[vgprValuC+29]     // convert C to fp16
v_pack_b32_f16 v28, v[vgprValuC+28], v[vgprValuC+29] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+30], v[vgprValuC+30]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+31], v[vgprValuC+31]     // convert C to fp16
v_pack_b32_f16 v29, v[vgprValuC+30], v[vgprValuC+31] // Pack with neighbor
buffer_store_dwordx2 v[28:29], v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[12:13], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[14:15], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+36]          // k1 * x
v_fma_f32 v4, v[vgprValuC+36], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+36], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+36], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+36], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+37]          // k1 * x
v_fma_f32 v4, v[vgprValuC+37], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+37], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+37], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+37], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+38]          // k1 * x
v_fma_f32 v4, v[vgprValuC+38], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+38], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+38], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+38], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+39]          // k1 * x
v_fma_f32 v4, v[vgprValuC+39], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+39], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+39], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+39], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
buffer_store_dwordx2 v[36:37], v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v16, 1.0, v16, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(16)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v19, 1.0, v19, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(16)(2)
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[12:13], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[14:15], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+44]          // k1 * x
v_fma_f32 v4, v[vgprValuC+44], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+44], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+44], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+44], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+45]          // k1 * x
v_fma_f32 v4, v[vgprValuC+45], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+45], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+45], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+45], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+46]          // k1 * x
v_fma_f32 v4, v[vgprValuC+46], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+46], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+46], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+46], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+47]          // k1 * x
v_fma_f32 v4, v[vgprValuC+47], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+47], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+47], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+47], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
buffer_store_dwordx2 v[44:45], v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
label_Activation_End_Edge_1:
s_branch label_GW_End_1                            // jump to end
label_GW_Beta_1:
s_and_b32 s84, 255, s[sgprSizeI]                   // s84 = s[sgprSizeI] % 256
s_add_u32 s85, -0x1, s[sgprNumWorkGroups0]
s_cmp_ge_u32 s[sgprWorkGroup0], s85                // wg0 >= nwg0-1 ?
s_cselect_b32 s84, s84, 0                          // set rMT0
s_cmpk_gt_u32 s84, 0x0                             // rMT0 > 0
s_cbranch_scc1 label_GW_B1_E1_1                    // jump if edges required
s_and_b32 s84, 15, s[sgprSizeJ]                    // s84 = s[sgprSizeJ] % 16
s_add_u32 s85, -0x1, s[sgprNumWorkGroups1]
s_cmp_ge_u32 s[sgprWorkGroup1], s85                // wg1 >= nwg1-1
s_cselect_b32 s84, s84, 0                          // set rMT1
s_cmpk_gt_u32 s84, 0x0                             // rMT1 > 0
s_cbranch_scc1 label_GW_B1_E1_1                    // jump if edges required
label_GW_B1_E0_1:

/* edge=0, allocate 2 sgpr. perBatchTmpS=2 perBatchMaskS=0 perElementMaskS=0 elementsPerBatch=16 */
s_cmpk_eq_u32 s[sgprActivationType], 0             // activationType == 0
s_cbranch_scc1 label_Activation_None_Beta_1        // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 1             // activationType == 1
s_cbranch_scc1 label_Activation_Abs_Beta_1         // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 2             // activationType == 2
s_cbranch_scc1 label_Activation_Clippedrelu_Beta_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 3             // activationType == 3
s_cbranch_scc1 label_Activation_Gelu_Beta_1        // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 4             // activationType == 4
s_cbranch_scc1 label_Activation_Leakyrelu_Beta_1   // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 5             // activationType == 5
s_cbranch_scc1 label_Activation_Relu_Beta_1        // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 6             // activationType == 6
s_cbranch_scc1 label_Activation_Sigmoid_Beta_1     // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 7             // activationType == 7
s_cbranch_scc1 label_Activation_Tanh_Beta_1        // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 9             // activationType == 9
s_cbranch_scc1 label_Activation_Geluscaling_Beta_1 // Branch if true
label_Activation_None_Beta_1:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Beta Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_add_lshl_u32 v10, v2, v0, 0x1                    // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
buffer_load_dwordx2 v[14:15], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v11, v0, s66
v_lshlrev_b32 v11, 0x2, v11                        // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v11 offset:0                // load bias
v_lshlrev_b32 v12, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v12, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[28:29], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[30:31], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[40:41], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
v_add_lshl_u32 v9, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(0), vmcnt(3)                     // vmcnt(3) = 5 - 1 (beta) - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v14, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v14, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v15, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v15, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(2) = 5 - 2 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v28, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v28, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v29, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v29, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(1) = 5 - 3 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v30, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v30, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v31, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v31, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(0) = 5 - 4 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[20:21], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[22:23], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+44], s[sgprBeta], v40, v[vgprValuC+44] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+45], s[sgprBeta], v40, v[vgprValuC+45] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+46], s[sgprBeta], v41, v[vgprValuC+46] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+47], s[sgprBeta], v41, v[vgprValuC+47] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[44:45], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta_1
label_Activation_Abs_Beta_1:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Beta Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_add_lshl_u32 v10, v2, v0, 0x1                    // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
buffer_load_dwordx2 v[14:15], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v11, v0, s66
v_lshlrev_b32 v11, 0x2, v11                        // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v11 offset:0                // load bias
v_lshlrev_b32 v12, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v12, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[28:29], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[30:31], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[40:41], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
v_add_lshl_u32 v9, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(0), vmcnt(3)                     // vmcnt(3) = 5 - 1 (beta) - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v14, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v14, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v15, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v15, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_and_b32 v[vgprValuC+24], 0x7fffffff, v[vgprValuC+24] // Remove sign bit
v_and_b32 v[vgprValuC+25], 0x7fffffff, v[vgprValuC+25] // Remove sign bit
v_and_b32 v[vgprValuC+26], 0x7fffffff, v[vgprValuC+26] // Remove sign bit
v_and_b32 v[vgprValuC+27], 0x7fffffff, v[vgprValuC+27] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(2) = 5 - 2 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v28, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v28, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v29, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v29, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_and_b32 v[vgprValuC+32], 0x7fffffff, v[vgprValuC+32] // Remove sign bit
v_and_b32 v[vgprValuC+33], 0x7fffffff, v[vgprValuC+33] // Remove sign bit
v_and_b32 v[vgprValuC+34], 0x7fffffff, v[vgprValuC+34] // Remove sign bit
v_and_b32 v[vgprValuC+35], 0x7fffffff, v[vgprValuC+35] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(1) = 5 - 3 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v30, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v30, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v31, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v31, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_and_b32 v[vgprValuC+36], 0x7fffffff, v[vgprValuC+36] // Remove sign bit
v_and_b32 v[vgprValuC+37], 0x7fffffff, v[vgprValuC+37] // Remove sign bit
v_and_b32 v[vgprValuC+38], 0x7fffffff, v[vgprValuC+38] // Remove sign bit
v_and_b32 v[vgprValuC+39], 0x7fffffff, v[vgprValuC+39] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(0) = 5 - 4 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[20:21], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[22:23], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+44], s[sgprBeta], v40, v[vgprValuC+44] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+45], s[sgprBeta], v40, v[vgprValuC+45] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+46], s[sgprBeta], v41, v[vgprValuC+46] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+47], s[sgprBeta], v41, v[vgprValuC+47] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_and_b32 v[vgprValuC+44], 0x7fffffff, v[vgprValuC+44] // Remove sign bit
v_and_b32 v[vgprValuC+45], 0x7fffffff, v[vgprValuC+45] // Remove sign bit
v_and_b32 v[vgprValuC+46], 0x7fffffff, v[vgprValuC+46] // Remove sign bit
v_and_b32 v[vgprValuC+47], 0x7fffffff, v[vgprValuC+47] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[44:45], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta_1
label_Activation_Clippedrelu_Beta_1:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Beta Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_add_lshl_u32 v10, v2, v0, 0x1                    // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
buffer_load_dwordx2 v[14:15], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v11, v0, s66
v_lshlrev_b32 v11, 0x2, v11                        // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v11 offset:0                // load bias
v_lshlrev_b32 v12, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v12, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[28:29], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[30:31], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[40:41], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
v_add_lshl_u32 v9, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(0), vmcnt(3)                     // vmcnt(3) = 5 - 1 (beta) - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v14, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v14, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v15, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v15, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+24], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+24], s[sgpractivationBeta], v[vgprValuC+24] // min(x, beta)
v_cndmask_b32 v[vgprValuC+24], 0.0, v[vgprValuC+24], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+25], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+25], s[sgpractivationBeta], v[vgprValuC+25] // min(x, beta)
v_cndmask_b32 v[vgprValuC+25], 0.0, v[vgprValuC+25], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+26], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+26], s[sgpractivationBeta], v[vgprValuC+26] // min(x, beta)
v_cndmask_b32 v[vgprValuC+26], 0.0, v[vgprValuC+26], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+27], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+27], s[sgpractivationBeta], v[vgprValuC+27] // min(x, beta)
v_cndmask_b32 v[vgprValuC+27], 0.0, v[vgprValuC+27], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(2) = 5 - 2 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v28, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v28, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v29, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v29, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+32], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+32], s[sgpractivationBeta], v[vgprValuC+32] // min(x, beta)
v_cndmask_b32 v[vgprValuC+32], 0.0, v[vgprValuC+32], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+33], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+33], s[sgpractivationBeta], v[vgprValuC+33] // min(x, beta)
v_cndmask_b32 v[vgprValuC+33], 0.0, v[vgprValuC+33], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+34], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+34], s[sgpractivationBeta], v[vgprValuC+34] // min(x, beta)
v_cndmask_b32 v[vgprValuC+34], 0.0, v[vgprValuC+34], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+35], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+35], s[sgpractivationBeta], v[vgprValuC+35] // min(x, beta)
v_cndmask_b32 v[vgprValuC+35], 0.0, v[vgprValuC+35], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(1) = 5 - 3 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v30, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v30, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v31, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v31, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+36], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+36], s[sgpractivationBeta], v[vgprValuC+36] // min(x, beta)
v_cndmask_b32 v[vgprValuC+36], 0.0, v[vgprValuC+36], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+37], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+37], s[sgpractivationBeta], v[vgprValuC+37] // min(x, beta)
v_cndmask_b32 v[vgprValuC+37], 0.0, v[vgprValuC+37], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+38], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+38], s[sgpractivationBeta], v[vgprValuC+38] // min(x, beta)
v_cndmask_b32 v[vgprValuC+38], 0.0, v[vgprValuC+38], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+39], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+39], s[sgpractivationBeta], v[vgprValuC+39] // min(x, beta)
v_cndmask_b32 v[vgprValuC+39], 0.0, v[vgprValuC+39], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(0) = 5 - 4 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[20:21], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[22:23], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+44], s[sgprBeta], v40, v[vgprValuC+44] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+45], s[sgprBeta], v40, v[vgprValuC+45] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+46], s[sgprBeta], v41, v[vgprValuC+46] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+47], s[sgprBeta], v41, v[vgprValuC+47] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+44], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+44], s[sgpractivationBeta], v[vgprValuC+44] // min(x, beta)
v_cndmask_b32 v[vgprValuC+44], 0.0, v[vgprValuC+44], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+45], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+45], s[sgpractivationBeta], v[vgprValuC+45] // min(x, beta)
v_cndmask_b32 v[vgprValuC+45], 0.0, v[vgprValuC+45], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+46], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+46], s[sgpractivationBeta], v[vgprValuC+46] // min(x, beta)
v_cndmask_b32 v[vgprValuC+46], 0.0, v[vgprValuC+46], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+47], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+47], s[sgpractivationBeta], v[vgprValuC+47] // min(x, beta)
v_cndmask_b32 v[vgprValuC+47], 0.0, v[vgprValuC+47], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[44:45], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta_1
label_Activation_Gelu_Beta_1:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Beta Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_add_lshl_u32 v10, v2, v0, 0x1                    // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
buffer_load_dwordx2 v[14:15], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v11, v0, s66
v_lshlrev_b32 v11, 0x2, v11                        // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v11 offset:0                // load bias
v_lshlrev_b32 v12, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v12, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[28:29], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[30:31], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[40:41], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
v_add_lshl_u32 v9, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(0), vmcnt(3)                     // vmcnt(3) = 5 - 1 (beta) - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v14, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v14, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v15, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v15, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+24]          // k1 * x
v_fma_f32 v4, v[vgprValuC+24], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+24], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+24], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+24], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+25]          // k1 * x
v_fma_f32 v4, v[vgprValuC+25], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+25], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+25], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+25], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+26]          // k1 * x
v_fma_f32 v4, v[vgprValuC+26], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+26], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+26], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+26], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+27]          // k1 * x
v_fma_f32 v4, v[vgprValuC+27], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+27], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+27], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+27], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(2) = 5 - 2 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v28, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v28, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v29, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v29, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+32]          // k1 * x
v_fma_f32 v4, v[vgprValuC+32], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+32], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+32], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+32], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+33]          // k1 * x
v_fma_f32 v4, v[vgprValuC+33], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+33], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+33], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+33], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+34]          // k1 * x
v_fma_f32 v4, v[vgprValuC+34], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+34], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+34], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+34], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+35]          // k1 * x
v_fma_f32 v4, v[vgprValuC+35], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+35], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+35], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+35], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(1) = 5 - 3 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v30, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v30, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v31, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v31, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+36]          // k1 * x
v_fma_f32 v4, v[vgprValuC+36], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+36], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+36], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+36], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+37]          // k1 * x
v_fma_f32 v4, v[vgprValuC+37], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+37], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+37], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+37], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+38]          // k1 * x
v_fma_f32 v4, v[vgprValuC+38], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+38], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+38], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+38], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+39]          // k1 * x
v_fma_f32 v4, v[vgprValuC+39], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+39], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+39], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+39], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(0) = 5 - 4 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[20:21], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[22:23], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+44], s[sgprBeta], v40, v[vgprValuC+44] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+45], s[sgprBeta], v40, v[vgprValuC+45] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+46], s[sgprBeta], v41, v[vgprValuC+46] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+47], s[sgprBeta], v41, v[vgprValuC+47] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+44]          // k1 * x
v_fma_f32 v4, v[vgprValuC+44], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+44], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+44], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+44], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+45]          // k1 * x
v_fma_f32 v4, v[vgprValuC+45], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+45], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+45], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+45], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+46]          // k1 * x
v_fma_f32 v4, v[vgprValuC+46], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+46], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+46], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+46], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+47]          // k1 * x
v_fma_f32 v4, v[vgprValuC+47], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+47], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+47], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+47], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[44:45], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta_1
label_Activation_Leakyrelu_Beta_1:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Beta Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_add_lshl_u32 v10, v2, v0, 0x1                    // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
buffer_load_dwordx2 v[14:15], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v11, v0, s66
v_lshlrev_b32 v11, 0x2, v11                        // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v11 offset:0                // load bias
v_lshlrev_b32 v12, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v12, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[28:29], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[30:31], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[40:41], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
v_add_lshl_u32 v9, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(0), vmcnt(3)                     // vmcnt(3) = 5 - 1 (beta) - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v14, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v14, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v15, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v15, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+24] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+24], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+24], v4, v[vgprValuC+24], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+25] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+25], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+25], v4, v[vgprValuC+25], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+26] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+26], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+26], v4, v[vgprValuC+26], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+27] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+27], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+27], v4, v[vgprValuC+27], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(2) = 5 - 2 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v28, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v28, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v29, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v29, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+32] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+32], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+32], v4, v[vgprValuC+32], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+33] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+33], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+33], v4, v[vgprValuC+33], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+34] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+34], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+34], v4, v[vgprValuC+34], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+35] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+35], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+35], v4, v[vgprValuC+35], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(1) = 5 - 3 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v30, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v30, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v31, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v31, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+36] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+36], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+36], v4, v[vgprValuC+36], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+37] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+37], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+37], v4, v[vgprValuC+37], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+38] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+38], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+38], v4, v[vgprValuC+38], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+39] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+39], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+39], v4, v[vgprValuC+39], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(0) = 5 - 4 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[20:21], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[22:23], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+44], s[sgprBeta], v40, v[vgprValuC+44] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+45], s[sgprBeta], v40, v[vgprValuC+45] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+46], s[sgprBeta], v41, v[vgprValuC+46] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+47], s[sgprBeta], v41, v[vgprValuC+47] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+44] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+44], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+44], v4, v[vgprValuC+44], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+45] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+45], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+45], v4, v[vgprValuC+45], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+46] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+46], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+46], v4, v[vgprValuC+46], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+47] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+47], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+47], v4, v[vgprValuC+47], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[44:45], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta_1
label_Activation_Relu_Beta_1:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Beta Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_add_lshl_u32 v10, v2, v0, 0x1                    // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
buffer_load_dwordx2 v[14:15], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v11, v0, s66
v_lshlrev_b32 v11, 0x2, v11                        // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v11 offset:0                // load bias
v_lshlrev_b32 v12, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v12, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[28:29], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[30:31], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[40:41], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
v_add_lshl_u32 v9, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(0), vmcnt(3)                     // vmcnt(3) = 5 - 1 (beta) - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v14, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v14, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v15, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v15, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_max_f32 v[vgprValuC+24], v[vgprValuC+24], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+25], v[vgprValuC+25], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+26], v[vgprValuC+26], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+27], v[vgprValuC+27], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(2) = 5 - 2 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v28, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v28, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v29, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v29, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_max_f32 v[vgprValuC+32], v[vgprValuC+32], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+33], v[vgprValuC+33], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+34], v[vgprValuC+34], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+35], v[vgprValuC+35], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(1) = 5 - 3 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v30, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v30, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v31, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v31, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_max_f32 v[vgprValuC+36], v[vgprValuC+36], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+37], v[vgprValuC+37], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+38], v[vgprValuC+38], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+39], v[vgprValuC+39], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(0) = 5 - 4 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[20:21], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[22:23], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+44], s[sgprBeta], v40, v[vgprValuC+44] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+45], s[sgprBeta], v40, v[vgprValuC+45] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+46], s[sgprBeta], v41, v[vgprValuC+46] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+47], s[sgprBeta], v41, v[vgprValuC+47] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_max_f32 v[vgprValuC+44], v[vgprValuC+44], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+45], v[vgprValuC+45], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+46], v[vgprValuC+46], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+47], v[vgprValuC+47], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[44:45], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta_1
label_Activation_Sigmoid_Beta_1:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Beta Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_add_lshl_u32 v10, v2, v0, 0x1                    // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
buffer_load_dwordx2 v[14:15], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v11, v0, s66
v_lshlrev_b32 v11, 0x2, v11                        // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v11 offset:0                // load bias
v_lshlrev_b32 v12, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v12, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[28:29], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[30:31], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[40:41], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
v_add_lshl_u32 v9, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(0), vmcnt(3)                     // vmcnt(3) = 5 - 1 (beta) - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v14, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v14, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v15, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v15, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v[vgprValuC+24], 0xbfb8aa3b, v[vgprValuC+24] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+24], v[vgprValuC+24]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+24], 1.0, v[vgprValuC+24]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+24], v[vgprValuC+24]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+25], 0xbfb8aa3b, v[vgprValuC+25] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+25], v[vgprValuC+25]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+25], 1.0, v[vgprValuC+25]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+25], v[vgprValuC+25]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+26], 0xbfb8aa3b, v[vgprValuC+26] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+26], v[vgprValuC+26]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+26], 1.0, v[vgprValuC+26]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+26], v[vgprValuC+26]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+27], 0xbfb8aa3b, v[vgprValuC+27] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+27], v[vgprValuC+27]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+27], 1.0, v[vgprValuC+27]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+27], v[vgprValuC+27]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(2) = 5 - 2 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v28, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v28, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v29, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v29, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v[vgprValuC+32], 0xbfb8aa3b, v[vgprValuC+32] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+32], v[vgprValuC+32]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+32], 1.0, v[vgprValuC+32]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+32], v[vgprValuC+32]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+33], 0xbfb8aa3b, v[vgprValuC+33] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+33], v[vgprValuC+33]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+33], 1.0, v[vgprValuC+33]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+33], v[vgprValuC+33]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+34], 0xbfb8aa3b, v[vgprValuC+34] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+34], v[vgprValuC+34]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+34], 1.0, v[vgprValuC+34]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+34], v[vgprValuC+34]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+35], 0xbfb8aa3b, v[vgprValuC+35] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+35], v[vgprValuC+35]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+35], 1.0, v[vgprValuC+35]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+35], v[vgprValuC+35]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(1) = 5 - 3 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v30, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v30, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v31, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v31, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v[vgprValuC+36], 0xbfb8aa3b, v[vgprValuC+36] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+36], v[vgprValuC+36]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+36], 1.0, v[vgprValuC+36]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+36], v[vgprValuC+36]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+37], 0xbfb8aa3b, v[vgprValuC+37] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+37], v[vgprValuC+37]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+37], 1.0, v[vgprValuC+37]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+37], v[vgprValuC+37]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+38], 0xbfb8aa3b, v[vgprValuC+38] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+38], v[vgprValuC+38]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+38], 1.0, v[vgprValuC+38]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+38], v[vgprValuC+38]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+39], 0xbfb8aa3b, v[vgprValuC+39] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+39], v[vgprValuC+39]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+39], 1.0, v[vgprValuC+39]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+39], v[vgprValuC+39]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(0) = 5 - 4 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[20:21], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[22:23], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+44], s[sgprBeta], v40, v[vgprValuC+44] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+45], s[sgprBeta], v40, v[vgprValuC+45] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+46], s[sgprBeta], v41, v[vgprValuC+46] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+47], s[sgprBeta], v41, v[vgprValuC+47] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_mul_f32 v[vgprValuC+44], 0xbfb8aa3b, v[vgprValuC+44] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+44], v[vgprValuC+44]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+44], 1.0, v[vgprValuC+44]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+44], v[vgprValuC+44]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+45], 0xbfb8aa3b, v[vgprValuC+45] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+45], v[vgprValuC+45]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+45], 1.0, v[vgprValuC+45]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+45], v[vgprValuC+45]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+46], 0xbfb8aa3b, v[vgprValuC+46] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+46], v[vgprValuC+46]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+46], 1.0, v[vgprValuC+46]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+46], v[vgprValuC+46]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+47], 0xbfb8aa3b, v[vgprValuC+47] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+47], v[vgprValuC+47]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+47], 1.0, v[vgprValuC+47]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+47], v[vgprValuC+47]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[44:45], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta_1
label_Activation_Tanh_Beta_1:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Beta Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_add_lshl_u32 v10, v2, v0, 0x1                    // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
buffer_load_dwordx2 v[14:15], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v11, v0, s66
v_lshlrev_b32 v11, 0x2, v11                        // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v11 offset:0                // load bias
v_lshlrev_b32 v12, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v12, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[28:29], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[30:31], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[40:41], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
v_add_lshl_u32 v9, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(0), vmcnt(3)                     // vmcnt(3) = 5 - 1 (beta) - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v14, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v14, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v15, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v15, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v[vgprValuC+24], s[sgpractivationAlpha], v[vgprValuC+24] // x * alpha
v_mul_f32 v[vgprValuC+24], 0x4038aa3b, v[vgprValuC+24] //  (fused 2)
v_exp_f32 v[vgprValuC+24], v[vgprValuC+24]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+24], 1.0, v[vgprValuC+24]    // e^2x + 1
v_rcp_f32 v[vgprValuC+24], v[vgprValuC+24]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+24], -2.0, v[vgprValuC+24], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+24], s[sgpractivationBeta], v[vgprValuC+24] // beta * tanh(x)
v_mul_f32 v[vgprValuC+25], s[sgpractivationAlpha], v[vgprValuC+25] // x * alpha
v_mul_f32 v[vgprValuC+25], 0x4038aa3b, v[vgprValuC+25] //  (fused 2)
v_exp_f32 v[vgprValuC+25], v[vgprValuC+25]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+25], 1.0, v[vgprValuC+25]    // e^2x + 1
v_rcp_f32 v[vgprValuC+25], v[vgprValuC+25]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+25], -2.0, v[vgprValuC+25], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+25], s[sgpractivationBeta], v[vgprValuC+25] // beta * tanh(x)
v_mul_f32 v[vgprValuC+26], s[sgpractivationAlpha], v[vgprValuC+26] // x * alpha
v_mul_f32 v[vgprValuC+26], 0x4038aa3b, v[vgprValuC+26] //  (fused 2)
v_exp_f32 v[vgprValuC+26], v[vgprValuC+26]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+26], 1.0, v[vgprValuC+26]    // e^2x + 1
v_rcp_f32 v[vgprValuC+26], v[vgprValuC+26]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+26], -2.0, v[vgprValuC+26], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+26], s[sgpractivationBeta], v[vgprValuC+26] // beta * tanh(x)
v_mul_f32 v[vgprValuC+27], s[sgpractivationAlpha], v[vgprValuC+27] // x * alpha
v_mul_f32 v[vgprValuC+27], 0x4038aa3b, v[vgprValuC+27] //  (fused 2)
v_exp_f32 v[vgprValuC+27], v[vgprValuC+27]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+27], 1.0, v[vgprValuC+27]    // e^2x + 1
v_rcp_f32 v[vgprValuC+27], v[vgprValuC+27]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+27], -2.0, v[vgprValuC+27], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+27], s[sgpractivationBeta], v[vgprValuC+27] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(2) = 5 - 2 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v28, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v28, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v29, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v29, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v[vgprValuC+32], s[sgpractivationAlpha], v[vgprValuC+32] // x * alpha
v_mul_f32 v[vgprValuC+32], 0x4038aa3b, v[vgprValuC+32] //  (fused 2)
v_exp_f32 v[vgprValuC+32], v[vgprValuC+32]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+32], 1.0, v[vgprValuC+32]    // e^2x + 1
v_rcp_f32 v[vgprValuC+32], v[vgprValuC+32]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+32], -2.0, v[vgprValuC+32], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+32], s[sgpractivationBeta], v[vgprValuC+32] // beta * tanh(x)
v_mul_f32 v[vgprValuC+33], s[sgpractivationAlpha], v[vgprValuC+33] // x * alpha
v_mul_f32 v[vgprValuC+33], 0x4038aa3b, v[vgprValuC+33] //  (fused 2)
v_exp_f32 v[vgprValuC+33], v[vgprValuC+33]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+33], 1.0, v[vgprValuC+33]    // e^2x + 1
v_rcp_f32 v[vgprValuC+33], v[vgprValuC+33]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+33], -2.0, v[vgprValuC+33], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+33], s[sgpractivationBeta], v[vgprValuC+33] // beta * tanh(x)
v_mul_f32 v[vgprValuC+34], s[sgpractivationAlpha], v[vgprValuC+34] // x * alpha
v_mul_f32 v[vgprValuC+34], 0x4038aa3b, v[vgprValuC+34] //  (fused 2)
v_exp_f32 v[vgprValuC+34], v[vgprValuC+34]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+34], 1.0, v[vgprValuC+34]    // e^2x + 1
v_rcp_f32 v[vgprValuC+34], v[vgprValuC+34]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+34], -2.0, v[vgprValuC+34], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+34], s[sgpractivationBeta], v[vgprValuC+34] // beta * tanh(x)
v_mul_f32 v[vgprValuC+35], s[sgpractivationAlpha], v[vgprValuC+35] // x * alpha
v_mul_f32 v[vgprValuC+35], 0x4038aa3b, v[vgprValuC+35] //  (fused 2)
v_exp_f32 v[vgprValuC+35], v[vgprValuC+35]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+35], 1.0, v[vgprValuC+35]    // e^2x + 1
v_rcp_f32 v[vgprValuC+35], v[vgprValuC+35]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+35], -2.0, v[vgprValuC+35], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+35], s[sgpractivationBeta], v[vgprValuC+35] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(1) = 5 - 3 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v30, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v30, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v31, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v31, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v[vgprValuC+36], s[sgpractivationAlpha], v[vgprValuC+36] // x * alpha
v_mul_f32 v[vgprValuC+36], 0x4038aa3b, v[vgprValuC+36] //  (fused 2)
v_exp_f32 v[vgprValuC+36], v[vgprValuC+36]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+36], 1.0, v[vgprValuC+36]    // e^2x + 1
v_rcp_f32 v[vgprValuC+36], v[vgprValuC+36]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+36], -2.0, v[vgprValuC+36], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+36], s[sgpractivationBeta], v[vgprValuC+36] // beta * tanh(x)
v_mul_f32 v[vgprValuC+37], s[sgpractivationAlpha], v[vgprValuC+37] // x * alpha
v_mul_f32 v[vgprValuC+37], 0x4038aa3b, v[vgprValuC+37] //  (fused 2)
v_exp_f32 v[vgprValuC+37], v[vgprValuC+37]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+37], 1.0, v[vgprValuC+37]    // e^2x + 1
v_rcp_f32 v[vgprValuC+37], v[vgprValuC+37]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+37], -2.0, v[vgprValuC+37], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+37], s[sgpractivationBeta], v[vgprValuC+37] // beta * tanh(x)
v_mul_f32 v[vgprValuC+38], s[sgpractivationAlpha], v[vgprValuC+38] // x * alpha
v_mul_f32 v[vgprValuC+38], 0x4038aa3b, v[vgprValuC+38] //  (fused 2)
v_exp_f32 v[vgprValuC+38], v[vgprValuC+38]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+38], 1.0, v[vgprValuC+38]    // e^2x + 1
v_rcp_f32 v[vgprValuC+38], v[vgprValuC+38]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+38], -2.0, v[vgprValuC+38], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+38], s[sgpractivationBeta], v[vgprValuC+38] // beta * tanh(x)
v_mul_f32 v[vgprValuC+39], s[sgpractivationAlpha], v[vgprValuC+39] // x * alpha
v_mul_f32 v[vgprValuC+39], 0x4038aa3b, v[vgprValuC+39] //  (fused 2)
v_exp_f32 v[vgprValuC+39], v[vgprValuC+39]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+39], 1.0, v[vgprValuC+39]    // e^2x + 1
v_rcp_f32 v[vgprValuC+39], v[vgprValuC+39]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+39], -2.0, v[vgprValuC+39], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+39], s[sgpractivationBeta], v[vgprValuC+39] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(0) = 5 - 4 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[20:21], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[22:23], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+44], s[sgprBeta], v40, v[vgprValuC+44] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+45], s[sgprBeta], v40, v[vgprValuC+45] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+46], s[sgprBeta], v41, v[vgprValuC+46] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+47], s[sgprBeta], v41, v[vgprValuC+47] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_mul_f32 v[vgprValuC+44], s[sgpractivationAlpha], v[vgprValuC+44] // x * alpha
v_mul_f32 v[vgprValuC+44], 0x4038aa3b, v[vgprValuC+44] //  (fused 2)
v_exp_f32 v[vgprValuC+44], v[vgprValuC+44]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+44], 1.0, v[vgprValuC+44]    // e^2x + 1
v_rcp_f32 v[vgprValuC+44], v[vgprValuC+44]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+44], -2.0, v[vgprValuC+44], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+44], s[sgpractivationBeta], v[vgprValuC+44] // beta * tanh(x)
v_mul_f32 v[vgprValuC+45], s[sgpractivationAlpha], v[vgprValuC+45] // x * alpha
v_mul_f32 v[vgprValuC+45], 0x4038aa3b, v[vgprValuC+45] //  (fused 2)
v_exp_f32 v[vgprValuC+45], v[vgprValuC+45]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+45], 1.0, v[vgprValuC+45]    // e^2x + 1
v_rcp_f32 v[vgprValuC+45], v[vgprValuC+45]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+45], -2.0, v[vgprValuC+45], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+45], s[sgpractivationBeta], v[vgprValuC+45] // beta * tanh(x)
v_mul_f32 v[vgprValuC+46], s[sgpractivationAlpha], v[vgprValuC+46] // x * alpha
v_mul_f32 v[vgprValuC+46], 0x4038aa3b, v[vgprValuC+46] //  (fused 2)
v_exp_f32 v[vgprValuC+46], v[vgprValuC+46]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+46], 1.0, v[vgprValuC+46]    // e^2x + 1
v_rcp_f32 v[vgprValuC+46], v[vgprValuC+46]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+46], -2.0, v[vgprValuC+46], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+46], s[sgpractivationBeta], v[vgprValuC+46] // beta * tanh(x)
v_mul_f32 v[vgprValuC+47], s[sgpractivationAlpha], v[vgprValuC+47] // x * alpha
v_mul_f32 v[vgprValuC+47], 0x4038aa3b, v[vgprValuC+47] //  (fused 2)
v_exp_f32 v[vgprValuC+47], v[vgprValuC+47]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+47], 1.0, v[vgprValuC+47]    // e^2x + 1
v_rcp_f32 v[vgprValuC+47], v[vgprValuC+47]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+47], -2.0, v[vgprValuC+47], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+47], s[sgpractivationBeta], v[vgprValuC+47] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[44:45], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta_1
label_Activation_Geluscaling_Beta_1:
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 */

/******************************************/
/* Global Write Beta Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_add_lshl_u32 v10, v2, v0, 0x1                    // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
buffer_load_dwordx2 v[14:15], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s66, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v11, v0, s66
v_lshlrev_b32 v11, 0x2, v11                        // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v11 offset:0                // load bias
v_lshlrev_b32 v12, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v12, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[28:29], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[30:31], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
s_lshl_b32 s66, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[40:41], v10, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
v_add_lshl_u32 v9, v3, v0, 0x1                     // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+44], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+45], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+46], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+47], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha

/* apply mask, calc new C and issue writes */

s_waitcnt lgkmcnt(0), vmcnt(3)                     // vmcnt(3) = 5 - 1 (beta) - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v14, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v14, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v15, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v15, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+24]          // k1 * x
v_fma_f32 v4, v[vgprValuC+24], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+24], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+24], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+24], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+25]          // k1 * x
v_fma_f32 v4, v[vgprValuC+25], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+25], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+25], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+25], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+26]          // k1 * x
v_fma_f32 v4, v[vgprValuC+26], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+26], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+26], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+26], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+27]          // k1 * x
v_fma_f32 v4, v[vgprValuC+27], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+27], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+27], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+27], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(2) = 5 - 2 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v28, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v28, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v29, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v29, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+32]          // k1 * x
v_fma_f32 v4, v[vgprValuC+32], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+32], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+32], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+32], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+33]          // k1 * x
v_fma_f32 v4, v[vgprValuC+33], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+33], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+33], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+33], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+34]          // k1 * x
v_fma_f32 v4, v[vgprValuC+34], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+34], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+34], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+34], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+35]          // k1 * x
v_fma_f32 v4, v[vgprValuC+35], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+35], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+35], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+35], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(1) = 5 - 3 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+36], s[sgprBeta], v30, v[vgprValuC+36] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+37], s[sgprBeta], v30, v[vgprValuC+37] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+38], s[sgprBeta], v31, v[vgprValuC+38] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+39], s[sgprBeta], v31, v[vgprValuC+39] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+36:vgprValuC+36+1], v[16:17], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[vgprValuC+38:vgprValuC+38+1], v[18:19], v[vgprValuC+38:vgprValuC+38+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+36]          // k1 * x
v_fma_f32 v4, v[vgprValuC+36], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+36], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+36], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+36], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+37]          // k1 * x
v_fma_f32 v4, v[vgprValuC+37], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+37], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+37], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+37], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+38]          // k1 * x
v_fma_f32 v4, v[vgprValuC+38], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+38], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+38], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+38], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+39]          // k1 * x
v_fma_f32 v4, v[vgprValuC+39], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+39], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+39], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+39], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+36], v[vgprValuC+36]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+37], v[vgprValuC+37]     // convert C to fp16
v_pack_b32_f16 v36, v[vgprValuC+36], v[vgprValuC+37] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+38], v[vgprValuC+38]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+39], v[vgprValuC+39]     // convert C to fp16
v_pack_b32_f16 v37, v[vgprValuC+38], v[vgprValuC+39] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

s_waitcnt vmcnt(3)                                 // vmcnt(0) = 5 - 4 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[20:21], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[22:23], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+44], s[sgprBeta], v40, v[vgprValuC+44] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+45], s[sgprBeta], v40, v[vgprValuC+45] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+46], s[sgprBeta], v41, v[vgprValuC+46] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+47], s[sgprBeta], v41, v[vgprValuC+47] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+44:vgprValuC+44+1], v[16:17], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[vgprValuC+46:vgprValuC+46+1], v[18:19], v[vgprValuC+46:vgprValuC+46+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+44]          // k1 * x
v_fma_f32 v4, v[vgprValuC+44], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+44], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+44], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+44], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+45]          // k1 * x
v_fma_f32 v4, v[vgprValuC+45], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+45], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+45], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+45], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+46]          // k1 * x
v_fma_f32 v4, v[vgprValuC+46], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+46], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+46], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+46], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+47]          // k1 * x
v_fma_f32 v4, v[vgprValuC+47], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+47], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+47], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+47], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+44], v[vgprValuC+44]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+45], v[vgprValuC+45]     // convert C to fp16
v_pack_b32_f16 v44, v[vgprValuC+44], v[vgprValuC+45] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+46], v[vgprValuC+46]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+47], v[vgprValuC+47]     // convert C to fp16
v_pack_b32_f16 v45, v[vgprValuC+46], v[vgprValuC+47] // Pack with neighbor
s_lshl_b32 s66, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s66        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[44:45], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
label_Activation_End_Beta_1:
s_branch label_GW_End_1                            // jump to end
label_GW_B1_E1_1:

/* edge=1, allocate 6 sgpr. perBatchTmpS=4 perBatchMaskS=2 perElementMaskS=0 elementsPerBatch=12 */
s_cmpk_eq_u32 s[sgprActivationType], 0             // activationType == 0
s_cbranch_scc1 label_Activation_None_Beta_Edge_1   // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 1             // activationType == 1
s_cbranch_scc1 label_Activation_Abs_Beta_Edge_1    // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 2             // activationType == 2
s_cbranch_scc1 label_Activation_Clippedrelu_Beta_Edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 3             // activationType == 3
s_cbranch_scc1 label_Activation_Gelu_Beta_Edge_1   // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 4             // activationType == 4
s_cbranch_scc1 label_Activation_Leakyrelu_Beta_Edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 5             // activationType == 5
s_cbranch_scc1 label_Activation_Relu_Beta_Edge_1   // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 6             // activationType == 6
s_cbranch_scc1 label_Activation_Sigmoid_Beta_Edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 7             // activationType == 7
s_cbranch_scc1 label_Activation_Tanh_Beta_Edge_1   // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 9             // activationType == 9
s_cbranch_scc1 label_Activation_Geluscaling_Beta_Edge_1 // Branch if true
label_Activation_None_Beta_Edge_1:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v47, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v9, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v47, v9, s[88:89]                // LDC clip if OOB. offset
buffer_load_dwordx2 v[12:13], v9, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v0, s84
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
v_cndmask_b32 v10, v47, v10, s[88:89]              // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v10 offset:0                // load bias
v_lshlrev_b32 v11, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v11, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v9, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v47, v9, s[88:89]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v14, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v47, v14, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[30:31], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v15, v0, s84
v_lshlrev_b32 v15, 0x2, v15                        // Bias address scaled by BPE
v_cndmask_b32 v15, v47, v15, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v28, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v14, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v47, v14, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v29, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v29, v47, v29, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[38:39], v29, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v36, v0, s84
v_lshlrev_b32 v36, 0x2, v36                        // Bias address scaled by BPE
v_cndmask_b32 v36, v47, v36, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v37, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v29, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v29, v47, v29, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v44, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v44, v47, v44, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[48:49], v44, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v45, v0, s84
v_lshlrev_b32 v45, 0x2, v45                        // Bias address scaled by BPE
v_cndmask_b32 v45, v47, v45, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v46, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v44, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v44, v47, v44, s[88:89]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+40], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+41], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+42], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+43], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+52], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+53], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+54], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+55], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v30, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v30, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v31, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v31, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v38, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+41], s[sgprBeta], v38, v[vgprValuC+41] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+42], s[sgprBeta], v39, v[vgprValuC+42] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+43], s[sgprBeta], v39, v[vgprValuC+43] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
buffer_store_dwordx2 v[40:41], v29, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+52], s[sgprBeta], v48, v[vgprValuC+52] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+53], s[sgprBeta], v48, v[vgprValuC+53] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+54], s[sgprBeta], v49, v[vgprValuC+54] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+55], s[sgprBeta], v49, v[vgprValuC+55] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+52:vgprValuC+52+1], v[16:17], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[vgprValuC+54:vgprValuC+54+1], v[18:19], v[vgprValuC+54:vgprValuC+54+1] // C += bias
v_cvt_f16_f32 v[vgprValuC+52], v[vgprValuC+52]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+53], v[vgprValuC+53]     // convert C to fp16
v_pack_b32_f16 v52, v[vgprValuC+52], v[vgprValuC+53] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+54], v[vgprValuC+54]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+55], v[vgprValuC+55]     // convert C to fp16
v_pack_b32_f16 v53, v[vgprValuC+54], v[vgprValuC+55] // Pack with neighbor
buffer_store_dwordx2 v[52:53], v44, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta_Edge_1
label_Activation_Abs_Beta_Edge_1:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v47, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v9, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v47, v9, s[88:89]                // LDC clip if OOB. offset
buffer_load_dwordx2 v[12:13], v9, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v0, s84
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
v_cndmask_b32 v10, v47, v10, s[88:89]              // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v10 offset:0                // load bias
v_lshlrev_b32 v11, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v11, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v9, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v47, v9, s[88:89]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v14, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v47, v14, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[30:31], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v15, v0, s84
v_lshlrev_b32 v15, 0x2, v15                        // Bias address scaled by BPE
v_cndmask_b32 v15, v47, v15, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v28, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v14, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v47, v14, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v29, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v29, v47, v29, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[38:39], v29, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v36, v0, s84
v_lshlrev_b32 v36, 0x2, v36                        // Bias address scaled by BPE
v_cndmask_b32 v36, v47, v36, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v37, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v29, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v29, v47, v29, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v44, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v44, v47, v44, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[48:49], v44, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v45, v0, s84
v_lshlrev_b32 v45, 0x2, v45                        // Bias address scaled by BPE
v_cndmask_b32 v45, v47, v45, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v46, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v44, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v44, v47, v44, s[88:89]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+40], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+41], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+42], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+43], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+52], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+53], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+54], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+55], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_and_b32 v[vgprValuC+24], 0x7fffffff, v[vgprValuC+24] // Remove sign bit
v_and_b32 v[vgprValuC+25], 0x7fffffff, v[vgprValuC+25] // Remove sign bit
v_and_b32 v[vgprValuC+26], 0x7fffffff, v[vgprValuC+26] // Remove sign bit
v_and_b32 v[vgprValuC+27], 0x7fffffff, v[vgprValuC+27] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v30, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v30, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v31, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v31, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_and_b32 v[vgprValuC+32], 0x7fffffff, v[vgprValuC+32] // Remove sign bit
v_and_b32 v[vgprValuC+33], 0x7fffffff, v[vgprValuC+33] // Remove sign bit
v_and_b32 v[vgprValuC+34], 0x7fffffff, v[vgprValuC+34] // Remove sign bit
v_and_b32 v[vgprValuC+35], 0x7fffffff, v[vgprValuC+35] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v38, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+41], s[sgprBeta], v38, v[vgprValuC+41] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+42], s[sgprBeta], v39, v[vgprValuC+42] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+43], s[sgprBeta], v39, v[vgprValuC+43] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_and_b32 v[vgprValuC+40], 0x7fffffff, v[vgprValuC+40] // Remove sign bit
v_and_b32 v[vgprValuC+41], 0x7fffffff, v[vgprValuC+41] // Remove sign bit
v_and_b32 v[vgprValuC+42], 0x7fffffff, v[vgprValuC+42] // Remove sign bit
v_and_b32 v[vgprValuC+43], 0x7fffffff, v[vgprValuC+43] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
buffer_store_dwordx2 v[40:41], v29, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+52], s[sgprBeta], v48, v[vgprValuC+52] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+53], s[sgprBeta], v48, v[vgprValuC+53] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+54], s[sgprBeta], v49, v[vgprValuC+54] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+55], s[sgprBeta], v49, v[vgprValuC+55] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+52:vgprValuC+52+1], v[16:17], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[vgprValuC+54:vgprValuC+54+1], v[18:19], v[vgprValuC+54:vgprValuC+54+1] // C += bias
v_and_b32 v[vgprValuC+52], 0x7fffffff, v[vgprValuC+52] // Remove sign bit
v_and_b32 v[vgprValuC+53], 0x7fffffff, v[vgprValuC+53] // Remove sign bit
v_and_b32 v[vgprValuC+54], 0x7fffffff, v[vgprValuC+54] // Remove sign bit
v_and_b32 v[vgprValuC+55], 0x7fffffff, v[vgprValuC+55] // Remove sign bit
v_cvt_f16_f32 v[vgprValuC+52], v[vgprValuC+52]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+53], v[vgprValuC+53]     // convert C to fp16
v_pack_b32_f16 v52, v[vgprValuC+52], v[vgprValuC+53] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+54], v[vgprValuC+54]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+55], v[vgprValuC+55]     // convert C to fp16
v_pack_b32_f16 v53, v[vgprValuC+54], v[vgprValuC+55] // Pack with neighbor
buffer_store_dwordx2 v[52:53], v44, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta_Edge_1
label_Activation_Clippedrelu_Beta_Edge_1:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v47, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v9, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v47, v9, s[88:89]                // LDC clip if OOB. offset
buffer_load_dwordx2 v[12:13], v9, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v0, s84
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
v_cndmask_b32 v10, v47, v10, s[88:89]              // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v10 offset:0                // load bias
v_lshlrev_b32 v11, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v11, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v9, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v47, v9, s[88:89]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v14, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v47, v14, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[30:31], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v15, v0, s84
v_lshlrev_b32 v15, 0x2, v15                        // Bias address scaled by BPE
v_cndmask_b32 v15, v47, v15, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v28, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v14, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v47, v14, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v29, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v29, v47, v29, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[38:39], v29, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v36, v0, s84
v_lshlrev_b32 v36, 0x2, v36                        // Bias address scaled by BPE
v_cndmask_b32 v36, v47, v36, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v37, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v29, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v29, v47, v29, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v44, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v44, v47, v44, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[48:49], v44, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v45, v0, s84
v_lshlrev_b32 v45, 0x2, v45                        // Bias address scaled by BPE
v_cndmask_b32 v45, v47, v45, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v46, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v44, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v44, v47, v44, s[88:89]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+40], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+41], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+42], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+43], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+52], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+53], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+54], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+55], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+24], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+24], s[sgpractivationBeta], v[vgprValuC+24] // min(x, beta)
v_cndmask_b32 v[vgprValuC+24], 0.0, v[vgprValuC+24], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+25], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+25], s[sgpractivationBeta], v[vgprValuC+25] // min(x, beta)
v_cndmask_b32 v[vgprValuC+25], 0.0, v[vgprValuC+25], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+26], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+26], s[sgpractivationBeta], v[vgprValuC+26] // min(x, beta)
v_cndmask_b32 v[vgprValuC+26], 0.0, v[vgprValuC+26], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+27], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+27], s[sgpractivationBeta], v[vgprValuC+27] // min(x, beta)
v_cndmask_b32 v[vgprValuC+27], 0.0, v[vgprValuC+27], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v30, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v30, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v31, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v31, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+32], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+32], s[sgpractivationBeta], v[vgprValuC+32] // min(x, beta)
v_cndmask_b32 v[vgprValuC+32], 0.0, v[vgprValuC+32], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+33], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+33], s[sgpractivationBeta], v[vgprValuC+33] // min(x, beta)
v_cndmask_b32 v[vgprValuC+33], 0.0, v[vgprValuC+33], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+34], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+34], s[sgpractivationBeta], v[vgprValuC+34] // min(x, beta)
v_cndmask_b32 v[vgprValuC+34], 0.0, v[vgprValuC+34], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+35], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+35], s[sgpractivationBeta], v[vgprValuC+35] // min(x, beta)
v_cndmask_b32 v[vgprValuC+35], 0.0, v[vgprValuC+35], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v38, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+41], s[sgprBeta], v38, v[vgprValuC+41] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+42], s[sgprBeta], v39, v[vgprValuC+42] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+43], s[sgprBeta], v39, v[vgprValuC+43] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+40], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+40], s[sgpractivationBeta], v[vgprValuC+40] // min(x, beta)
v_cndmask_b32 v[vgprValuC+40], 0.0, v[vgprValuC+40], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+41], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+41], s[sgpractivationBeta], v[vgprValuC+41] // min(x, beta)
v_cndmask_b32 v[vgprValuC+41], 0.0, v[vgprValuC+41], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+42], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+42], s[sgpractivationBeta], v[vgprValuC+42] // min(x, beta)
v_cndmask_b32 v[vgprValuC+42], 0.0, v[vgprValuC+42], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+43], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+43], s[sgpractivationBeta], v[vgprValuC+43] // min(x, beta)
v_cndmask_b32 v[vgprValuC+43], 0.0, v[vgprValuC+43], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
buffer_store_dwordx2 v[40:41], v29, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+52], s[sgprBeta], v48, v[vgprValuC+52] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+53], s[sgprBeta], v48, v[vgprValuC+53] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+54], s[sgprBeta], v49, v[vgprValuC+54] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+55], s[sgprBeta], v49, v[vgprValuC+55] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+52:vgprValuC+52+1], v[16:17], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[vgprValuC+54:vgprValuC+54+1], v[18:19], v[vgprValuC+54:vgprValuC+54+1] // C += bias
v_cmp_gt_f32 vcc, v[vgprValuC+52], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+52], s[sgpractivationBeta], v[vgprValuC+52] // min(x, beta)
v_cndmask_b32 v[vgprValuC+52], 0.0, v[vgprValuC+52], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+53], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+53], s[sgpractivationBeta], v[vgprValuC+53] // min(x, beta)
v_cndmask_b32 v[vgprValuC+53], 0.0, v[vgprValuC+53], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+54], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+54], s[sgpractivationBeta], v[vgprValuC+54] // min(x, beta)
v_cndmask_b32 v[vgprValuC+54], 0.0, v[vgprValuC+54], vcc // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v[vgprValuC+55], s[sgpractivationAlpha] // x > alpha ?
v_min_f32 v[vgprValuC+55], s[sgpractivationBeta], v[vgprValuC+55] // min(x, beta)
v_cndmask_b32 v[vgprValuC+55], 0.0, v[vgprValuC+55], vcc // set x to 0 if <= alpha
v_cvt_f16_f32 v[vgprValuC+52], v[vgprValuC+52]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+53], v[vgprValuC+53]     // convert C to fp16
v_pack_b32_f16 v52, v[vgprValuC+52], v[vgprValuC+53] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+54], v[vgprValuC+54]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+55], v[vgprValuC+55]     // convert C to fp16
v_pack_b32_f16 v53, v[vgprValuC+54], v[vgprValuC+55] // Pack with neighbor
buffer_store_dwordx2 v[52:53], v44, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta_Edge_1
label_Activation_Gelu_Beta_Edge_1:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v47, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v9, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v47, v9, s[88:89]                // LDC clip if OOB. offset
buffer_load_dwordx2 v[12:13], v9, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v0, s84
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
v_cndmask_b32 v10, v47, v10, s[88:89]              // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v10 offset:0                // load bias
v_lshlrev_b32 v11, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v11, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v9, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v47, v9, s[88:89]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v14, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v47, v14, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[30:31], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v15, v0, s84
v_lshlrev_b32 v15, 0x2, v15                        // Bias address scaled by BPE
v_cndmask_b32 v15, v47, v15, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v28, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v14, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v47, v14, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v29, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v29, v47, v29, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[38:39], v29, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v36, v0, s84
v_lshlrev_b32 v36, 0x2, v36                        // Bias address scaled by BPE
v_cndmask_b32 v36, v47, v36, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v37, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v29, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v29, v47, v29, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v44, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v44, v47, v44, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[48:49], v44, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v45, v0, s84
v_lshlrev_b32 v45, 0x2, v45                        // Bias address scaled by BPE
v_cndmask_b32 v45, v47, v45, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v46, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v44, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v44, v47, v44, s[88:89]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+40], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+41], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+42], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+43], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+52], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+53], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+54], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+55], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+24]          // k1 * x
v_fma_f32 v4, v[vgprValuC+24], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+24], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+24], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+24], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+25]          // k1 * x
v_fma_f32 v4, v[vgprValuC+25], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+25], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+25], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+25], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+26]          // k1 * x
v_fma_f32 v4, v[vgprValuC+26], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+26], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+26], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+26], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+27]          // k1 * x
v_fma_f32 v4, v[vgprValuC+27], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+27], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+27], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+27], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v30, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v30, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v31, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v31, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+32]          // k1 * x
v_fma_f32 v4, v[vgprValuC+32], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+32], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+32], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+32], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+33]          // k1 * x
v_fma_f32 v4, v[vgprValuC+33], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+33], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+33], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+33], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+34]          // k1 * x
v_fma_f32 v4, v[vgprValuC+34], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+34], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+34], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+34], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+35]          // k1 * x
v_fma_f32 v4, v[vgprValuC+35], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+35], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+35], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+35], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v38, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+41], s[sgprBeta], v38, v[vgprValuC+41] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+42], s[sgprBeta], v39, v[vgprValuC+42] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+43], s[sgprBeta], v39, v[vgprValuC+43] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+40]          // k1 * x
v_fma_f32 v4, v[vgprValuC+40], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+40], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+40], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+40], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+41]          // k1 * x
v_fma_f32 v4, v[vgprValuC+41], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+41], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+41], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+41], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+42]          // k1 * x
v_fma_f32 v4, v[vgprValuC+42], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+42], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+42], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+42], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+43]          // k1 * x
v_fma_f32 v4, v[vgprValuC+43], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+43], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+43], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+43], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
buffer_store_dwordx2 v[40:41], v29, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+52], s[sgprBeta], v48, v[vgprValuC+52] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+53], s[sgprBeta], v48, v[vgprValuC+53] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+54], s[sgprBeta], v49, v[vgprValuC+54] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+55], s[sgprBeta], v49, v[vgprValuC+55] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+52:vgprValuC+52+1], v[16:17], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[vgprValuC+54:vgprValuC+54+1], v[18:19], v[vgprValuC+54:vgprValuC+54+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+52]          // k1 * x
v_fma_f32 v4, v[vgprValuC+52], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+52], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+52], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+52], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+53]          // k1 * x
v_fma_f32 v4, v[vgprValuC+53], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+53], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+53], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+53], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+54]          // k1 * x
v_fma_f32 v4, v[vgprValuC+54], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+54], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+54], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+54], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, 0x3d372713, v[vgprValuC+55]          // k1 * x
v_fma_f32 v4, v[vgprValuC+55], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+55], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+55], v4                  // x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+55], 0.5, v4                 // 0.5 * x * (1 + tanh(...))
v_cvt_f16_f32 v[vgprValuC+52], v[vgprValuC+52]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+53], v[vgprValuC+53]     // convert C to fp16
v_pack_b32_f16 v52, v[vgprValuC+52], v[vgprValuC+53] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+54], v[vgprValuC+54]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+55], v[vgprValuC+55]     // convert C to fp16
v_pack_b32_f16 v53, v[vgprValuC+54], v[vgprValuC+55] // Pack with neighbor
buffer_store_dwordx2 v[52:53], v44, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta_Edge_1
label_Activation_Leakyrelu_Beta_Edge_1:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v47, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v9, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v47, v9, s[88:89]                // LDC clip if OOB. offset
buffer_load_dwordx2 v[12:13], v9, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v0, s84
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
v_cndmask_b32 v10, v47, v10, s[88:89]              // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v10 offset:0                // load bias
v_lshlrev_b32 v11, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v11, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v9, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v47, v9, s[88:89]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v14, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v47, v14, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[30:31], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v15, v0, s84
v_lshlrev_b32 v15, 0x2, v15                        // Bias address scaled by BPE
v_cndmask_b32 v15, v47, v15, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v28, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v14, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v47, v14, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v29, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v29, v47, v29, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[38:39], v29, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v36, v0, s84
v_lshlrev_b32 v36, 0x2, v36                        // Bias address scaled by BPE
v_cndmask_b32 v36, v47, v36, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v37, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v29, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v29, v47, v29, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v44, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v44, v47, v44, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[48:49], v44, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v45, v0, s84
v_lshlrev_b32 v45, 0x2, v45                        // Bias address scaled by BPE
v_cndmask_b32 v45, v47, v45, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v46, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v44, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v44, v47, v44, s[88:89]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+40], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+41], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+42], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+43], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+52], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+53], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+54], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+55], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+24] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+24], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+24], v4, v[vgprValuC+24], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+25] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+25], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+25], v4, v[vgprValuC+25], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+26] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+26], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+26], v4, v[vgprValuC+26], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+27] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+27], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+27], v4, v[vgprValuC+27], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v30, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v30, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v31, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v31, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+32] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+32], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+32], v4, v[vgprValuC+32], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+33] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+33], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+33], v4, v[vgprValuC+33], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+34] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+34], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+34], v4, v[vgprValuC+34], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+35] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+35], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+35], v4, v[vgprValuC+35], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v38, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+41], s[sgprBeta], v38, v[vgprValuC+41] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+42], s[sgprBeta], v39, v[vgprValuC+42] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+43], s[sgprBeta], v39, v[vgprValuC+43] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+40] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+40], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+40], v4, v[vgprValuC+40], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+41] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+41], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+41], v4, v[vgprValuC+41], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+42] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+42], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+42], v4, v[vgprValuC+42], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+43] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+43], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+43], v4, v[vgprValuC+43], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
buffer_store_dwordx2 v[40:41], v29, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+52], s[sgprBeta], v48, v[vgprValuC+52] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+53], s[sgprBeta], v48, v[vgprValuC+53] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+54], s[sgprBeta], v49, v[vgprValuC+54] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+55], s[sgprBeta], v49, v[vgprValuC+55] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+52:vgprValuC+52+1], v[16:17], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[vgprValuC+54:vgprValuC+54+1], v[18:19], v[vgprValuC+54:vgprValuC+54+1] // C += bias
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+52] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+52], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+52], v4, v[vgprValuC+52], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+53] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+53], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+53], v4, v[vgprValuC+53], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+54] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+54], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+54], v4, v[vgprValuC+54], vcc // set x to tmp if < 0
v_mul_f32 v4, s[sgpractivationAlpha], v[vgprValuC+55] // tmp = x * alpha
v_cmp_ge_f32 vcc, v[vgprValuC+55], 0.0             // x >= 0 ?
v_cndmask_b32 v[vgprValuC+55], v4, v[vgprValuC+55], vcc // set x to tmp if < 0
v_cvt_f16_f32 v[vgprValuC+52], v[vgprValuC+52]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+53], v[vgprValuC+53]     // convert C to fp16
v_pack_b32_f16 v52, v[vgprValuC+52], v[vgprValuC+53] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+54], v[vgprValuC+54]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+55], v[vgprValuC+55]     // convert C to fp16
v_pack_b32_f16 v53, v[vgprValuC+54], v[vgprValuC+55] // Pack with neighbor
buffer_store_dwordx2 v[52:53], v44, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta_Edge_1
label_Activation_Relu_Beta_Edge_1:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v47, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v9, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v47, v9, s[88:89]                // LDC clip if OOB. offset
buffer_load_dwordx2 v[12:13], v9, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v0, s84
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
v_cndmask_b32 v10, v47, v10, s[88:89]              // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v10 offset:0                // load bias
v_lshlrev_b32 v11, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v11, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v9, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v47, v9, s[88:89]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v14, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v47, v14, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[30:31], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v15, v0, s84
v_lshlrev_b32 v15, 0x2, v15                        // Bias address scaled by BPE
v_cndmask_b32 v15, v47, v15, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v28, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v14, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v47, v14, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v29, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v29, v47, v29, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[38:39], v29, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v36, v0, s84
v_lshlrev_b32 v36, 0x2, v36                        // Bias address scaled by BPE
v_cndmask_b32 v36, v47, v36, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v37, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v29, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v29, v47, v29, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v44, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v44, v47, v44, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[48:49], v44, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v45, v0, s84
v_lshlrev_b32 v45, 0x2, v45                        // Bias address scaled by BPE
v_cndmask_b32 v45, v47, v45, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v46, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v44, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v44, v47, v44, s[88:89]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+40], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+41], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+42], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+43], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+52], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+53], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+54], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+55], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_max_f32 v[vgprValuC+24], v[vgprValuC+24], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+25], v[vgprValuC+25], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+26], v[vgprValuC+26], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+27], v[vgprValuC+27], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v30, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v30, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v31, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v31, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_max_f32 v[vgprValuC+32], v[vgprValuC+32], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+33], v[vgprValuC+33], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+34], v[vgprValuC+34], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+35], v[vgprValuC+35], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v38, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+41], s[sgprBeta], v38, v[vgprValuC+41] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+42], s[sgprBeta], v39, v[vgprValuC+42] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+43], s[sgprBeta], v39, v[vgprValuC+43] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_max_f32 v[vgprValuC+40], v[vgprValuC+40], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+41], v[vgprValuC+41], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+42], v[vgprValuC+42], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+43], v[vgprValuC+43], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
buffer_store_dwordx2 v[40:41], v29, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+52], s[sgprBeta], v48, v[vgprValuC+52] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+53], s[sgprBeta], v48, v[vgprValuC+53] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+54], s[sgprBeta], v49, v[vgprValuC+54] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+55], s[sgprBeta], v49, v[vgprValuC+55] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+52:vgprValuC+52+1], v[16:17], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[vgprValuC+54:vgprValuC+54+1], v[18:19], v[vgprValuC+54:vgprValuC+54+1] // C += bias
v_max_f32 v[vgprValuC+52], v[vgprValuC+52], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+53], v[vgprValuC+53], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+54], v[vgprValuC+54], 0      // x = max(0, x)
v_max_f32 v[vgprValuC+55], v[vgprValuC+55], 0      // x = max(0, x)
v_cvt_f16_f32 v[vgprValuC+52], v[vgprValuC+52]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+53], v[vgprValuC+53]     // convert C to fp16
v_pack_b32_f16 v52, v[vgprValuC+52], v[vgprValuC+53] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+54], v[vgprValuC+54]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+55], v[vgprValuC+55]     // convert C to fp16
v_pack_b32_f16 v53, v[vgprValuC+54], v[vgprValuC+55] // Pack with neighbor
buffer_store_dwordx2 v[52:53], v44, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta_Edge_1
label_Activation_Sigmoid_Beta_Edge_1:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v47, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v9, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v47, v9, s[88:89]                // LDC clip if OOB. offset
buffer_load_dwordx2 v[12:13], v9, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v0, s84
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
v_cndmask_b32 v10, v47, v10, s[88:89]              // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v10 offset:0                // load bias
v_lshlrev_b32 v11, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v11, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v9, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v47, v9, s[88:89]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v14, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v47, v14, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[30:31], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v15, v0, s84
v_lshlrev_b32 v15, 0x2, v15                        // Bias address scaled by BPE
v_cndmask_b32 v15, v47, v15, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v28, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v14, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v47, v14, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v29, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v29, v47, v29, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[38:39], v29, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v36, v0, s84
v_lshlrev_b32 v36, 0x2, v36                        // Bias address scaled by BPE
v_cndmask_b32 v36, v47, v36, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v37, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v29, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v29, v47, v29, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v44, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v44, v47, v44, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[48:49], v44, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v45, v0, s84
v_lshlrev_b32 v45, 0x2, v45                        // Bias address scaled by BPE
v_cndmask_b32 v45, v47, v45, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v46, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v44, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v44, v47, v44, s[88:89]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+40], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+41], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+42], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+43], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+52], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+53], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+54], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+55], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v[vgprValuC+24], 0xbfb8aa3b, v[vgprValuC+24] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+24], v[vgprValuC+24]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+24], 1.0, v[vgprValuC+24]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+24], v[vgprValuC+24]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+25], 0xbfb8aa3b, v[vgprValuC+25] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+25], v[vgprValuC+25]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+25], 1.0, v[vgprValuC+25]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+25], v[vgprValuC+25]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+26], 0xbfb8aa3b, v[vgprValuC+26] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+26], v[vgprValuC+26]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+26], 1.0, v[vgprValuC+26]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+26], v[vgprValuC+26]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+27], 0xbfb8aa3b, v[vgprValuC+27] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+27], v[vgprValuC+27]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+27], 1.0, v[vgprValuC+27]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+27], v[vgprValuC+27]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v30, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v30, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v31, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v31, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v[vgprValuC+32], 0xbfb8aa3b, v[vgprValuC+32] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+32], v[vgprValuC+32]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+32], 1.0, v[vgprValuC+32]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+32], v[vgprValuC+32]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+33], 0xbfb8aa3b, v[vgprValuC+33] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+33], v[vgprValuC+33]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+33], 1.0, v[vgprValuC+33]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+33], v[vgprValuC+33]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+34], 0xbfb8aa3b, v[vgprValuC+34] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+34], v[vgprValuC+34]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+34], 1.0, v[vgprValuC+34]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+34], v[vgprValuC+34]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+35], 0xbfb8aa3b, v[vgprValuC+35] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+35], v[vgprValuC+35]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+35], 1.0, v[vgprValuC+35]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+35], v[vgprValuC+35]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v38, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+41], s[sgprBeta], v38, v[vgprValuC+41] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+42], s[sgprBeta], v39, v[vgprValuC+42] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+43], s[sgprBeta], v39, v[vgprValuC+43] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_mul_f32 v[vgprValuC+40], 0xbfb8aa3b, v[vgprValuC+40] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+40], v[vgprValuC+40]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+40], 1.0, v[vgprValuC+40]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+40], v[vgprValuC+40]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+41], 0xbfb8aa3b, v[vgprValuC+41] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+41], v[vgprValuC+41]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+41], 1.0, v[vgprValuC+41]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+41], v[vgprValuC+41]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+42], 0xbfb8aa3b, v[vgprValuC+42] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+42], v[vgprValuC+42]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+42], 1.0, v[vgprValuC+42]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+42], v[vgprValuC+42]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+43], 0xbfb8aa3b, v[vgprValuC+43] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+43], v[vgprValuC+43]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+43], 1.0, v[vgprValuC+43]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+43], v[vgprValuC+43]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
buffer_store_dwordx2 v[40:41], v29, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+52], s[sgprBeta], v48, v[vgprValuC+52] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+53], s[sgprBeta], v48, v[vgprValuC+53] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+54], s[sgprBeta], v49, v[vgprValuC+54] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+55], s[sgprBeta], v49, v[vgprValuC+55] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+52:vgprValuC+52+1], v[16:17], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[vgprValuC+54:vgprValuC+54+1], v[18:19], v[vgprValuC+54:vgprValuC+54+1] // C += bias
v_mul_f32 v[vgprValuC+52], 0xbfb8aa3b, v[vgprValuC+52] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+52], v[vgprValuC+52]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+52], 1.0, v[vgprValuC+52]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+52], v[vgprValuC+52]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+53], 0xbfb8aa3b, v[vgprValuC+53] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+53], v[vgprValuC+53]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+53], 1.0, v[vgprValuC+53]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+53], v[vgprValuC+53]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+54], 0xbfb8aa3b, v[vgprValuC+54] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+54], v[vgprValuC+54]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+54], 1.0, v[vgprValuC+54]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+54], v[vgprValuC+54]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v[vgprValuC+55], 0xbfb8aa3b, v[vgprValuC+55] //  (fused -1.442695)
v_exp_f32 v[vgprValuC+55], v[vgprValuC+55]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+55], 1.0, v[vgprValuC+55]    // 1 + exp(-x)
v_rcp_f32 v[vgprValuC+55], v[vgprValuC+55]         // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_cvt_f16_f32 v[vgprValuC+52], v[vgprValuC+52]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+53], v[vgprValuC+53]     // convert C to fp16
v_pack_b32_f16 v52, v[vgprValuC+52], v[vgprValuC+53] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+54], v[vgprValuC+54]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+55], v[vgprValuC+55]     // convert C to fp16
v_pack_b32_f16 v53, v[vgprValuC+54], v[vgprValuC+55] // Pack with neighbor
buffer_store_dwordx2 v[52:53], v44, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta_Edge_1
label_Activation_Tanh_Beta_Edge_1:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v47, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v9, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v47, v9, s[88:89]                // LDC clip if OOB. offset
buffer_load_dwordx2 v[12:13], v9, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v0, s84
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
v_cndmask_b32 v10, v47, v10, s[88:89]              // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v10 offset:0                // load bias
v_lshlrev_b32 v11, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v11, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v9, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v47, v9, s[88:89]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v14, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v47, v14, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[30:31], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v15, v0, s84
v_lshlrev_b32 v15, 0x2, v15                        // Bias address scaled by BPE
v_cndmask_b32 v15, v47, v15, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v28, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v14, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v47, v14, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v29, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v29, v47, v29, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[38:39], v29, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v36, v0, s84
v_lshlrev_b32 v36, 0x2, v36                        // Bias address scaled by BPE
v_cndmask_b32 v36, v47, v36, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v37, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v29, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v29, v47, v29, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v44, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v44, v47, v44, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[48:49], v44, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v45, v0, s84
v_lshlrev_b32 v45, 0x2, v45                        // Bias address scaled by BPE
v_cndmask_b32 v45, v47, v45, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v46, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v44, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v44, v47, v44, s[88:89]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+40], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+41], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+42], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+43], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+52], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+53], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+54], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+55], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v[vgprValuC+24], s[sgpractivationAlpha], v[vgprValuC+24] // x * alpha
v_mul_f32 v[vgprValuC+24], 0x4038aa3b, v[vgprValuC+24] //  (fused 2)
v_exp_f32 v[vgprValuC+24], v[vgprValuC+24]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+24], 1.0, v[vgprValuC+24]    // e^2x + 1
v_rcp_f32 v[vgprValuC+24], v[vgprValuC+24]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+24], -2.0, v[vgprValuC+24], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+24], s[sgpractivationBeta], v[vgprValuC+24] // beta * tanh(x)
v_mul_f32 v[vgprValuC+25], s[sgpractivationAlpha], v[vgprValuC+25] // x * alpha
v_mul_f32 v[vgprValuC+25], 0x4038aa3b, v[vgprValuC+25] //  (fused 2)
v_exp_f32 v[vgprValuC+25], v[vgprValuC+25]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+25], 1.0, v[vgprValuC+25]    // e^2x + 1
v_rcp_f32 v[vgprValuC+25], v[vgprValuC+25]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+25], -2.0, v[vgprValuC+25], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+25], s[sgpractivationBeta], v[vgprValuC+25] // beta * tanh(x)
v_mul_f32 v[vgprValuC+26], s[sgpractivationAlpha], v[vgprValuC+26] // x * alpha
v_mul_f32 v[vgprValuC+26], 0x4038aa3b, v[vgprValuC+26] //  (fused 2)
v_exp_f32 v[vgprValuC+26], v[vgprValuC+26]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+26], 1.0, v[vgprValuC+26]    // e^2x + 1
v_rcp_f32 v[vgprValuC+26], v[vgprValuC+26]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+26], -2.0, v[vgprValuC+26], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+26], s[sgpractivationBeta], v[vgprValuC+26] // beta * tanh(x)
v_mul_f32 v[vgprValuC+27], s[sgpractivationAlpha], v[vgprValuC+27] // x * alpha
v_mul_f32 v[vgprValuC+27], 0x4038aa3b, v[vgprValuC+27] //  (fused 2)
v_exp_f32 v[vgprValuC+27], v[vgprValuC+27]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+27], 1.0, v[vgprValuC+27]    // e^2x + 1
v_rcp_f32 v[vgprValuC+27], v[vgprValuC+27]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+27], -2.0, v[vgprValuC+27], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+27], s[sgpractivationBeta], v[vgprValuC+27] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v30, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v30, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v31, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v31, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v[vgprValuC+32], s[sgpractivationAlpha], v[vgprValuC+32] // x * alpha
v_mul_f32 v[vgprValuC+32], 0x4038aa3b, v[vgprValuC+32] //  (fused 2)
v_exp_f32 v[vgprValuC+32], v[vgprValuC+32]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+32], 1.0, v[vgprValuC+32]    // e^2x + 1
v_rcp_f32 v[vgprValuC+32], v[vgprValuC+32]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+32], -2.0, v[vgprValuC+32], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+32], s[sgpractivationBeta], v[vgprValuC+32] // beta * tanh(x)
v_mul_f32 v[vgprValuC+33], s[sgpractivationAlpha], v[vgprValuC+33] // x * alpha
v_mul_f32 v[vgprValuC+33], 0x4038aa3b, v[vgprValuC+33] //  (fused 2)
v_exp_f32 v[vgprValuC+33], v[vgprValuC+33]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+33], 1.0, v[vgprValuC+33]    // e^2x + 1
v_rcp_f32 v[vgprValuC+33], v[vgprValuC+33]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+33], -2.0, v[vgprValuC+33], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+33], s[sgpractivationBeta], v[vgprValuC+33] // beta * tanh(x)
v_mul_f32 v[vgprValuC+34], s[sgpractivationAlpha], v[vgprValuC+34] // x * alpha
v_mul_f32 v[vgprValuC+34], 0x4038aa3b, v[vgprValuC+34] //  (fused 2)
v_exp_f32 v[vgprValuC+34], v[vgprValuC+34]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+34], 1.0, v[vgprValuC+34]    // e^2x + 1
v_rcp_f32 v[vgprValuC+34], v[vgprValuC+34]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+34], -2.0, v[vgprValuC+34], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+34], s[sgpractivationBeta], v[vgprValuC+34] // beta * tanh(x)
v_mul_f32 v[vgprValuC+35], s[sgpractivationAlpha], v[vgprValuC+35] // x * alpha
v_mul_f32 v[vgprValuC+35], 0x4038aa3b, v[vgprValuC+35] //  (fused 2)
v_exp_f32 v[vgprValuC+35], v[vgprValuC+35]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+35], 1.0, v[vgprValuC+35]    // e^2x + 1
v_rcp_f32 v[vgprValuC+35], v[vgprValuC+35]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+35], -2.0, v[vgprValuC+35], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+35], s[sgpractivationBeta], v[vgprValuC+35] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v38, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+41], s[sgprBeta], v38, v[vgprValuC+41] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+42], s[sgprBeta], v39, v[vgprValuC+42] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+43], s[sgprBeta], v39, v[vgprValuC+43] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_mul_f32 v[vgprValuC+40], s[sgpractivationAlpha], v[vgprValuC+40] // x * alpha
v_mul_f32 v[vgprValuC+40], 0x4038aa3b, v[vgprValuC+40] //  (fused 2)
v_exp_f32 v[vgprValuC+40], v[vgprValuC+40]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+40], 1.0, v[vgprValuC+40]    // e^2x + 1
v_rcp_f32 v[vgprValuC+40], v[vgprValuC+40]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+40], -2.0, v[vgprValuC+40], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+40], s[sgpractivationBeta], v[vgprValuC+40] // beta * tanh(x)
v_mul_f32 v[vgprValuC+41], s[sgpractivationAlpha], v[vgprValuC+41] // x * alpha
v_mul_f32 v[vgprValuC+41], 0x4038aa3b, v[vgprValuC+41] //  (fused 2)
v_exp_f32 v[vgprValuC+41], v[vgprValuC+41]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+41], 1.0, v[vgprValuC+41]    // e^2x + 1
v_rcp_f32 v[vgprValuC+41], v[vgprValuC+41]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+41], -2.0, v[vgprValuC+41], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+41], s[sgpractivationBeta], v[vgprValuC+41] // beta * tanh(x)
v_mul_f32 v[vgprValuC+42], s[sgpractivationAlpha], v[vgprValuC+42] // x * alpha
v_mul_f32 v[vgprValuC+42], 0x4038aa3b, v[vgprValuC+42] //  (fused 2)
v_exp_f32 v[vgprValuC+42], v[vgprValuC+42]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+42], 1.0, v[vgprValuC+42]    // e^2x + 1
v_rcp_f32 v[vgprValuC+42], v[vgprValuC+42]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+42], -2.0, v[vgprValuC+42], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+42], s[sgpractivationBeta], v[vgprValuC+42] // beta * tanh(x)
v_mul_f32 v[vgprValuC+43], s[sgpractivationAlpha], v[vgprValuC+43] // x * alpha
v_mul_f32 v[vgprValuC+43], 0x4038aa3b, v[vgprValuC+43] //  (fused 2)
v_exp_f32 v[vgprValuC+43], v[vgprValuC+43]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+43], 1.0, v[vgprValuC+43]    // e^2x + 1
v_rcp_f32 v[vgprValuC+43], v[vgprValuC+43]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+43], -2.0, v[vgprValuC+43], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+43], s[sgpractivationBeta], v[vgprValuC+43] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
buffer_store_dwordx2 v[40:41], v29, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+52], s[sgprBeta], v48, v[vgprValuC+52] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+53], s[sgprBeta], v48, v[vgprValuC+53] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+54], s[sgprBeta], v49, v[vgprValuC+54] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+55], s[sgprBeta], v49, v[vgprValuC+55] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+52:vgprValuC+52+1], v[16:17], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[vgprValuC+54:vgprValuC+54+1], v[18:19], v[vgprValuC+54:vgprValuC+54+1] // C += bias
v_mul_f32 v[vgprValuC+52], s[sgpractivationAlpha], v[vgprValuC+52] // x * alpha
v_mul_f32 v[vgprValuC+52], 0x4038aa3b, v[vgprValuC+52] //  (fused 2)
v_exp_f32 v[vgprValuC+52], v[vgprValuC+52]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+52], 1.0, v[vgprValuC+52]    // e^2x + 1
v_rcp_f32 v[vgprValuC+52], v[vgprValuC+52]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+52], -2.0, v[vgprValuC+52], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+52], s[sgpractivationBeta], v[vgprValuC+52] // beta * tanh(x)
v_mul_f32 v[vgprValuC+53], s[sgpractivationAlpha], v[vgprValuC+53] // x * alpha
v_mul_f32 v[vgprValuC+53], 0x4038aa3b, v[vgprValuC+53] //  (fused 2)
v_exp_f32 v[vgprValuC+53], v[vgprValuC+53]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+53], 1.0, v[vgprValuC+53]    // e^2x + 1
v_rcp_f32 v[vgprValuC+53], v[vgprValuC+53]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+53], -2.0, v[vgprValuC+53], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+53], s[sgpractivationBeta], v[vgprValuC+53] // beta * tanh(x)
v_mul_f32 v[vgprValuC+54], s[sgpractivationAlpha], v[vgprValuC+54] // x * alpha
v_mul_f32 v[vgprValuC+54], 0x4038aa3b, v[vgprValuC+54] //  (fused 2)
v_exp_f32 v[vgprValuC+54], v[vgprValuC+54]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+54], 1.0, v[vgprValuC+54]    // e^2x + 1
v_rcp_f32 v[vgprValuC+54], v[vgprValuC+54]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+54], -2.0, v[vgprValuC+54], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+54], s[sgpractivationBeta], v[vgprValuC+54] // beta * tanh(x)
v_mul_f32 v[vgprValuC+55], s[sgpractivationAlpha], v[vgprValuC+55] // x * alpha
v_mul_f32 v[vgprValuC+55], 0x4038aa3b, v[vgprValuC+55] //  (fused 2)
v_exp_f32 v[vgprValuC+55], v[vgprValuC+55]         // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v[vgprValuC+55], 1.0, v[vgprValuC+55]    // e^2x + 1
v_rcp_f32 v[vgprValuC+55], v[vgprValuC+55]         // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v[vgprValuC+55], -2.0, v[vgprValuC+55], 1.0 // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v[vgprValuC+55], s[sgpractivationBeta], v[vgprValuC+55] // beta * tanh(x)
v_cvt_f16_f32 v[vgprValuC+52], v[vgprValuC+52]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+53], v[vgprValuC+53]     // convert C to fp16
v_pack_b32_f16 v52, v[vgprValuC+52], v[vgprValuC+53] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+54], v[vgprValuC+54]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+55], v[vgprValuC+55]     // convert C to fp16
v_pack_b32_f16 v53, v[vgprValuC+54], v[vgprValuC+55] // Pack with neighbor
buffer_store_dwordx2 v[52:53], v44, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
s_branch label_Activation_End_Beta_Edge_1
label_Activation_Geluscaling_Beta_Edge_1:
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v47, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v9, v2, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v47, v9, s[88:89]                // LDC clip if OOB. offset
buffer_load_dwordx2 v[12:13], v9, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v10, v0, s84
v_lshlrev_b32 v10, 0x2, v10                        // Bias address scaled by BPE
v_cndmask_b32 v10, v47, v10, s[88:89]              // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v10 offset:0                // load bias
v_lshlrev_b32 v11, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v11, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v9, v3, v0, 0x1                     // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v9, v47, v9, s[88:89]                // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v14, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v47, v14, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[30:31], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v15, v0, s84
v_lshlrev_b32 v15, 0x2, v15                        // Bias address scaled by BPE
v_cndmask_b32 v15, v47, v15, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v28, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v14, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v47, v14, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v29, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v29, v47, v29, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[38:39], v29, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v36, v0, s84
v_lshlrev_b32 v36, 0x2, v36                        // Bias address scaled by BPE
v_cndmask_b32 v36, v47, v36, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v37, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v29, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v29, v47, v29, s[88:89]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[84:85], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[88:89], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[88:89], s[84:85], s[88:89]             // in0 && in1

/* MultipleBufferSingleKernel edge */
v_add_lshl_u32 v44, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v44, v47, v44, s[88:89]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[48:49], v44, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s84, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v45, v0, s84
v_lshlrev_b32 v45, 0x2, v45                        // Bias address scaled by BPE
v_cndmask_b32 v45, v47, v45, s[88:89]              // LDBias clip if OOB. offset
v_lshlrev_b32 v46, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v44, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v44, v47, v44, s[88:89]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+40], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+41], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+42], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+43], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+52], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+53], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+54], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+55], acc15          // copy acc to vreg[15]
s_nop 1                                            // 2 wait states required before reading vgpr

/* MultipleBufferSingleKernel store after Acc */

/* _GlobalAccumulation: None */

/* GlobalSplitU: 1 */
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst

//GSUSYNC sourece store done 1


/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+24], s[sgprBeta], v12, v[vgprValuC+24] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+25], s[sgprBeta], v12, v[vgprValuC+25] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+26], s[sgprBeta], v13, v[vgprValuC+26] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+27], s[sgprBeta], v13, v[vgprValuC+27] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+24:vgprValuC+24+1], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[vgprValuC+26:vgprValuC+26+1], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+24]          // k1 * x
v_fma_f32 v4, v[vgprValuC+24], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+24], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+24], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+24], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+25]          // k1 * x
v_fma_f32 v4, v[vgprValuC+25], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+25], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+25], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+25], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+26]          // k1 * x
v_fma_f32 v4, v[vgprValuC+26], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+26], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+26], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+26], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+27]          // k1 * x
v_fma_f32 v4, v[vgprValuC+27], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+27], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+27], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+27], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+24], v[vgprValuC+24]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+25], v[vgprValuC+25]     // convert C to fp16
v_pack_b32_f16 v24, v[vgprValuC+24], v[vgprValuC+25] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+26], v[vgprValuC+26]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+27], v[vgprValuC+27]     // convert C to fp16
v_pack_b32_f16 v25, v[vgprValuC+26], v[vgprValuC+27] // Pack with neighbor
buffer_store_dwordx2 v[24:25], v9, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+32], s[sgprBeta], v30, v[vgprValuC+32] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+33], s[sgprBeta], v30, v[vgprValuC+33] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+34], s[sgprBeta], v31, v[vgprValuC+34] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+35], s[sgprBeta], v31, v[vgprValuC+35] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+32:vgprValuC+32+1], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[vgprValuC+34:vgprValuC+34+1], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+32]          // k1 * x
v_fma_f32 v4, v[vgprValuC+32], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+32], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+32], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+32], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+33]          // k1 * x
v_fma_f32 v4, v[vgprValuC+33], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+33], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+33], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+33], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+34]          // k1 * x
v_fma_f32 v4, v[vgprValuC+34], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+34], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+34], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+34], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+35]          // k1 * x
v_fma_f32 v4, v[vgprValuC+35], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+35], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+35], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+35], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+32], v[vgprValuC+32]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+33], v[vgprValuC+33]     // convert C to fp16
v_pack_b32_f16 v32, v[vgprValuC+32], v[vgprValuC+33] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+34], v[vgprValuC+34]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+35], v[vgprValuC+35]     // convert C to fp16
v_pack_b32_f16 v33, v[vgprValuC+34], v[vgprValuC+35] // Pack with neighbor
buffer_store_dwordx2 v[32:33], v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+40], s[sgprBeta], v38, v[vgprValuC+40] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+41], s[sgprBeta], v38, v[vgprValuC+41] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+42], s[sgprBeta], v39, v[vgprValuC+42] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+43], s[sgprBeta], v39, v[vgprValuC+43] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+40:vgprValuC+40+1], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[vgprValuC+42:vgprValuC+42+1], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+40]          // k1 * x
v_fma_f32 v4, v[vgprValuC+40], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+40], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+40], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+40], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+41]          // k1 * x
v_fma_f32 v4, v[vgprValuC+41], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+41], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+41], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+41], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+42]          // k1 * x
v_fma_f32 v4, v[vgprValuC+42], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+42], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+42], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+42], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+43]          // k1 * x
v_fma_f32 v4, v[vgprValuC+43], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+43], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+43], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+43], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+40], v[vgprValuC+40]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+41], v[vgprValuC+41]     // convert C to fp16
v_pack_b32_f16 v40, v[vgprValuC+40], v[vgprValuC+41] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+42], v[vgprValuC+42]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+43], v[vgprValuC+43]     // convert C to fp16
v_pack_b32_f16 v41, v[vgprValuC+42], v[vgprValuC+43] // Pack with neighbor
buffer_store_dwordx2 v[40:41], v29, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(20)(2)
v_fma_mix_f32 v[vgprValuC+52], s[sgprBeta], v48, v[vgprValuC+52] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+53], s[sgprBeta], v48, v[vgprValuC+53] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+54], s[sgprBeta], v49, v[vgprValuC+54] op_sel:[0,0,0] op_sel_hi:[0,1,0] // //C*=beta
v_fma_mix_f32 v[vgprValuC+55], s[sgprBeta], v49, v[vgprValuC+55] op_sel:[0,1,0] op_sel_hi:[0,1,0] // //C*=beta
v_pk_add_f32 v[vgprValuC+52:vgprValuC+52+1], v[16:17], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[vgprValuC+54:vgprValuC+54+1], v[18:19], v[vgprValuC+54:vgprValuC+54+1] // C += bias
v_mul_f32 v4, 0x3d372713, v[vgprValuC+52]          // k1 * x
v_fma_f32 v4, v[vgprValuC+52], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+52], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+52], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+52], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+53]          // k1 * x
v_fma_f32 v4, v[vgprValuC+53], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+53], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+53], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+53], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+54]          // k1 * x
v_fma_f32 v4, v[vgprValuC+54], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+54], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+54], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+54], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v4, 0x3d372713, v[vgprValuC+55]          // k1 * x
v_fma_f32 v4, v[vgprValuC+55], v4, 1.0             // 1 + (k1 * x * x)
v_mul_f32 v4, v[vgprValuC+55], v4                  // x * (1 + k1 * x * x)
v_mul_f32 v4, 0x40135761, v4                       //  (fused 2.302208)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 2.0                        //  ( + 1 (fused))
v_mul_f32 v4, v[vgprValuC+55], v4                  // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v4                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v[vgprValuC+55], s[sgpractivationAlpha], v4 // 0.5 * x * (1 + tanh(...)) * scale
v_cvt_f16_f32 v[vgprValuC+52], v[vgprValuC+52]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+53], v[vgprValuC+53]     // convert C to fp16
v_pack_b32_f16 v52, v[vgprValuC+52], v[vgprValuC+53] // Pack with neighbor
v_cvt_f16_f32 v[vgprValuC+54], v[vgprValuC+54]     // convert C to fp16
v_cvt_f16_f32 v[vgprValuC+55], v[vgprValuC+55]     // convert C to fp16
v_pack_b32_f16 v53, v[vgprValuC+54], v[vgprValuC+55] // Pack with neighbor
buffer_store_dwordx2 v[52:53], v44, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0, sc0 sc1 // store D not StoreRemapVectorWidth
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
//synchronizer end
label_Activation_End_Beta_Edge_1:
s_branch label_GW_End_1                            // jump to end
label_GW_End_1:
label_KernelEnd:
s_endpgm                                           // Kernel End
